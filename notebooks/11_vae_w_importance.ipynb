{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n",
    "from src.eval.ds import filter_ds_to_known\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    model=\"microsoft/phi-2\",\n",
    "    batch_size=1,\n",
    "    prompt_format=\"phi\",\n",
    ")\n",
    "cfg\n",
    "\n",
    "# params\n",
    "batch_size = 128\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "MAX_ROWS = 80000\n",
    "SKIP=5\n",
    "STRIDE=2\n",
    "device = \"cuda:0\"\n",
    "max_epochs = 100\n",
    "\n",
    "VAE_EPOCH_MULT = 1\n",
    "l1_coeff = 1.0e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58931ef2e224bccb0250d7a370270c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    model_class=PhiForCausalLMWHS, # ti add hidden states\n",
    ")\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 76.89% based on knowledge\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['end_logits_base', 'choice_probs_base', 'binary_ans_base', 'label_true_base', 'label_instructed_base', 'instructed_to_lie_base', 'sys_instr_name_base', 'example_i_base', 'ds_string_base', 'template_name_base', 'correct_truth_telling_base', 'correct_instruction_following_base', 'end_residual_stream_base', 'end_logits_adapt', 'choice_probs_adapt', 'binary_ans_adapt', 'label_true_adapt', 'label_instructed_adapt', 'instructed_to_lie_adapt', 'sys_instr_name_adapt', 'example_i_adapt', 'ds_string_adapt', 'template_name_adapt', 'correct_truth_telling_adapt', 'correct_instruction_following_adapt', 'end_residual_stream_adapt'],\n",
       "    num_rows: 2140\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load hidden state from a previously loaded adapter\n",
    "# the columns with _base are from the base model, and adapt from adapter\n",
    "# FROM TRAIING TRUTH\n",
    "f1_ood = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_OOD_6d3ece46c44f6c3b'\n",
    "f1_val = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_valtest_73b754e8fdff9f2f'\n",
    "ds_val = Dataset.from_file(f1_val)\n",
    "ds_oos = Dataset.from_file(f1_ood)\n",
    "\n",
    "ds_out = datasets.interleave_datasets([ds_val, ds_oos], seed=42, \n",
    "                                    #   probabilities=[0.5, 0.5]\n",
    "                                      )\n",
    "ds_out2 = filter_ds_to_known(ds_out, verbose=True)\n",
    "\n",
    "# ds_out2 = ds_out2.select_columns(['end_residual_stream_base', 'end_residual_stream_adapt', 'binary_ans_base', 'binary_ans_adapt'])\n",
    "# ds_known1 = ds_out\n",
    "# ds_known1\n",
    "ds_out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['super_glue:axg', 'glue:qnli', 'super_glue:rte', 'amazon_polarity', 'hans', 'sst2'] ['super_glue:axg', 'super_glue:boolq', 'imdb']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc174edc60904461b0cb8175da5a98fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b1bb7279514d53a3110a1ae21bba90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1529, 1115)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "outsample_datasets = list(set(ds_oos['ds_string_base']))\n",
    "print(insample_datasets, outsample_datasets)\n",
    "\n",
    "\n",
    "ds_trainval = ds_out2.filter(lambda example: example[\"ds_string_base\"] in insample_datasets)\n",
    "ds_test = ds_out2.filter(lambda example: example[\"ds_string_base\"] not in outsample_datasets)\n",
    "\n",
    "MAX_SAMPLES = min(len(ds_trainval), MAX_ROWS)\n",
    "ds_trainval = ds_trainval.select(range(MAX_SAMPLES))\n",
    "\n",
    "MAX_SAMPLES = min(len(ds_test), MAX_ROWS)\n",
    "ds_test = ds_test.select(range(MAX_SAMPLES))\n",
    "\n",
    "len(ds_trainval), len(ds_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edb5cef35a8426184405474c2122de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a28e5993ca4ea192cf5416fa4e8970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ds2xy(row):\n",
    "    X = torch.stack([row['end_residual_stream_base'], row['end_residual_stream_adapt']], dim=-1)[:, SKIP::STRIDE]\n",
    "    y = row['binary_ans_base']-row['binary_ans_adapt']\n",
    "    return dict(X=X, y=y)\n",
    "\n",
    "\n",
    "def prepare_ds(ds):\n",
    "    \"\"\"prepared a dataset for training\"\"\"\n",
    "    ds = ds.with_format(\"torch\")\n",
    "    ds = ds.map(ds2xy)\n",
    "    ds = ds.select_columns(['X', 'y'])\n",
    "    return ds\n",
    "\n",
    "\n",
    "ds_trainval2 = prepare_ds(ds_trainval)\n",
    "ds_test2 = prepare_ds(ds_test)\n",
    "\n",
    "# next(iter(ds_trainval2)).keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.datasets.dm.DeceptionDataModule at 0x7f55d627b110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.datasets.dm import DeceptionDataModule\n",
    "\n",
    "# .select_columns(self.x_cols)\n",
    "\n",
    "# TEMP try with the counterfactual residual stream...\n",
    "dm = DeceptionDataModule(ds_trainval2, batch_size=batch_size)\n",
    "dm.setup(\"train\")\n",
    "dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_oos = DeceptionDataModule(ds_test2, batch_size=batch_size, \n",
    "                         )\n",
    "dm_oos.setup(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Optional, Callable, Union, List, Tuple\n",
    "\n",
    "from torch import dropout\n",
    "from src.probes.pl_ranking_probe import InceptionBlock, LinBnDrop, ConvBlock\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, n_channels, hs, c_out, ks=[7, 5, 3], dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm1d(n_channels, affine=False),\n",
    "            InceptionBlock(n_channels, hs, ks=ks, coord=True, conv_dropout=dropout),\n",
    "            InceptionBlock(hs * 4, hs, ks=ks, coord=True, conv_dropout=dropout),\n",
    "            # InceptionBlock(hs*4, hs, ks=ks, coord=True, conv_dropout=dropout),\n",
    "            InceptionBlock(hs * 4, hs, ks=ks, coord=True),\n",
    "            InceptionBlock(hs * 4, hs, ks=ks),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            LinBnDrop(hs * 4 * n_layers, c_out * n_layers, dropout=dropout),\n",
    "            nn.Linear(c_out * n_layers, c_out * n_layers),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = rearrange(x, \"b c l -> b (c l)\")\n",
    "        x = self.fc(x)\n",
    "        x = rearrange(x, \"b (c l) -> b c l\", l=self.n_layers)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_latent, n_layers, hs, c_out=1, ks=[7, 5, 3], dropout=0):\n",
    "        super().__init__()\n",
    "        self.layers = n_layers\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(\n",
    "                n_latent * n_layers, affine=False\n",
    "            ),  # center it, regularize it\n",
    "            LinBnDrop(n_latent * n_layers, hs * n_layers, dropout=dropout),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            InceptionBlock(hs, hs, ks=ks, coord=True, conv_dropout=dropout),\n",
    "            InceptionBlock(hs * 4, hs, ks=ks, conv_dropout=dropout),\n",
    "            InceptionBlock(hs * 4, hs, ks=ks, coord=True),\n",
    "            nn.Conv1d(hs * 4, c_out, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"b l c -> b (l c)\")\n",
    "        x = self.fc(x)\n",
    "        x = rearrange(x, \"b (c l) -> b c l\", l=self.layers)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, c_in, depth=3, n_hidden=32, n_latent=32, l1_coeff: float = 1.0, dropout=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.l1_coeff = l1_coeff\n",
    "        n_layers, n_channels = c_in\n",
    "        self.enc = Encoder(n_layers, n_channels, n_hidden, n_latent, dropout=dropout)\n",
    "        self.dec = Decoder(\n",
    "            n_latent, n_layers, n_hidden // 4, c_out=n_channels, dropout=dropout\n",
    "        )\n",
    "        self.apply_weight_norm(self.dec)\n",
    "        self.apply_weight_norm(self.enc)\n",
    "\n",
    "    def apply_weight_norm(self, net):\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                # I think it's 1. In the example they use 2, but their weights are transposed before use\n",
    "                torch.nn.utils.parametrizations.weight_norm(m, dim=1)\n",
    "\n",
    "    def forward(self, h: Float[Tensor, \"batch_size n_hidden n_channels\"]):\n",
    "        latent = self.enc(h)\n",
    "        h_rec = self.dec(latent)\n",
    "\n",
    "        # Compute loss, return values\n",
    "        l2_loss = (\n",
    "            (h_rec - h).pow(2).mean(-1).sum(1)\n",
    "        )  # shape [batch_size sum(neurons) mean(layers)] - punish the model for not reconstructing the input\n",
    "        l1_loss = (\n",
    "            latent.abs().sum(-1).sum(1)\n",
    "        )  # shape [batch_size sum(latent) sum(layers)] - punish the model for large latent values\n",
    "        loss = (self.l1_coeff * l1_loss + l2_loss).mean(0)  # scalar\n",
    "\n",
    "        return l1_loss, l2_loss, loss, latent, h_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_requires_grad(model, mode: bool = False):\n",
    "    print(f\"requires_grad: {mode}\")\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.pl_base import PLBase\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "\n",
    "class PLAE(PLBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in,\n",
    "        total_steps,\n",
    "        depth=0,\n",
    "        lr=4e-3,\n",
    "        weight_decay=1e-9,\n",
    "        hs=64,\n",
    "        n_latent=32,\n",
    "        l1_coeff=1,\n",
    "        dropout=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(total_steps=total_steps, lr=lr, weight_decay=weight_decay)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.ae = AutoEncoder(\n",
    "            c_in,\n",
    "            n_hidden=hs,\n",
    "            n_latent=n_latent,\n",
    "            depth=depth,\n",
    "            l1_coeff=l1_coeff,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        n_layers, n_channels = c_in\n",
    "        n = n_latent * n_layers\n",
    "        self.head = nn.Sequential(\n",
    "            LinBnDrop(n, n // 4, dropout=dropout),\n",
    "            LinBnDrop(n // 4, n // 12, dropout=dropout),\n",
    "            nn.Linear(n // 12, 1),\n",
    "            # nn.Tanh(),\n",
    "        )\n",
    "        self._ae_mode = True\n",
    "\n",
    "    def ae_mode(self, mode=0):\n",
    "        \"\"\"\n",
    "        mode 0, train the ae\n",
    "        mode 1, train only the prob\n",
    "        mode 2, train both\n",
    "        \"\"\"\n",
    "        self._ae_mode = mode\n",
    "        recursive_requires_grad(self.ae, mode in [0, 2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 4:\n",
    "            x = x.squeeze(3)\n",
    "        x = rearrange(x, \"b l h -> b h l\")\n",
    "        # if not self._ae_mode:\n",
    "        #     with torch.no_grad():\n",
    "        #         l1_loss, l2_loss, loss, latent, h_rec = self.ae(x)\n",
    "        # else:\n",
    "        l1_loss, l2_loss, loss, latent, h_rec = self.ae(x)\n",
    "\n",
    "        latent2 = rearrange(latent, \"b l h -> b (l h)\")\n",
    "        pred = self.head(latent2).squeeze(1)\n",
    "        return dict(\n",
    "            pred=pred,\n",
    "            l1_loss=l1_loss,\n",
    "            l2_loss=l2_loss,\n",
    "            loss=loss,\n",
    "            latent=latent,\n",
    "            h_rec=h_rec,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch, batch_idx, stage=\"train\"):\n",
    "        # if stage=='train':\n",
    "        #     # Normalize the decoder weights before each optimization step (from https://colab.research.google.com/drive/1rPy82rL3iZzy2_Rd3F82RwFhlVnnroIh?usp=sharing#scrollTo=q1JctT2Pvw-r)\n",
    "        #     # Presumably this is a way to implement weight norm to regularize the decoder\n",
    "        #     self.normalize_decoder()\n",
    "\n",
    "        x0, x1, y = batch\n",
    "        info0 = self(x0)\n",
    "        info1 = self(x1)\n",
    "        ypred1 = info1[\"pred\"]\n",
    "        ypred0 = info0[\"pred\"]\n",
    "\n",
    "        if stage == \"pred\":\n",
    "            return (ypred1 - ypred0).float()\n",
    "\n",
    "        pred_loss = F.smooth_l1_loss(ypred1 - ypred0, y)\n",
    "        rec_loss = info0[\"loss\"] + info1[\"loss\"]\n",
    "        l1_loss = (info0[\"l1_loss\"] + info1[\"l1_loss\"]).mean()\n",
    "        l2_loss = (info0[\"l2_loss\"] + info1[\"l2_loss\"]).mean()\n",
    "\n",
    "        y_cls = ypred1 > ypred0  # switch2bool(ypred1-ypred0)\n",
    "        self.log(\n",
    "            f\"{stage}/acc\",\n",
    "            accuracy(y_cls, y > 0, \"binary\"),\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{stage}/loss_pred\",\n",
    "            float(pred_loss),\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{stage}/loss_rec\",\n",
    "            float(rec_loss),\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(f\"{stage}/l1_loss\", l1_loss, on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/l2_loss\", l2_loss, on_epoch=True, on_step=False)\n",
    "        self.log(\n",
    "            f\"{stage}/n\",\n",
    "            float(len(y)),\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            reduce_fx=torch.sum,\n",
    "        )\n",
    "        if self._ae_mode == 0:\n",
    "            return rec_loss\n",
    "        elif self._ae_mode == 1:\n",
    "            return pred_loss\n",
    "        elif self._ae_mode == 2:\n",
    "            return pred_loss * 50000 + rec_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_subset(df, query, verbose=True):\n",
    "    if query:\n",
    "        df = df.query(query)\n",
    "    acc = (df[\"probe_pred\"] == df[\"y\"]).mean()\n",
    "    if verbose:\n",
    "        print(f\"acc={acc:2.2%},\\tn={len(df)},\\t[{query}] \")\n",
    "    return acc\n",
    "\n",
    "\n",
    "def calc_metrics(dm, trainer, net, use_val=False, verbose=True):\n",
    "    dl_test = dm.test_dataloader()\n",
    "    rt = trainer.predict(net, dataloaders=dl_test)\n",
    "    y_test_pred = np.concatenate(rt)\n",
    "    splits = dm.splits[\"test\"]\n",
    "    df_test = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "    df_test[\"probe_pred\"] = y_test_pred > 0.0\n",
    "\n",
    "    if use_val:\n",
    "        dl_val = dm.val_dataloader()\n",
    "        rv = trainer.predict(net, dataloaders=dl_val)\n",
    "        y_val_pred = np.concatenate(rv)\n",
    "        splits = dm.splits[\"val\"]\n",
    "        df_val = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "        df_val[\"probe_pred\"] = y_val_pred > 0.0\n",
    "\n",
    "        df_test = pd.concat([df_val, df_test])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"probe results on subsets of the data\")\n",
    "    acc = get_acc_subset(df_test, \"\", verbose=verbose)\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True\", verbose=verbose\n",
    "    )  # it was ph told to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False\", verbose=verbose\n",
    "    )  # it was told not to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"llm_ans==label_true\", verbose=verbose\n",
    "    )  # the llm gave the true ans\n",
    "    get_acc_subset(\n",
    "        df_test, \"llm_ans==label_instructed\", verbose=verbose\n",
    "    )  # the llm gave the desired ans\n",
    "    acc_lie_lie = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=verbose\n",
    "    )  # it was told to lie, and it did lie\n",
    "    acc_lie_truth = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    a = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & llm_ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    b = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & llm_ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "    c = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    d = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "    d1 = pd.DataFrame(\n",
    "        [[a, b], [c, d]],\n",
    "        index=[\"instructed_to_lie==False\", \"instructed_to_lie==True\"],\n",
    "        columns=[\"llm_ans==label_instructed\", \"llm_ans!=label_instructed\"],\n",
    "    )\n",
    "    d1 = pd.DataFrame(\n",
    "        [[a, b], [c, d]],\n",
    "        index=[\"tell a truth\", \"tell a lie\"],\n",
    "        columns=[\"did\", \"didn't\"],\n",
    "    )\n",
    "    d1.index.name = \"instructed to\"\n",
    "    d1.columns.name = \"llm gave\"\n",
    "    print(\"probe accuracy for quadrants\")\n",
    "    display(d1.round(2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"⭐PRIMARY METRIC⭐ acc={acc:2.2%} from probe\")\n",
    "        print(f\"⭐SECONDARY METRIC⭐ acc_lie_lie={acc_lie_lie:2.2%} from probe\")\n",
    "    return dict(acc=acc, acc_lie_lie=acc_lie_lie, acc_lie_truth=acc_lie_truth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 3\n",
      "torch.Size([128, 33, 1277, 2]) x\n"
     ]
    }
   ],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "print(len(dl_train), len(dl_val))\n",
    "b = next(iter(dl_train))\n",
    "x, y = b['X'], b['y']\n",
    "print(x.shape, \"x\")\n",
    "if x.ndim == 3:\n",
    "    x = x.unsqueeze(-1)\n",
    "c_in = x.shape[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PLBase.__init__() got an unexpected keyword argument 'total_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m net \u001b[39m=\u001b[39m PLAE(\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     c_in\u001b[39m=\u001b[39;49mc_in,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     total_steps\u001b[39m=\u001b[39;49mmax_epochs \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(dl_train) \u001b[39m*\u001b[39;49m VAE_EPOCH_MULT,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mwd,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     hs\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     n_latent\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     l1_coeff\u001b[39m=\u001b[39;49ml1_coeff,  \u001b[39m# neel uses 3e-4 ! https://github.dev/neelnanda-io/1L-Sparse-Autoencoder/blob/bcae01328a2f41d24bd4a9160828f2fc22737f75/utils.py#L106, but them they sum l1 where mean l2\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# x_feats=x_feats\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(c_in)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     c_in,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(total_steps\u001b[39m=\u001b[39;49mtotal_steps, lr\u001b[39m=\u001b[39;49mlr, weight_decay\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_hyperparameters()\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mae \u001b[39m=\u001b[39m AutoEncoder(\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         c_in,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         n_hidden\u001b[39m=\u001b[39mhs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         dropout\u001b[39m=\u001b[39mdropout,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/11_vae_w_importance.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: PLBase.__init__() got an unexpected keyword argument 'total_steps'"
     ]
    }
   ],
   "source": [
    "\n",
    "net = PLAE(\n",
    "    c_in=c_in,\n",
    "    total_steps=max_epochs * len(dl_train) * VAE_EPOCH_MULT,\n",
    "    lr=lr,\n",
    "    weight_decay=wd,\n",
    "    hs=32,\n",
    "    dropout=0.1,\n",
    "    n_latent=6,\n",
    "    l1_coeff=l1_coeff,  # neel uses 3e-4 ! https://github.dev/neelnanda-io/1L-Sparse-Autoencoder/blob/bcae01328a2f41d24bd4a9160828f2fc22737f75/utils.py#L106, but them they sum l1 where mean l2\n",
    "    # x_feats=x_feats\n",
    ")\n",
    "print(c_in)\n",
    "with torch.no_grad():\n",
    "    y = net(x)\n",
    "{k: v.abs().mean() for k, v in y.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(net, input_data=x)  # input_size=(batch_size, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(0)\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    # devices=2,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"1\",\n",
    "    max_epochs=max_epochs * VAE_EPOCH_MULT,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    ")\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = read_metrics_csv(trainer1.logger.experiment.metrics_file_path).ffill().bfill()\n",
    "for key in [\"loss_rec\"]:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot(logy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_hist[[c for c in df_hist.columns if \"train/l2\" in c]]\n",
    "a = (a / l1_coeff).rename(columns=lambda x: f\"{x} * {1/l1_coeff}\")\n",
    "b = df_hist[[c for c in df_hist.columns if \"train/l1\" in c]]\n",
    "pd.concat([a, b], axis=1).plot(logy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize latent space\n",
    "from matplotlib import cm\n",
    "\n",
    "latent = y[\"latent\"].cpu()  # .reshape(64, 24, 12) # [Batch, Latent, Layer]\n",
    "vmax = latent.abs().max()\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    vmax = latent[i].abs().max()\n",
    "    plt.imshow(\n",
    "        latent[i],\n",
    "        cmap=cm.coolwarm,\n",
    "        interpolation=\"none\",\n",
    "        aspect=\"auto\",\n",
    "        vmin=-vmax,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    plt.xlabel(\"layer\")\n",
    "    plt.ylabel(\"neuron\")\n",
    "    if i < 2:\n",
    "        plt.xlabel(\"\")\n",
    "        plt.xticks([])\n",
    "    if i % 2 == 1:\n",
    "        plt.ylabel(\"\")\n",
    "        plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.colorbar()\n",
    "# plt.colorbar()\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.imshow(latent[1], cmap=cm.coolwarm, interpolation='none', aspect='auto', vmin=-vmax, vmax=vmax)\n",
    "# plt.xlabel('layer')\n",
    "# plt.ylabel('neuron')\n",
    "# plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "latentf = rearrange(latent, \"b n l -> (b n) l\").flatten()\n",
    "vmax = (latentf.abs().mean() + 5 * latentf.abs().std()).item()\n",
    "plt.hist(latentf, bins=55, range=[-vmax, vmax], histtype=\"step\")\n",
    "plt.title(\"latents by layer\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(1)\n",
    "trainer2 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    ")\n",
    "trainer2.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at hist\n",
    "df_hist = read_metrics_csv(trainer2.logger.experiment.metrics_file_path).ffill().bfill()\n",
    "for key in [\"loss_pred\"]:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot()\n",
    "\n",
    "for key in [\"acc\"]:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot()\n",
    "df_hist\n",
    "\n",
    "# predict\n",
    "dl_test = dm.test_dataloader()\n",
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs = trainer2.test(net, dataloaders=[dl_train, dl_val, dl_test, dl_oos])\n",
    "\n",
    "testval_metrics = calc_metrics(dm, trainer2, net, use_val=True)\n",
    "rs = rename(rs, [\"train\", \"val\", \"test\", \"oos\"])\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how well does it generalize to other datasets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs2 = trainer1.test(net, dataloaders=[dl_oos])\n",
    "rs2 = rename(rs2, ks=[\"oos\"])\n",
    "\n",
    "testval_metrics2 = calc_metrics(dm_oos, trainer1, net, use_val=True)\n",
    "rs[\"oos\"][\"acc_lie_lie\"] = testval_metrics2[\"acc_lie_lie\"]\n",
    "rs[\"oos_metrics\"] = rs2[\"oos\"]\n",
    "rs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(2)\n",
    "trainer2 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    ")\n",
    "trainer2.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at hist\n",
    "df_hist = read_metrics_csv(trainer2.logger.experiment.metrics_file_path).ffill().bfill()\n",
    "for key in [\"loss_pred\"]:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot()\n",
    "\n",
    "for key in [\"acc\"]:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot()\n",
    "df_hist\n",
    "\n",
    "# predict\n",
    "dl_test = dm.test_dataloader()\n",
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs = trainer2.test(net, dataloaders=[dl_train, dl_val, dl_test, dl_oos])\n",
    "\n",
    "testval_metrics = calc_metrics(dm, trainer2, net, use_val=True)\n",
    "rs = rename(rs, [\"train\", \"val\", \"test\", \"oos\"])\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs2 = trainer1.test(net, dataloaders=[dl_oos])\n",
    "rs2 = rename(rs2, ks=[\"oos\"])\n",
    "\n",
    "testval_metrics2 = calc_metrics(dm_oos, trainer1, net, use_val=True)\n",
    "rs[\"oos\"][\"acc_lie_lie\"] = testval_metrics2[\"acc_lie_lie\"]\n",
    "rs[\"oos_metrics\"] = rs2[\"oos\"]\n",
    "rs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
