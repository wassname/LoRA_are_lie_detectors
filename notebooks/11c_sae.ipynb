{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a sparse 1 layer autoencoder, then a probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.llms.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.llms.phi.model_phi import PhiForCausalLMWHS\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "from src.datasets.act_dm import ActivationDataModule\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.style.use(\"seaborn-v0_8\")\n",
    "import seaborn as sns\n",
    "sns.set_theme('paper')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramsnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "# cfg = ExtractConfig(\n",
    "#     # model=\"microsoft/phi-2\",\n",
    "#     # # batch_size=1,\n",
    "#     # prompt_format=\"phi\",\n",
    "# )\n",
    "# cfg\n",
    "\n",
    "# params\n",
    "batch_size = 32\n",
    "lr = 1e-3 # at 3e-4 I get nan\n",
    "wd = 0 # 1e-5\n",
    "\n",
    "MAX_ROWS = 2000\n",
    "\n",
    "SKIP=5 # skip initial N layers\n",
    "STRIDE=4 # skip every N layers\n",
    "DECIMATE=1 # discard N features for speed\n",
    "\n",
    "device = \"cuda:0\"\n",
    "max_epochs = 44\n",
    "\n",
    "l1_coeff = 0.5  # neel uses 3e-4 ! https://github.dev/neelnanda-io/1L-Sparse-Autoencoder/blob/bcae01328a2f41d24bd4a9160828f2fc22737f75/utils.py#L106, but them they sum l1 where mean l2\n",
    "    # x_feats=x_feats. other use 1e-1\n",
    "\n",
    "\n",
    "BASE_FOLDER = Path(\"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/\")\n",
    "layers_names = (\n",
    "    # 'fc1', 'Wqkv',\n",
    "                 'fc2', 'out_proj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/hidden_states/.ds/ds_valtest_8b8fd6070504d5ef'),\n",
       " PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/hidden_states/.ds/ds_OOD_a41d3a61513ade30'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load hidden state from a previously loaded adapter\n",
    "# the columns with _base are from the base model, and adapt from adapter\n",
    "# FROM TRAINING TRUTH\n",
    "f1_val = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_valtest_*')))\n",
    "f1_ood = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_OOD_*')))\n",
    "f1_val, f1_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "# # outsample_datasets = list(set(ds_ood['ds_string_base']))\n",
    "# # print(insample_datasets, outsample_datasets)\n",
    "# from src.datasets.act_dm import ActivationDataModule, SharedDataset\n",
    "\n",
    "\n",
    "# class ActivationDataModule2(ActivationDataModule):\n",
    "#     def to_tds(self, ds, name):\n",
    "#         \"\"\"huggingface dataset to pytorch.\"\"\"\n",
    "#         h = self.hparams\n",
    "#         # 4x faster if we make it a tensor ourselves\n",
    "#         ds = ds.with_format(None)\n",
    "#         tds = torch.utils.data.TensorDataset(\n",
    "#             torch.FloatTensor(ds['X'][..., 0]), torch.FloatTensor(ds['y']))\n",
    "        \n",
    "#         # this shared dataset is 10x faster with multiple workers\n",
    "#         if h.num_workers>0: \n",
    "#             tds = SharedDataset(tds, f\"{self.hparams.name}_{name}\") \n",
    "#         return tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 74.39% based on knowledge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88608ff1f5aa4a4b8daf58d71f3c7e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-07 08:40:25.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.act_dm\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mconverting datasets this may take a while... ds_valtest_8b8fd6070504d5ef train\u001b[0m\n",
      "2024-01-07T08:40:25.362241+0800 INFO converting datasets this may take a while... ds_valtest_8b8fd6070504d5ef train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 74.39% based on knowledge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4451e3d8826d452086ee256e4892d3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-07 08:40:34.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.act_dm\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mconverting datasets this may take a while... ds_OOD_a41d3a61513ade30 all\u001b[0m\n",
      "2024-01-07T08:40:34.556765+0800 INFO converting datasets this may take a while... ds_OOD_a41d3a61513ade30 all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making shared dataset with unique name all\n"
     ]
    }
   ],
   "source": [
    "input_columns = ['binary_ans_base', 'binary_ans_adapt' ] + [f'end_residual_{layer}_base' for layer in layers_names] + [f'end_residual_{layer}_adapt' for layer in layers_names]\n",
    "\n",
    "def ds2xy_batched(ds):\n",
    "    data = []\n",
    "    for layer in layers_names:\n",
    "        # Stack the base and adapter representations as a 4th dim\n",
    "        X1 = [ds[f'end_residual_{layer}_base'], ds[f'end_residual_{layer}_adapt']]\n",
    "        X1 = rearrange(X1, 'versions b l f  -> b l f versions')[..., 0]\n",
    "        data.append(X1)\n",
    "    \n",
    "    # concat layers\n",
    "    # x = rearrange(data, 'b parts l f v -> b l (parts f) v')\n",
    "    X = torch.concat(data, dim=2)[:, SKIP::STRIDE, ::DECIMATE]\n",
    "\n",
    "    y = ds['binary_ans_base']-ds['binary_ans_adapt']\n",
    "    return dict(X=X, y=y)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_ds(ds):\n",
    "    \"\"\"\n",
    "    prepare a dataset for training\n",
    "\n",
    "    this should front load much of the computation\n",
    "    it should restrict it to the needed rows X and y\n",
    "    \n",
    "    \"\"\"\n",
    "    ds = (ds\n",
    "          .with_format(\"torch\")\n",
    "          .select_columns(input_columns)\n",
    "          .map(ds2xy_batched, batched=True, batch_size=128,\n",
    "        remove_columns=input_columns)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def load_file_to_dm(f, stage):\n",
    "    ds = Dataset.from_file(str(f1_val), in_memory=True).with_format(\"torch\")\n",
    "    ds = filter_ds_to_known(ds, verbose=True, true_col='truth')\n",
    "    ds = prepare_ds(ds)\n",
    "\n",
    "    # limit size\n",
    "    MAX_SAMPLES = min(len(ds), MAX_ROWS*2)\n",
    "    ds = ds.select(range(0, MAX_SAMPLES))\n",
    "\n",
    "    dm = ActivationDataModule(ds, f.stem, batch_size=batch_size, num_workers=0)\n",
    "    dm.setup(stage)\n",
    "    return dm\n",
    "\n",
    "\n",
    "dm = load_file_to_dm(f1_val, 'train')\n",
    "dm_ood = load_file_to_dm(f1_ood, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "dl_test = dm.test_dataloader()\n",
    "dl_ood = dm_ood.all_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with dataloading speeds:\n",
    "- does it help to save the Xy dataset to disc, then load, while keeping in mem?. no not faster at all\n",
    "- does it help to use num_workers > 0? yes 3x faster\n",
    "- the shared dataset wrapper is 10x faster, and less mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get importance matrix from adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wqkv torch.Size([32, 7680])\n",
      "out_proj torch.Size([32, 2560])\n",
      "fc1 torch.Size([32, 10240])\n",
      "fc2 torch.Size([32, 2560])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.probes.importance_matrix import get_importance_matrix\n",
    "\n",
    "\n",
    "f = f\"{BASE_FOLDER}/checkpoint_last/adapter_model.safetensors\"\n",
    "importance_matrix = get_importance_matrix(f, layers=layers_names)[SKIP::STRIDE, ::DECIMATE]\n",
    "plt.hist(importance_matrix.flatten(), bins=155);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1674)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test2 = dm.datasets['test']\n",
    "shape1 = ds_test2[0][0].shape\n",
    "shape2= importance_matrix.shape\n",
    "np.testing.assert_equal(shape1, shape2, err_msg=\"shape mismatch between ds and importance matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.vae.conv_inception import PLAE, LinBnDrop, PLBase, recursive_requires_grad, accuracy, auroc\n",
    "\n",
    "from src.vae.sae import AutoEncoder, AutoEncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PLAE(PLBase):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in,\n",
    "        steps_per_epoch,\n",
    "        max_epochs,\n",
    "        # depth=0,\n",
    "        lr=4e-3,\n",
    "        weight_decay=1e-9,\n",
    "        # hs=64,\n",
    "        n_latent=32,\n",
    "        l1_coeff=1,\n",
    "        dropout=0,\n",
    "        importance_matrix=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(steps_per_epoch=steps_per_epoch, max_epochs=max_epochs, lr=lr, weight_decay=weight_decay)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        n_layers, n_channels = c_in\n",
    "        self.ae_cfg = AutoEncoderConfig(\n",
    "            n_instances=n_layers,\n",
    "            n_input_ae=n_channels,\n",
    "            n_hidden_ae=n_latent,\n",
    "            tied_weights=False,\n",
    "            l1_coeff=l1_coeff,\n",
    "        )\n",
    "\n",
    "        self.ae = AutoEncoder(\n",
    "            self.ae_cfg,\n",
    "            importance_matrix=importance_matrix,\n",
    "        )\n",
    "        \n",
    "        n = n_latent * n_layers\n",
    "        self.head = nn.Sequential(\n",
    "            LinBnDrop(n, n, bn=False),\n",
    "            LinBnDrop(n, n // 4, dropout=dropout, bn=False),\n",
    "            LinBnDrop(n // 4, n // 12, bn=False),\n",
    "            nn.Linear(n // 12, 1),\n",
    "            # nn.Tanh(),\n",
    "        )\n",
    "        self._ae_mode = True\n",
    "\n",
    "    def ae_mode(self, mode=0):\n",
    "        \"\"\"\n",
    "        mode 0, train the ae\n",
    "        mode 1, train only the prob\n",
    "        mode 2, train both\n",
    "        \"\"\"\n",
    "        if mode==0:\n",
    "            print('training ae')\n",
    "        elif mode==1:\n",
    "            print('training probe')\n",
    "        elif mode==2:\n",
    "            print('training both ae and probe')\n",
    "        self._ae_mode = mode\n",
    "        recursive_requires_grad(self.ae, mode in [0, 2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 4:\n",
    "            x = x.squeeze(3)\n",
    "        # x = rearrange(x, \"b l h -> b h l\")\n",
    "        # if not self._ae_mode:\n",
    "        #     with torch.no_grad():\n",
    "        #         l1_loss, l2_loss, loss, latent, h_rec = self.ae(x)\n",
    "        # else:\n",
    "        l1_loss, l2_loss, loss, latent, h_rec = self.ae(x)\n",
    "\n",
    "        latent2 = rearrange(latent, \"b l h -> b (l h)\")\n",
    "        pred = self.head(latent2).squeeze(1)\n",
    "        return dict(\n",
    "            pred=pred,\n",
    "            l1_loss=l1_loss,\n",
    "            l2_loss=l2_loss,\n",
    "            loss=loss,\n",
    "            latent=latent,\n",
    "            h_rec=h_rec,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch, batch_idx, stage=\"train\"):        \n",
    "        if stage == \"train\":\n",
    "            # Normalize the decoder weights before each optimization step\n",
    "            self.ae.normalize_decoder()\n",
    "\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "        x, y = batch # batch['X'], batch['y']\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        x0 = x#[..., 0]\n",
    "        # x1 = x[..., 1]\n",
    "        info0 = self(x0)\n",
    "        # info1 = self(x1)\n",
    "        # ypred1 = info1[\"pred\"]\n",
    "        logits = info0[\"pred\"]\n",
    "        y_probs = F.sigmoid(logits)\n",
    "        y_cls = y_probs > 0.5\n",
    "\n",
    "        if stage == \"pred\":\n",
    "            return (y_probs).float()\n",
    "        \n",
    "        pred_loss = F.binary_cross_entropy_with_logits(logits, (y>0.).float())\n",
    "\n",
    "        # pred_loss = F.smooth_l1_loss(ypred0, y)\n",
    "        rec_loss = info0[\"loss\"] \n",
    "        l1_loss = info0[\"l1_loss\"].mean()\n",
    "        l2_loss = info0[\"l2_loss\"].mean()\n",
    "\n",
    "        self.log(\n",
    "            f\"{stage}/auroc\",\n",
    "            auroc(y_probs, y > 0, \"binary\"),\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{stage}/acc\",\n",
    "            accuracy(y_cls, y > 0, \"binary\"),\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{stage}/loss_pred\",\n",
    "            float(pred_loss),\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            f\"{stage}/loss_rec\",\n",
    "            float(rec_loss),\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(f\"{stage}/l1_loss\", l1_loss, on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/l2_loss\", l2_loss, on_epoch=True, on_step=False)\n",
    "        self.log(\n",
    "            f\"{stage}/n\",\n",
    "            float(len(y)),\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            reduce_fx=torch.sum,\n",
    "        )\n",
    "        if self._ae_mode == 0:\n",
    "            assert torch.isfinite(rec_loss), \"rec_loss is not finite\"\n",
    "            return rec_loss\n",
    "        elif self._ae_mode == 1:\n",
    "            assert torch.isfinite(pred_loss), \"pred_loss is not finite\"\n",
    "            return pred_loss\n",
    "        elif self._ae_mode == 2:\n",
    "            # , train/loss_pred_epoch=0.0195, train/loss_rec_epoch=169.0\n",
    "            assert torch.isfinite(pred_loss), \"pred_loss is not finite\"\n",
    "            assert torch.isfinite(rec_loss), \"rec_loss is not finite\"\n",
    "            return pred_loss * 50000 + rec_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5\n",
      "torch.Size([32, 7, 5120]) x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(dl_train), len(dl_val))\n",
    "b = next(iter(dl_train))\n",
    "x, y = b # b['X'], b['y']\n",
    "print(x.shape, \"x\")\n",
    "if x.ndim == 3:\n",
    "    x = x.unsqueeze(-1)\n",
    "c_in = x.shape[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# for b in tqdm(dl_train):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # oh no, this is very slow\n",
    "# g = iter(dl_train)\n",
    "# b = next(g)\n",
    "# b = next(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5120])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'pred': tensor(0.1292),\n",
       "  'l1_loss': tensor(1.2090),\n",
       "  'l2_loss': tensor(13.0143),\n",
       "  'loss': tensor(95.3315),\n",
       "  'latent': tensor(0.0756),\n",
       "  'h_rec': tensor(0.0016)},\n",
       " {'pred': torch.Size([32]),\n",
       "  'l1_loss': torch.Size([32, 7]),\n",
       "  'l2_loss': torch.Size([32, 7]),\n",
       "  'loss': torch.Size([]),\n",
       "  'latent': torch.Size([32, 7, 16]),\n",
       "  'h_rec': torch.Size([32, 7, 5120])})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net = PLAE(\n",
    "    c_in=c_in,\n",
    "    steps_per_epoch=len(dl_train),\n",
    "    max_epochs=max_epochs,\n",
    "    lr=lr,\n",
    "    weight_decay=wd,\n",
    "    # hs=64,\n",
    "    dropout=0,\n",
    "    n_latent=16, # there will be layers * n_latent latent features\n",
    "    l1_coeff=l1_coeff, \n",
    "    importance_matrix=importance_matrix,\n",
    ")\n",
    "print(c_in)\n",
    "x1= x[..., 0]\n",
    "with torch.no_grad():\n",
    "    y = net(x1)\n",
    "{k: v.abs().mean() for k, v in y.items()}, {k: v.shape for k, v in y.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "PLAE                                     [32, 7, 5120]             --\n",
       "├─AutoEncoder: 1-1                       [32, 7]                   1,182,832\n",
       "├─Sequential: 1-2                        [32, 1]                   --\n",
       "│    └─LinBnDrop: 2-1                    [32, 112]                 --\n",
       "│    │    └─Linear: 3-1                  [32, 112]                 12,656\n",
       "│    │    └─ReLU: 3-2                    [32, 112]                 --\n",
       "│    └─LinBnDrop: 2-2                    [32, 28]                  --\n",
       "│    │    └─Linear: 3-3                  [32, 28]                  3,164\n",
       "│    │    └─ReLU: 3-4                    [32, 28]                  --\n",
       "│    └─LinBnDrop: 2-3                    [32, 9]                   --\n",
       "│    │    └─Linear: 3-5                  [32, 9]                   261\n",
       "│    │    └─ReLU: 3-6                    [32, 9]                   --\n",
       "│    └─Linear: 2-4                       [32, 1]                   10\n",
       "==========================================================================================\n",
       "Total params: 1,198,923\n",
       "Trainable params: 1,198,923\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.51\n",
       "==========================================================================================\n",
       "Input size (MB): 4.59\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 4.80\n",
       "Estimated Total Size (MB): 9.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(net, input_data=x1, depth=4)  # input_size=(batch_size, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0015e7d6b197461e9781b74db06bc7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 78.9 ms, sys: 7.55 ms, total: 86.4 ms\n",
      "Wall time: 12.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for b in tqdm(dl_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv, plot_hist, rename_pl_test_results\n",
    "\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name | Type        | Params\n",
      "-------------------------------------\n",
      "0 | ae   | AutoEncoder | 1.2 M \n",
      "1 | head | Sequential  | 16.1 K\n",
      "-------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.796     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ae\n",
      "requires_grad: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6b8dca19d64413820cf7109bab9f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c33f9468c7540f5b11caa215833d0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21da0f5743f74a69a0d04f57bf17987c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed73670229f4a82bcd2650c4f9d25b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e67658987944dca2b6bcb1f05907e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a1ec7cbbc44670a5f34a7e9d765f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807e61beb98f427c89c70d7e46b7b035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dff2dda66534d9b8a399db5e1cc0b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b675aebd4394782997f22bab151fd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c624bc77aa6b43b6b85488d3c8d84702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aedfbaa141845d393e933d0417b7a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3c906f805a4ac9ac6c816a190d31a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f228d69d6a4104b8775dbc654af724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5673fa329aa3454ebd6289ced39a95d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7dc7ad14274981bac1ac510e868688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dff409f92bd46cabe1f25b686b1a2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d6de29437d4d8f8be81b1d3438c56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653a0403836648ec982d72d0a28c20ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93e49a45b4f4b89b253b6b14d5ffff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc514d48c754df888ae69c007491e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c6b8dae8824f8c909b2782a0cc289c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a31cd735dc34a4595922ae9925ef02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5afd583133b4c51a62871596c849019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net.ae_mode(0)\n",
    " \n",
    "lr_logger = LearningRateMonitor(logging_interval='step')\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    # devices=2,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"1\",\n",
    "    max_epochs=max_epochs,# * VAE_EPOCH_MULT,\n",
    "    log_every_n_steps=1,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    "    callbacks=[lr_logger],\n",
    ")\n",
    "\n",
    "# LOAD_CHECKPONT = Path('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_52/1_ae.ckpt')\n",
    "LOAD_CHECKPONT = None\n",
    "if LOAD_CHECKPONT:\n",
    "    PLAE.load_from_checkpoint(LOAD_CHECKPONT)\n",
    "else:\n",
    "    trainer1.fit(model=net, train_dataloaders=dl_train, \n",
    "                 val_dataloaders=dl_val # FIXME why does this slow it down with multiple processes?\n",
    "                );\n",
    "\n",
    "    df_hist, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "    plot_hist(df_hist, ['l2_loss', 'l1_loss', 'loss_rec'], logy=True)\n",
    "    plt.show()\n",
    "    plot_hist(df_hist_step, ['loss_rec_step'], logy=True)\n",
    "\n",
    "    display(df_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the LR\n",
    "df_hist, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "df_hist['lr-AdamW'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/10 [00:04<00:04, 1.10it/s, v_num=295, val/loss_pred=0.0849, val/loss_rec=9.58e+5, train/loss_pred=0.350, train/loss_rec=9.8e+5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize latent space\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def plot_latent(latent):\n",
    "\n",
    "    # plot image of latent space\n",
    "    vmax = latent.abs().max()\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        vmax = latent[i].abs().max()\n",
    "        plt.imshow(\n",
    "            latent[i],\n",
    "            cmap=cm.coolwarm,\n",
    "            interpolation=\"none\",\n",
    "            aspect=\"auto\",\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        plt.xlabel(\"layer\")\n",
    "        plt.ylabel(\"neuron\")\n",
    "        if i < 2:\n",
    "            plt.xlabel(\"\")\n",
    "            plt.xticks([])\n",
    "        if i % 2 == 1:\n",
    "            plt.ylabel(\"\")\n",
    "            plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.colorbar()\n",
    "    # plt.colorbar()\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "    # histogram\n",
    "    latentf = rearrange(latent, \"b n l -> (b n) l\").flatten()\n",
    "    vmax = (latentf.abs().mean() + 5 * latentf.abs().std()).item()\n",
    "    plt.hist(latentf, bins=55, range=[-vmax, vmax], histtype=\"step\")\n",
    "    plt.title(\"latents by layer\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "latent = y[\"latent\"].cpu()  # .reshape(64, 24, 12) # [Batch, Latent, Layer]\n",
    "plot_latent(latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent.shape, latent.diff(dim=1).std(), latent.std(), latent.diff(dim=2).std(), 16*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # net.save_checkpoint\n",
    "# f = Path(trainer1.log_dir)/\"1_ae.ckpt\"\n",
    "# trainer1.save_checkpoint(f)\n",
    "# # PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_52/1_ae.ckpt')\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for b in tqdm(dl_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for b in tqdm(dl_train):\n",
    "    y = net.predict_step(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     b = next(iter(dl_train))\n",
    "#     y = net.predict_step(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(1)\n",
    "trainer2 = pl.Trainer(\n",
    "    # precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    "    # callbacks=[lr_logger],\n",
    ")\n",
    "trainer2.fit(model=net, train_dataloaders=dl_train, \n",
    "            #  val_dataloaders=dl_val\n",
    "             );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist, _ = read_metrics_csv(trainer2.logger.experiment.metrics_file_path)\n",
    "plot_hist(df_hist, ['loss_pred_epoch', 'auroc'])\n",
    "df_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs = trainer2.test(net, dataloaders=[dl_train, dl_val, dl_test, dl_ood])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testval_metrics = calc_metrics(dm, trainer2, net, use_val=True)\n",
    "rs = rename_pl_test_results(rs, [\"train\", \"val\", \"test\", \"ood\"])\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "# rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how well does it generalize to other datasets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs2 = trainer1.test(net, dataloaders=[dl_ood])\n",
    "rs2 = rename_pl_test_results(rs2, ks=[\"ood\"])\n",
    "\n",
    "# testval_metrics2 = calc_metrics(dm_ood, trainer1, net, use_val=True)\n",
    "# rs[\"ood\"][\"acc_lie_lie\"] = testval_metrics2[\"acc_lie_lie\"]\n",
    "# rs[\"ood_metrics\"] = rs2[\"ood\"]\n",
    "# rs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(2)\n",
    "trainer3 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    ")\n",
    "trainer3.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at hist\n",
    "df_hist, _ = read_metrics_csv(trainer3.logger.experiment.metrics_file_path)\n",
    "plot_hist(df_hist, ['loss_pred', 'acc'])\n",
    "\n",
    "# predict\n",
    "# dl_test = dm.test_dataloader()\n",
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs = trainer3.test(net, dataloaders=[dl_train, dl_val, dl_test, dl_ood])\n",
    "\n",
    "testval_metrics = calc_metrics(dm, trainer3, net, use_val=True)\n",
    "rs = rename_pl_test_results(rs, [\"train\", \"val\", \"test\", \"ood\"])\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs2 = trainer3.test(net, dataloaders=[dl_ood])\n",
    "rs2 = rename_pl_test_results(rs2, ks=[\"ood\"])\n",
    "\n",
    "testval_metrics2 = calc_metrics(dm_ood, trainer3, net, use_val=True)\n",
    "rs[\"ood\"][\"acc_lie_lie\"] = testval_metrics2[\"acc_lie_lie\"]\n",
    "rs[\"ood_metrics\"] = rs2[\"ood\"]\n",
    "rs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
