{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    "# )\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset, load_preproc_datasets\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 5\n",
    "device = \"cuda:0\"\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    max_examples=(1000, 1000),\n",
    "    # model=\"wassname/phi-1_5-w_hidden_states\",\n",
    "    # batch_size=3,\n",
    "    # model=\"wassname/phi-2-w_hidden_states\",\n",
    "    model=\"microsoft/phi-2\",\n",
    "    # model=\"microsoft/phi-1_5\",\n",
    "    # model=\"Walmart-the-bag/phi-2-uncensored\",\n",
    "    batch_size=1,\n",
    "    prompt_format=\"phi\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    model_class=PhiForCausalLMWHS, # ti add hidden states\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for normal seetings see https://github.com/huggingface/peft/blob/cf04d0353f0343cbf66627228c4495f51669af34/src/peft/utils/constants.py#L81\n",
    "# and https://github.com/huggingface/peft/blob/cf04d0353f0343cbf66627228c4495f51669af34/src/peft/utils/constants.py#L102\n",
    "# \"llama\": [\"k_proj\", \"v_proj\", \"down_proj\"],\n",
    "# \"gptj\": [\"q_proj\", \"v_proj\", \"fc_out\"],\n",
    "# \"falcon\": [\"query_key_value\", \"dense_4h_to_h\"],\n",
    "\n",
    "# for activation gathering\n",
    "peft_config = IA3Config(\n",
    "    task_type=TaskType.SEQ_CLS, target_modules=[ \"fc1\", \"Wqkv\",], \n",
    "        feedforward_modules=[]\n",
    ")\n",
    "# peft_config = IA3Config(\n",
    "#     task_type=TaskType.SEQ_CLS, target_modules=[ \"fc1\", \"fc2\", \"Wqkv\",\"out_proj\"], \n",
    "#         feedforward_modules=[\"fc2\",\"out_proj\", \"fc1\"]\n",
    "# )\n",
    "\n",
    "# peft_config = IA3Config(\n",
    "#     task_type=TaskType.SEQ_CLS, target_modules=[  \"fc2\", \"out_proj\"], \n",
    "#         feedforward_modules=[]\n",
    "# )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(cfg.datasets).intersection(cfg.datasets_ood))==0, \"datasets overlap\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_tokens = load_preproc_datasets(\n",
    "    cfg.datasets,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokens2 = load_preproc_datasets(\n",
    "    cfg.datasets_ood,\n",
    "    tokenizer,\n",
    "    N=N // 2,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "class AtapterFinetunerLie(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        simply train it to lie\n",
    "        \"\"\"\n",
    "\n",
    "        log_probs_a = torch.log_softmax(out_a[\"logits\"][:, -1,], -1,)\n",
    "\n",
    "        # batch['instructed_to_lie']\n",
    "        lie_label = ~batch['label_true']\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long())\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long())\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "\n",
    "class AtapterFinetunerToldToLie(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        train it to lie when instructed\n",
    "        \"\"\"\n",
    "\n",
    "        end_logits = out_a[\"logits\"][\n",
    "            :,\n",
    "            -1,\n",
    "        ]\n",
    "        log_probs_a = torch.log_softmax(end_logits, -1)\n",
    "\n",
    "        lie_label = batch[\"label_true\"] ^ batch[\"instructed_to_lie\"]\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long())\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long())\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "\n",
    "class AtapterFinetunerTruth(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        train it to lie when instructed\n",
    "        \"\"\"\n",
    "\n",
    "        end_logits = out_a[\"logits\"][\n",
    "            :,\n",
    "            -1,\n",
    "        ]\n",
    "        log_probs_a = torch.log_softmax(end_logits, -1)\n",
    "\n",
    "        lie_label = batch[\"label_true\"] #^ batch[\"instructed_to_lie\"]\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long())\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long())\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = AtapterFinetunerToldToLie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "len(dl_train), len(dl_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b[\"input_ids\"].shape)\n",
    "c_in = b[\"input_ids\"].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model_cls(\n",
    "    model, tokenizer, lr=5e-3, weight_decay=1e-5, total_steps=len(dl_train) * max_epochs\n",
    ")\n",
    "\n",
    "print(c_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# # net.half()\n",
    "# with torch.no_grad():\n",
    "#     o = net.training_step(b, None)\n",
    "# o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.predict_step(b, None)\n",
    "# o.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to init lightning early, so it inits accelerate\n",
    "trainer = pl.Trainer(\n",
    "    precision='16-mixed',\n",
    "\n",
    "    gradient_clip_val=20,\n",
    "    devices=\"1\",\n",
    "    accelerator=\"gpu\",\n",
    "    accumulate_grad_batches=8,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    # enable_model_summary=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(trainer.log_dir) / \"checkpoint_last\"\n",
    "model.save_pretrained(checkpoint_path)\n",
    "checkpoint_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save config\n",
    "f = Path(trainer.log_dir) / 'config.yaml'\n",
    "cfg.save_yaml(f)\n",
    "f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv\n",
    "\n",
    "df_histe, df_hist = read_metrics_csv(trainer.logger.experiment.metrics_file_path)\n",
    "df_hist[[\"train/loss_step\", \"val/loss_step\"]].plot(style=\".\")\n",
    "df_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_histe[[\"train/loss_step\", \"val/loss_step\"]].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n",
    "\n",
    "This acts a QC to check of the trained adapter is still coherent while giving the opposite answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.gen import gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We need to reload it from checkpoint, since lightning seems to bug it after running\n",
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    "    bnb=False,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.half()\n",
    ";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose a row where we will see the difference\n",
    "\n",
    "from src.eval.ds import ds2df\n",
    "df_tokens = ds2df(ds_tokens).reset_index()\n",
    "mask = (\n",
    "    (df_tokens['instructed_to_lie']==True) &\n",
    "    (df_tokens['label_true']==False)\n",
    ")\n",
    "bis = df_tokens[mask].index\n",
    "\n",
    "\n",
    "# # mask = (\n",
    "# #     (ds_tokens['instructed_to_lie']==True) &\n",
    "# #     (ds_tokens['label_true']==False)\n",
    "# # ).float()\n",
    "bi = int(np.random.choice(bis))\n",
    "bi\n",
    "# # TODO doesn't work if the model gets it wrong\n",
    "inputs = ds_tokens.with_format(\"torch\")[bi]\n",
    "df_tokens.iloc[bi]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.disable_adapter():\n",
    "    gen(model, inputs, tokenizer)\n",
    "\n",
    "gen(model, inputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.labels import ds2label_model_obey, ds2label_model_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_MULT = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2 = DeceptionDataModule(ds_tokens2, batch_size=cfg.batch_size * TEST_BATCH_MULT)\n",
    "dl_train2 = dm2.train_dataloader()\n",
    "dl_train2.shuffle = False\n",
    "\n",
    "dl_val2 = dm2.val_dataloader()\n",
    "dl_test2 = dm2.test_dataloader()\n",
    "\n",
    "dl_valtest2 = DataLoader(\n",
    "    torch.utils.data.ConcatDataset([dm.datasets[\"val\"], dm.datasets[\"test\"]]),\n",
    "    batch_size=cfg.batch_size * TEST_BATCH_MULT,\n",
    ")\n",
    "len(dl_valtest2.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_OOD = DataLoader(\n",
    "    ds_tokens2, batch_size=cfg.batch_size * TEST_BATCH_MULT, drop_last=False, shuffle=False\n",
    ")\n",
    "len(dl_OOD.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    ")\n",
    "net = model_cls(model, tokenizer)\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import rename_pl_test_results\n",
    "\n",
    "rs1 = trainer.test(\n",
    "    net,\n",
    "    dataloaders=[\n",
    "        dl_train2,\n",
    "        dl_val2,\n",
    "        dl_test2,\n",
    "        dl_OOD,\n",
    "    ],\n",
    "    verbose=False\n",
    ")\n",
    "rs = rename_pl_test_results(rs1, [\"train\", \"val\", \"test\", \"OOD\"])\n",
    "df_testing = pd.DataFrame(rs)\n",
    "df_testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Here we want to see if we can do a probe on the hidden states to see if it's lying...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "\n",
    "- see how acc each was for instructions vs truth\n",
    "- see how a linear probe trained on the diff can do for truth, vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    "    bnb=False,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.collect import manual_collect2\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "from src.eval.labels import LABEL_MAPPING\n",
    "from src.eval.ds import qc_ds, ds2df, qc_dsdf\n",
    "from src.helpers.torch_helpers import batch_to_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for single process DEBUGING\n",
    "# from src.eval.collect import generate_batches\n",
    "# o = next(iter(generate_batches(dl_OOD, model)))\n",
    "\n",
    "collection_layers = cfg.collection_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=Path(trainer.log_dir)/'hidden_states'\n",
    "ds_out_OOD, f = manual_collect2(dl_OOD, model, dataset_name=\"OOD\", layers=collection_layers, dataset_dir=dataset_dir)\n",
    "ds_out_valtest, f = manual_collect2(dl_valtest2, model, dataset_name=\"valtest\", layers=collection_layers, dataset_dir=dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dfres2_pretty(styler):\n",
    "    styler.set_caption(\"Dataset metrics\")\n",
    "    styler.background_gradient(axis=1, vmin=0, vmax=1, cmap=\"RdYlGn\", \n",
    "                               subset=['auroc', 'lie_auroc', 'known_lie_auroc', 'choice_cov']\n",
    "                               )\n",
    "    styler.background_gradient(axis=1, vmin=0, vmax=0.5, cmap=\"RdYlGn\", \n",
    "                               subset=['balance']\n",
    "                               )\n",
    "    return styler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ds2df(ds_out_valtest)\n",
    "df_b = df1.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "res_b = qc_dsdf(df_b)\n",
    "df_a = df1.rename(columns=lambda x: x.replace(\"_adapt\", \"\")).copy()\n",
    "res_a = qc_dsdf(df_a)\n",
    "df_res_ab = pd.DataFrame([res_b, res_a], index=[\"base\", \"adapter\"])\n",
    "print(\"🥉 secondary metric: dataset quality: performance of base model and adapter\")\n",
    "display(df_res_ab.style.pipe(make_dfres2_pretty))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ds2df(ds_out_OOD)\n",
    "df_b = df1.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "res_b = qc_dsdf(df_b)\n",
    "df_a = df1.rename(columns=lambda x: x.replace(\"_adapt\", \"\")).copy()\n",
    "res_a = qc_dsdf(df_a)\n",
    "df_res_ab = pd.DataFrame([res_b, res_a], index=[\"base\", \"adapter\"])\n",
    "print(\"🥉 secondary metric: dataset quality: performance of base model and adapter\")\n",
    "display(df_res_ab.style.pipe(make_dfres2_pretty))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: code from 10_compare probes\n",
    "from src.eval.interventions import test_intervention_quality2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_intervention_quality2(ds_known, label_fn, title=\"\", skip=0, stride=1, model_kwargs={}):\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyse_intervention(ds_out, cfg, model_kwargs={}):\n",
    "    ds_known = filter_ds_to_known(ds_out, verbose=True)\n",
    "\n",
    "    print(\n",
    "        f\"🥇 primary metric: predictive power (of logistic regression on top of intervened hidden states of known question)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    The roc_auc should go up on the right given the intervented states\n",
    "    \"\"\"\n",
    "    )\n",
    "    for label_name, label_fn in LABEL_MAPPING.items():\n",
    "        try:\n",
    "            # fit probe\n",
    "            # print('='*80)\n",
    "            # print(f\"predicting label={label_name}\")\n",
    "            df_res = test_intervention_quality2(ds_known, label_fn, title=f\"predicting label={label_name}\",\n",
    "                                                skip=cfg.skip_layers, stride=cfg.stride_layers, model_kwargs=model_kwargs)\n",
    "            display(df_res)\n",
    "        except Exception as e:\n",
    "            raise\n",
    "            print(f\"Exception {e}\")\n",
    "\n",
    "    # df1 = ds2df(ds_out)\n",
    "    # df_b = df1.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "    # res_b = qc_dsdf(df_b)\n",
    "    # df_a = df1.rename(columns=lambda x: x.replace(\"_adapt\", \"\")).copy()\n",
    "    # res_a = qc_dsdf(df_a)\n",
    "    # df_res_ab = pd.DataFrame([res_b, res_a], index=[\"base\", \"adapter\"])\n",
    "    # print(\"🥉 secondary metric: dataset quality: performance of base model and adapter\")\n",
    "    # display(df_res_ab.style.pipe(make_dfres2_pretty))\n",
    "    # return df_res_ab, df_res\n",
    "\n",
    "# analyse_intervention(ds_out_OOD, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"valtest\")\n",
    "df_res_ab_v, df_res_v = analyse_intervention(ds_out_valtest, cfg)\n",
    "\n",
    "print(\"out of distribution\")\n",
    "df_res_ab_o, df_res_o = analyse_intervention(ds_out_OOD, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: Probes on adapter are better than either probes or adapters.\n",
    "\n",
    "|model| val acc | OOD acc |\n",
    "|--|--|--|\n",
    "|base model  acc | 0.64  | 0.69 OOD |\n",
    "|adapter acc | 0.65  | 0.65 |\n",
    "|base+probe model residual auroc | 0.89 | 0.917|\n",
    "|adapter+probe residual auroc | **0.905** | **0.974** |\n",
    "\n",
    "So yes! Hypothesis confirmed\n",
    "mm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot labels vs each other\n",
    "\n",
    "to try and see why ranking is better\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
