{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    "# )\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset, load_preproc_datasets\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 2\n",
    "device = \"cuda:0\"\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    max_examples=(1600, 1600),\n",
    "    # model=\"wassname/phi-1_5-w_hidden_states\",\n",
    "    # batch_size=3,\n",
    "    # model=\"wassname/phi-2-w_hidden_states\",\n",
    "    model=\"microsoft/phi-2\",\n",
    "    # model=\"microsoft/phi-1_5\",\n",
    "    # model=\"Walmart-the-bag/phi-2-uncensored\",\n",
    "    batch_size=1,\n",
    "    prompt_format=\"phi\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf44a2d9cde41f783668bba47679b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    model_class=PhiForCausalLMWHS, # ti add hidden states\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,703,360 || all params: 2,782,387,200 || trainable%: 0.09715973391481962\n"
     ]
    }
   ],
   "source": [
    "# TODO I would like to only have biases, but for now lets just try a very small intervention on the last parts of a layer...\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=[\n",
    "        \"out_proj\",\n",
    "        \"mlp.fc2\",\n",
    "        \n",
    "        # \"mlp.fc1\",\n",
    "        \"Wqkv\",\n",
    "        # 'inner_attn',\n",
    "        # 'inner_cross_attn',\n",
    "    ],  # only the layers that go directly to the residual\n",
    "    # bias=\"lora_only\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=3,\n",
    "    lora_alpha=6,\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# peft_config = IA3Config(\n",
    "#     task_type=TaskType.SEQ_CLS, target_modules=[ \"out_proj\",\n",
    "#         \"mlp.fc2\",], feedforward_modules=[\"out_proj\", \"mlp.fc2\",]\n",
    "# )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(cfg.datasets).intersection(cfg.datasets_ood))==0, \"datasets overlap\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8086bfb5d714cdf838f5867b3fefe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "\u001b[32m2023-12-28 08:11:37.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_prompts\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1mExtracting 11 variants of each prompt\u001b[0m\n",
      "2023-12-28T08:11:37.608252+0800 INFO Extracting 11 variants of each prompt\n",
      "\u001b[32m2023-12-28 08:15:37.278\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1msetting tokenizer chat template to phi\u001b[0m\n",
      "2023-12-28T08:15:37.278799+0800 INFO setting tokenizer chat template to phi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4878f619725a46a9b58c60bfecde91ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "format_prompt:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2521807f41e7491b801eb24c3faabf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenize:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777d8133523b4fd7beafa6905305f90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "truncated:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec6a910107d41ea89f5cbe4f2cc54f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "truncated:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3b7fe587fc424a8542f0582208e456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompt_truncated:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ae95e0b3f947bfbd4324059bb0421c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "choice_ids:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-28 08:15:43.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 397.0 for amazon_polarity. max_length=776\u001b[0m\n",
      "2023-12-28T08:15:43.787190+0800 INFO median token length: 397.0 for amazon_polarity. max_length=776\n",
      "\u001b[32m2023-12-28 08:15:43.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 0.00% on amazon_polarity\u001b[0m\n",
      "2023-12-28T08:15:43.788711+0800 INFO truncation rate: 0.00% on amazon_polarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a766ef40754c3aa0babd83f2ec36ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Column name sys_instr_name_base not in the dataset. Current columns in the dataset: ['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m N \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(cfg\u001b[39m.\u001b[39mmax_examples)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ds_tokens \u001b[39m=\u001b[39m load_preproc_datasets(\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cfg\u001b[39m.\u001b[39;49mdatasets,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     N\u001b[39m=\u001b[39;49mN,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     seed\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     num_shots\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mnum_shots,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     prompt_format\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mprompt_format,\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/05_mjc_lie.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ds_tokens\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/prompts/prompt_loading.py:316\u001b[0m, in \u001b[0;36mload_preproc_datasets\u001b[0;34m(dataset_names, tokenizer, N, prompt_format, split_type, seed, num_shots, max_length)\u001b[0m\n\u001b[1;32m    314\u001b[0m n \u001b[39m=\u001b[39m N\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(dataset_names)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[39mfor\u001b[39;00m ds_name \u001b[39min\u001b[39;00m dataset_names:\n\u001b[0;32m--> 316\u001b[0m     ds_tokens1 \u001b[39m=\u001b[39m load_preproc_dataset(\n\u001b[1;32m    317\u001b[0m         ds_name,\n\u001b[1;32m    318\u001b[0m         tokenizer,\n\u001b[1;32m    319\u001b[0m         N\u001b[39m=\u001b[39;49mn,\n\u001b[1;32m    320\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    321\u001b[0m         num_shots\u001b[39m=\u001b[39;49mnum_shots,\n\u001b[1;32m    322\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    323\u001b[0m         prompt_format\u001b[39m=\u001b[39;49mprompt_format,\n\u001b[1;32m    324\u001b[0m     )\u001b[39m.\u001b[39mwith_format(\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    325\u001b[0m     datasets2\u001b[39m.\u001b[39mappend(ds_tokens1)\n\u001b[1;32m    326\u001b[0m ds_tokens \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39minterleave_datasets(datasets2, seed\u001b[39m=\u001b[39mseed)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/prompts/prompt_loading.py:398\u001b[0m, in \u001b[0;36mload_preproc_dataset\u001b[0;34m(ds_name, tokenizer, N, prompt_format, split_type, seed, num_shots, max_length)\u001b[0m\n\u001b[1;32m    395\u001b[0m ds_tokens \u001b[39m=\u001b[39m ds_tokens\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m r: r[\u001b[39m\"\u001b[39m\u001b[39mtruncated\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    397\u001b[0m \u001b[39m# print('num_rows', ds_tokens.num_rows)\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m ds_tokens \u001b[39m=\u001b[39m shuffle_dataset_by(ds_tokens, \u001b[39m'\u001b[39;49m\u001b[39mexample_i\u001b[39;49m\u001b[39m'\u001b[39;49m, rng\u001b[39m=\u001b[39;49mrng)\n\u001b[1;32m    399\u001b[0m \u001b[39m# print('num_rows', ds_tokens.num_rows)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[39m# ## Filter out truncated examples\u001b[39;00m\n\u001b[1;32m    402\u001b[0m ds_tokens \u001b[39m=\u001b[39m ds_tokens\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m r: \u001b[39mnot\u001b[39;00m r[\u001b[39m'\u001b[39m\u001b[39mtruncated\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/helpers/ds.py:30\u001b[0m, in \u001b[0;36mshuffle_dataset_by\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_dataset_by\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     29\u001b[0m ):\n\u001b[0;32m---> 30\u001b[0m     ds_train, ds_test \u001b[39m=\u001b[39m train_test_split_ds(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     31\u001b[0m     ds_out \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mconcatenate_datasets([ds_train, ds_test])\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m ds_out\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/helpers/ds.py:42\u001b[0m, in \u001b[0;36mtrain_test_split_ds\u001b[0;34m(ds, target, stratify_columns, random_state, test_size, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_test_split_ds\u001b[39m(\n\u001b[1;32m     36\u001b[0m     ds: Dataset,\n\u001b[1;32m     37\u001b[0m     target: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlabel_true_base\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m     stratify_columns: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39msys_instr_name_base\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     39\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     40\u001b[0m ):\n\u001b[1;32m     41\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m---> 42\u001b[0m         ds\u001b[39m.\u001b[39;49mselect_columns([target] \u001b[39m+\u001b[39;49m stratify_columns)\u001b[39m.\u001b[39mwith_format(\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     )\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m     44\u001b[0m     splitter \u001b[39m=\u001b[39m StratifiedShuffleSplit(random_state\u001b[39m=\u001b[39mrandom_state, test_size\u001b[39m=\u001b[39mtest_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     45\u001b[0m     train_indices, test_indices \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\n\u001b[1;32m     46\u001b[0m         splitter\u001b[39m.\u001b[39msplit(df, df[target], groups\u001b[39m=\u001b[39mdf[stratify_columns])\n\u001b[1;32m     47\u001b[0m     )\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/fingerprint.py:511\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    509\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m out \u001b[39m=\u001b[39m func(dataset, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    513\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:2339\u001b[0m, in \u001b[0;36mDataset.select_columns\u001b[0;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[39mfor\u001b[39;00m column_name \u001b[39min\u001b[39;00m column_names:\n\u001b[1;32m   2338\u001b[0m     \u001b[39mif\u001b[39;00m column_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names:\n\u001b[0;32m-> 2339\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2340\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn name \u001b[39m\u001b[39m{\u001b[39;00mcolumn_name\u001b[39m}\u001b[39;00m\u001b[39m not in the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2341\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mdataset. Current columns in the dataset: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2342\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mcolumn_names\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2343\u001b[0m         )\n\u001b[1;32m   2345\u001b[0m dataset \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n\u001b[1;32m   2346\u001b[0m dataset\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39mselect(column_names)\n",
      "\u001b[0;31mValueError\u001b[0m: Column name sys_instr_name_base not in the dataset. Current columns in the dataset: ['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids']."
     ]
    }
   ],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_tokens = load_preproc_datasets(\n",
    "    cfg.datasets,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tokens2 = load_preproc_datasets(\n",
    "    cfg.datasets_ood,\n",
    "    tokenizer,\n",
    "    N=N // 2,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "class AtapterFinetunerLie(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        simply train it to lie\n",
    "        \"\"\"\n",
    "\n",
    "        log_probs_a = torch.log_softmax(out_a[\"logits\"][:, -1,], -1,)\n",
    "\n",
    "        # batch['instructed_to_lie']\n",
    "        lie_label = ~batch['label_true']\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long())\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long())\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "\n",
    "class AtapterFinetunerToldToLie(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        train it to lie when instructed\n",
    "        \"\"\"\n",
    "\n",
    "        end_logits = out_a[\"logits\"][\n",
    "            :,\n",
    "            -1,\n",
    "        ]\n",
    "        log_probs_a = torch.log_softmax(end_logits, -1)\n",
    "\n",
    "        lie_label = batch[\"label_true\"] ^ batch[\"instructed_to_lie\"]\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long())\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long())\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "\n",
    "class AtapterFinetunerTruth(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        train it to lie when instructed\n",
    "        \"\"\"\n",
    "\n",
    "        end_logits = out_a[\"logits\"][\n",
    "            :,\n",
    "            -1,\n",
    "        ]\n",
    "        log_probs_a = torch.log_softmax(end_logits, -1)\n",
    "\n",
    "        lie_label = batch[\"label_true\"] #^ batch[\"instructed_to_lie\"]\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long())\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long())\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = AtapterFinetunerToldToLie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b[\"input_ids\"].shape)\n",
    "c_in = b[\"input_ids\"].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model_cls(\n",
    "    model, tokenizer, lr=5e-3, weight_decay=1e-5, total_steps=len(dl_train) * max_epochs\n",
    ")\n",
    "\n",
    "print(c_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.training_step(b, None)\n",
    "# o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.predict_step(b, None)\n",
    "# o.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to init lightning early, so it inits accelerate\n",
    "trainer1 = pl.Trainer(\n",
    "    gradient_clip_val=20,\n",
    "    devices=\"1\",\n",
    "    accelerator=\"gpu\",\n",
    "    accumulate_grad_batches=8,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    # enable_model_summary=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(trainer1.log_dir) / \"final\"\n",
    "model.save_pretrained(checkpoint_path)\n",
    "checkpoint_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv\n",
    "\n",
    "df_histe, df_hist = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "df_hist[[\"train/loss_step\", \"val/loss_step\"]].plot(style=\".\")\n",
    "df_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_histe[[\"train/loss_step\", \"val/loss_step\"]].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n",
    "\n",
    "This acts a QC to check of the trained adapter is still coherent while giving the opposite answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.gen import gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We need to reload it from checkpoint, since lightning seems to bug it after running\n",
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose a row where we will see the difference\n",
    "mask = (\n",
    "    (ds_tokens['instructed_to_lie']==True) &\n",
    "    (ds_tokens['label_true']==False)\n",
    ").float()\n",
    "bi = mask.argmax().item()\n",
    "\n",
    "# TODO doesn't work if the model gets it wrong\n",
    "inputs = ds_tokens.with_format(\"torch\")[bi]\n",
    "inputs['instructed_to_lie'], inputs['label_true']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.disable_adapter():\n",
    "    gen(model, inputs, tokenizer)\n",
    "\n",
    "gen(model, inputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.interventions import test_intervention_quality2\n",
    "from src.eval.labels import ds2label_model_obey, ds2label_model_truth\n",
    "\n",
    "TEST_BATCH_MULT = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2 = DeceptionDataModule(ds_tokens2, batch_size=cfg.batch_size * TEST_BATCH_MULT)\n",
    "dl_train2 = dm2.train_dataloader()\n",
    "dl_train2.shuffle = False\n",
    "\n",
    "dl_val2 = dm2.val_dataloader()\n",
    "dl_test2 = dm2.test_dataloader()\n",
    "\n",
    "dl_valtest2 = DataLoader(\n",
    "    torch.utils.data.ConcatDataset([dm.datasets[\"val\"], dm.datasets[\"test\"]]),\n",
    "    batch_size=cfg.batch_size * TEST_BATCH_MULT,\n",
    ")\n",
    "len(dl_valtest2.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_OOD = DataLoader(\n",
    "    ds_tokens2, batch_size=cfg.batch_size * TEST_BATCH_MULT, drop_last=False, shuffle=False\n",
    ")\n",
    "len(dl_OOD.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    ")\n",
    "net = model_cls(model, tokenizer)\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import rename_pl_test_results\n",
    "\n",
    "rs1 = trainer1.test(\n",
    "    net,\n",
    "    dataloaders=[\n",
    "        dl_train2,\n",
    "        dl_val2,\n",
    "        dl_test2,\n",
    "        dl_OOD,\n",
    "    ],\n",
    "    verbose=False\n",
    ")\n",
    "rs = rename_pl_test_results(rs1, [\"train\", \"val\", \"test\", \"OOD\"])\n",
    "df_testing = pd.DataFrame(rs)\n",
    "df_testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Here we want to see if we can do a probe on the hidden states to see if it's lying...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "\n",
    "- see how acc each was for instructions vs truth\n",
    "- see how a linear probe trained on the diff can do for truth, vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.collect import manual_collect2\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "from src.eval.labels import LABEL_MAPPING\n",
    "from src.eval.ds import qc_ds, ds2df, qc_dsdf\n",
    "from src.helpers.torch_helpers import batch_to_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for single process DEBUGING\n",
    "# from src.eval.collect import generate_batches\n",
    "# o = next(iter(generate_batches(dl_OOD, model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out_OOD, f = manual_collect2(dl_OOD, model, dataset_name=\"OOD\")\n",
    "ds_out_valtest, f = manual_collect2(dl_valtest2, model, dataset_name=\"valtest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dfres2_pretty(styler):\n",
    "    styler.set_caption(\"Dataset metrics\")\n",
    "    styler.background_gradient(axis=1, vmin=0, vmax=1, cmap=\"RdYlGn\", \n",
    "                               subset=['auroc', 'lie_auroc', 'known_lie_auroc', 'choice_cov']\n",
    "                               )\n",
    "    styler.background_gradient(axis=1, vmin=0, vmax=0.5, cmap=\"RdYlGn\", \n",
    "                               subset=['balance']\n",
    "                               )\n",
    "    return styler\n",
    "\n",
    "\n",
    "def analyse_intervention(ds_out, cfg, model_kwargs={}):\n",
    "    ds_known = filter_ds_to_known(ds_out, verbose=True)\n",
    "\n",
    "    print(\n",
    "        f\"🥇 primary metric: predictive power (of logistic regression on top of intervened hidden states of known question)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    The roc_auc should go up on the right given the intervented states\n",
    "    \"\"\"\n",
    "    )\n",
    "    for label_name, label_fn in LABEL_MAPPING.items():\n",
    "        try:\n",
    "            # fit probe\n",
    "            # print('='*80)\n",
    "            # print(f\"predicting label={label_name}\")\n",
    "            df_res = test_intervention_quality2(ds_known, label_fn, title=f\"predicting label={label_name}\",\n",
    "                                                skip=cfg.skip_layers, stride=cfg.stride_layers, model_kwargs=model_kwargs)\n",
    "            display(df_res)\n",
    "        except Exception as e:\n",
    "            raise\n",
    "            print(f\"Exception {e}\")\n",
    "\n",
    "    df1 = ds2df(ds_out)\n",
    "    df_b = df1.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "    res_b = qc_dsdf(df_b)\n",
    "    df_a = df1.rename(columns=lambda x: x.replace(\"_adapt\", \"\")).copy()\n",
    "    res_a = qc_dsdf(df_a)\n",
    "    df_res_ab = pd.DataFrame([res_b, res_a], index=[\"base\", \"adapter\"])\n",
    "    print(\"🥉 secondary metric: dataset quality: performance of base model and adapter\")\n",
    "    display(df_res_ab.style.pipe(make_dfres2_pretty))\n",
    "    return df_res_ab, df_res\n",
    "\n",
    "# analyse_intervention(ds_out_OOD, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"valtest\")\n",
    "df_res_ab_v, df_res_v = analyse_intervention(ds_out_valtest, cfg)\n",
    "\n",
    "print(\"out of distribution\")\n",
    "df_res_ab_o, df_res_o = analyse_intervention(ds_out_OOD, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: Probes on adapter are better than either probes or adapters.\n",
    "\n",
    "|model| val acc | OOD acc |\n",
    "|--|--|--|\n",
    "|base model  acc | 0.64  | 0.69 OOD |\n",
    "|adapter acc | 0.65  | 0.65 |\n",
    "|base+probe model residual auroc | 0.89 | 0.917|\n",
    "|adapter+probe residual auroc | **0.905** | **0.974** |\n",
    "\n",
    "So yes! Hypothesis confirmed\n",
    "mm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot labels vs each other\n",
    "\n",
    "to try and see why ranking is better\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
