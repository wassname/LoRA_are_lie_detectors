{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    "# )\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset, load_preproc_datasets\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 3\n",
    "device = \"cuda:0\"\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    max_examples=(1000, 1000),\n",
    "    # model=\"wassname/phi-1_5-w_hidden_states\",\n",
    "    # batch_size=3,\n",
    "    # model=\"wassname/phi-2-w_hidden_states\",\n",
    "    model=\"microsoft/phi-2\",\n",
    "    # model=\"microsoft/phi-1_5\",\n",
    "    # model=\"Walmart-the-bag/phi-2-uncensored\",\n",
    "    batch_size=1,\n",
    "    prompt_format=\"phi\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461078f558864351a707837b3c3baadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    trust_remote_code=True,\n",
    "    # model_class=PhiForCausalLMWHS, # ti add hidden states\n",
    "    # bnb=False,\n",
    ")\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 573,440 || all params: 2,780,257,280 || trainable%: 0.020625429312786478\n"
     ]
    }
   ],
   "source": [
    "# for normal seetings see https://github.com/huggingface/peft/blob/cf04d0353f0343cbf66627228c4495f51669af34/src/peft/utils/constants.py#L81\n",
    "# and https://github.com/huggingface/peft/blob/cf04d0353f0343cbf66627228c4495f51669af34/src/peft/utils/constants.py#L102\n",
    "# \"llama\": [\"k_proj\", \"v_proj\", \"down_proj\"],\n",
    "# \"gptj\": [\"q_proj\", \"v_proj\", \"fc_out\"],\n",
    "# \"falcon\": [\"query_key_value\", \"dense_4h_to_h\"],\n",
    "\n",
    "# for activation gathering\n",
    "# note ia3 would usually be on fc2, but it would be set as feedforward, meaning it's on the inputs to fc2\n",
    "# but to make collection easier we do everything on the outputs. So we use the outputs of the previous layer (fc1) instead of the inputs of fc2\n",
    "peft_config = IA3Config(\n",
    "    # task_type=TaskType.SEQ_CLS, \n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[ \"fc1\",  \"Wqkv\",], \n",
    "        feedforward_modules=[],\n",
    "        inference_mode=False,\n",
    ")\n",
    "# peft_config = IA3Config(\n",
    "#     task_type=TaskType.SEQ_CLS, es=[ \"fc1\", \"fc2\", \"Wqkv\",\"out_proj\"], \n",
    "#         feedforward_modules=[\"fc2\",\"out_proj\", \"fc1\"]\n",
    "# )\n",
    "\n",
    "# peft_config = IA3Config(\n",
    "#     task_type=TaskType.SEQ_CLS, target_modules=[  \"fc2\", \"out_proj\"], \n",
    "#         feedforward_modules=[]\n",
    "# )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.transformer.h.0.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.0.mlp.fc1',\n",
       " 'base_model.model.transformer.h.1.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.1.mlp.fc1',\n",
       " 'base_model.model.transformer.h.10.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.10.mlp.fc1',\n",
       " 'base_model.model.transformer.h.11.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.11.mlp.fc1',\n",
       " 'base_model.model.transformer.h.12.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.12.mlp.fc1',\n",
       " 'base_model.model.transformer.h.13.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.13.mlp.fc1',\n",
       " 'base_model.model.transformer.h.14.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.14.mlp.fc1',\n",
       " 'base_model.model.transformer.h.15.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.15.mlp.fc1',\n",
       " 'base_model.model.transformer.h.16.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.16.mlp.fc1',\n",
       " 'base_model.model.transformer.h.17.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.17.mlp.fc1',\n",
       " 'base_model.model.transformer.h.18.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.18.mlp.fc1',\n",
       " 'base_model.model.transformer.h.19.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.19.mlp.fc1',\n",
       " 'base_model.model.transformer.h.2.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.2.mlp.fc1',\n",
       " 'base_model.model.transformer.h.20.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.20.mlp.fc1',\n",
       " 'base_model.model.transformer.h.21.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.21.mlp.fc1',\n",
       " 'base_model.model.transformer.h.22.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.22.mlp.fc1',\n",
       " 'base_model.model.transformer.h.23.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.23.mlp.fc1',\n",
       " 'base_model.model.transformer.h.24.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.24.mlp.fc1',\n",
       " 'base_model.model.transformer.h.25.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.25.mlp.fc1',\n",
       " 'base_model.model.transformer.h.26.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.26.mlp.fc1',\n",
       " 'base_model.model.transformer.h.27.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.27.mlp.fc1',\n",
       " 'base_model.model.transformer.h.28.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.28.mlp.fc1',\n",
       " 'base_model.model.transformer.h.29.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.29.mlp.fc1',\n",
       " 'base_model.model.transformer.h.3.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.3.mlp.fc1',\n",
       " 'base_model.model.transformer.h.30.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.30.mlp.fc1',\n",
       " 'base_model.model.transformer.h.31.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.31.mlp.fc1',\n",
       " 'base_model.model.transformer.h.4.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.4.mlp.fc1',\n",
       " 'base_model.model.transformer.h.5.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.5.mlp.fc1',\n",
       " 'base_model.model.transformer.h.6.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.6.mlp.fc1',\n",
       " 'base_model.model.transformer.h.7.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.7.mlp.fc1',\n",
       " 'base_model.model.transformer.h.8.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.8.mlp.fc1',\n",
       " 'base_model.model.transformer.h.9.mixer.Wqkv',\n",
       " 'base_model.model.transformer.h.9.mlp.fc1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME, find the layer names using the IA3 config\n",
    "import itertools\n",
    "module_names = [key for key, _ in model.named_modules()]\n",
    "collection_layers = []\n",
    "for pattern in peft_config.target_modules:\n",
    "    collection_layers.extend([key for key in module_names if key.endswith(pattern)])\n",
    "collection_layers = sorted(collection_layers)\n",
    "collection_layers\n",
    "                           \n",
    "\n",
    "# see also how peft does regexp to layers https://github.dev/huggingface/peft/blob/cf04d0353f0343cbf66627228c4495f51669af34/src/peft/tuners/tuners_utils.py#L205\n",
    "# target_name_key = next(filter(lambda key: re.match(f\"(.*\\.)?{key}$\", current_key), pattern_keys), target_name)\n",
    "\n",
    "\n",
    "# collection_layers = cfg.collection_layers\n",
    "# target_module_found = any(key.endswith(target_key) for target_key in model.modules_to_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): IA3Model(\n",
       "    (model): PhiForCausalLM(\n",
       "      (transformer): PhiModel(\n",
       "        (embd): Embedding(\n",
       "          (wte): Embedding(51200, 2560)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (h): ModuleList(\n",
       "          (0-31): 32 x ParallelBlock(\n",
       "            (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (mixer): MHA(\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (Wqkv): ia3.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=7680, bias=True)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 7680x1 (cuda:0)])\n",
       "              )\n",
       "              (out_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "              (inner_attn): SelfAttention(\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (inner_cross_attn): CrossAttention(\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (mlp): MLP(\n",
       "              (fc1): ia3.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 10240x1 (cuda:0)])\n",
       "              )\n",
       "              (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): CausalLMHead(\n",
       "        (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "      )\n",
       "      (loss): CausalLMLoss(\n",
       "        (loss_fct): CrossEntropyLoss()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(cfg.datasets).intersection(cfg.datasets_ood))==0, \"datasets overlap\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:26:55.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1msetting tokenizer chat template to phi\u001b[0m\n",
      "2023-12-31T17:26:55.018580+0800 INFO setting tokenizer chat template to phi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541926dc2b6446d19943dce0677388c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompt_truncated:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4956576ec24d098d47fbbde419ffe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "choice_ids:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:26:57.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 398.0 for amazon_polarity. max_length=776\u001b[0m\n",
      "2023-12-31T17:26:57.704529+0800 INFO median token length: 398.0 for amazon_polarity. max_length=776\n",
      "\u001b[32m2023-12-31 17:26:57.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 0.00% on amazon_polarity\u001b[0m\n",
      "2023-12-31T17:26:57.705537+0800 INFO truncation rate: 0.00% on amazon_polarity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9411370b9f6b40d0a45a518bb4035f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c95aa88dc443e0a6cb550776110373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:26:58.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1204=>1204\u001b[0m\n",
      "2023-12-31T17:26:58.696858+0800 INFO num_rows (after filtering out truncated rows) 1204=>1204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34c9a9a0fbe44c095a48df0161eac4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompt_truncated:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2e8514a7364d4788daade429509a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "choice_ids:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:01.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 253.5 for glue:qnli. max_length=776\u001b[0m\n",
      "2023-12-31T17:27:01.348412+0800 INFO median token length: 253.5 for glue:qnli. max_length=776\n",
      "\u001b[32m2023-12-31 17:27:01.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 0.00% on glue:qnli\u001b[0m\n",
      "2023-12-31T17:27:01.349438+0800 INFO truncation rate: 0.00% on glue:qnli\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8af706a30fc4f758e272f6b5300a87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacbb77e774340718573c2cdca243051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:02.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1204=>1204\u001b[0m\n",
      "2023-12-31T17:27:02.324015+0800 INFO num_rows (after filtering out truncated rows) 1204=>1204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea5f1b6c5e94ef8a4c5ae678007e4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompt_truncated:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968e43c6ab5a40c0b2799f5d9c3d9b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "choice_ids:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:05.020\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 285.0 for super_glue:rte. max_length=776\u001b[0m\n",
      "2023-12-31T17:27:05.020349+0800 INFO median token length: 285.0 for super_glue:rte. max_length=776\n",
      "\u001b[32m2023-12-31 17:27:05.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 0.00% on super_glue:rte\u001b[0m\n",
      "2023-12-31T17:27:05.021336+0800 INFO truncation rate: 0.00% on super_glue:rte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d951dba142459fb0d04ade19f1d405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3122b2d827814de990f60a67a803bae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:06.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1204=>1204\u001b[0m\n",
      "2023-12-31T17:27:06.183149+0800 INFO num_rows (after filtering out truncated rows) 1204=>1204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41397b6fc81b46d0a0fe8706fad34477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompt_truncated:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a865bcb1514c44069e0484dd6669ddfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "choice_ids:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:08.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 122.0 for sst2. max_length=776\u001b[0m\n",
      "2023-12-31T17:27:08.853708+0800 INFO median token length: 122.0 for sst2. max_length=776\n",
      "\u001b[32m2023-12-31 17:27:08.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 0.00% on sst2\u001b[0m\n",
      "2023-12-31T17:27:08.854734+0800 INFO truncation rate: 0.00% on sst2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b509b49abf7e487d8505069a974a2dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411dfeb0360a415b81148be7197204cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:09.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1204=>1204\u001b[0m\n",
      "2023-12-31T17:27:09.798174+0800 INFO num_rows (after filtering out truncated rows) 1204=>1204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853a5dfe7b6541f4860432ca3f6c601d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompt_truncated:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be2241161b74473bd5039cc329bddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "choice_ids:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:12.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 134.0 for hans. max_length=776\u001b[0m\n",
      "2023-12-31T17:27:12.426749+0800 INFO median token length: 134.0 for hans. max_length=776\n",
      "\u001b[32m2023-12-31 17:27:12.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 0.00% on hans\u001b[0m\n",
      "2023-12-31T17:27:12.428122+0800 INFO truncation rate: 0.00% on hans\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f038ab0c28df4e7a98bf6c1948786fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187ebd6f418c4faaaf70c82793f2014b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:13.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1204=>1204\u001b[0m\n",
      "2023-12-31T17:27:13.374348+0800 INFO num_rows (after filtering out truncated rows) 1204=>1204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids'],\n",
       "    num_rows: 2005\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_tokens = load_preproc_datasets(\n",
    "    cfg.datasets,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5f95ca19f3447380b57e3e627adbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "format_prompt:   0%|          | 0/1004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f471ed88154e1fba00e49a5e67fab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenize:   0%|          | 0/1004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a88676d0f1408697079506c59f6db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "truncated:   0%|          | 0/1004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd4c91750754624a1193ab7ac1b7dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "truncated:   0%|          | 0/1004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bff5ba72d04b588ea7ce2196b2489e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompt_truncated:   0%|          | 0/1004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7412f4e263429aa6d519c81ca590c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "choice_ids:   0%|          | 0/1004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:17.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 484.0 for super_glue:boolq. max_length=776\u001b[0m\n",
      "2023-12-31T17:27:17.118606+0800 INFO median token length: 484.0 for super_glue:boolq. max_length=776\n",
      "\u001b[32m2023-12-31 17:27:17.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 2.49% on super_glue:boolq\u001b[0m\n",
      "2023-12-31T17:27:17.119597+0800 INFO truncation rate: 2.49% on super_glue:boolq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d294073c9e462a911cc30b6a495763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bb47fefcd44de781a305c02af6e576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/979 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-31 17:27:17.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1004=>979\u001b[0m\n",
      "2023-12-31T17:27:17.939527+0800 INFO num_rows (after filtering out truncated rows) 1004=>979\n",
      "\u001b[32m2023-12-31 17:27:18.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 167.5 for super_glue:axg. max_length=776\u001b[0m\n",
      "2023-12-31T17:27:18.005381+0800 INFO median token length: 167.5 for super_glue:axg. max_length=776\n",
      "\u001b[32m2023-12-31 17:27:18.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 0.00% on super_glue:axg\u001b[0m\n",
      "2023-12-31T17:27:18.006409+0800 INFO truncation rate: 0.00% on super_glue:axg\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "\u001b[32m2023-12-31 17:27:18.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 712=>712\u001b[0m\n",
      "2023-12-31T17:27:18.051012+0800 INFO num_rows (after filtering out truncated rows) 712=>712\n",
      "\u001b[32m2023-12-31 17:27:18.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m390\u001b[0m - \u001b[1mmedian token length: 776.0 for imdb. max_length=776\u001b[0m\n",
      "2023-12-31T17:27:18.120703+0800 INFO median token length: 776.0 for imdb. max_length=776\n",
      "\u001b[32m2023-12-31 17:27:18.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mtruncation rate: 63.35% on imdb\u001b[0m\n",
      "2023-12-31T17:27:18.122136+0800 INFO truncation rate: 63.35% on imdb\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "\u001b[32m2023-12-31 17:27:18.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1004=>368\u001b[0m\n",
      "2023-12-31T17:27:18.152464+0800 INFO num_rows (after filtering out truncated rows) 1004=>368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids'],\n",
       "    num_rows: 1002\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tokens2 = load_preproc_datasets(\n",
    "    cfg.datasets_ood,\n",
    "    tokenizer,\n",
    "    N=N // 2,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "class AtapterFinetunerLie(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        simply train it to lie\n",
    "        \"\"\"\n",
    "        device = out.logits.device\n",
    "\n",
    "        log_probs_a = torch.log_softmax(out_a[\"logits\"][:, -1,], -1,)\n",
    "\n",
    "        # batch['instructed_to_lie']\n",
    "        lie_label = ~batch['label_true']\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long()).to(device)\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long()).to(device)\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "\n",
    "class AtapterFinetunerToldToLie(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        train it to lie when instructed\n",
    "        \"\"\"\n",
    "        device = out.logits.device\n",
    "\n",
    "        end_logits = out_a[\"logits\"][\n",
    "            :,\n",
    "            -1,\n",
    "        ]\n",
    "        log_probs_a = torch.log_softmax(end_logits, -1)\n",
    "\n",
    "        lie_label = batch[\"label_true\"] ^ batch[\"instructed_to_lie\"]\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long()).to(device)\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long()).to(device)\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "from src.helpers.scores import select\n",
    "\n",
    "\n",
    "class AtapterFinetunerTruth(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        train it to lie when instructed\n",
    "        \"\"\"\n",
    "        device = out.logits.device\n",
    "\n",
    "        end_logits = out_a[\"logits\"][\n",
    "            :,\n",
    "            -1,\n",
    "        ]\n",
    "        log_probs_a = torch.log_softmax(end_logits, -1)\n",
    "\n",
    "        lie_label = batch[\"label_true\"] #^ batch[\"instructed_to_lie\"]\n",
    "        choice_ids1 = select(batch[\"choice_ids\"][:, :, 0], lie_label.long()).to(device)\n",
    "        choice_ids2 = select(batch[\"choice_ids\"][:, :, 1], lie_label.long()).to(device)\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)\n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cls = AtapterFinetunerToldToLie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1002, 501)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "len(dl_train), len(dl_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids']) torch.Size([1, 776])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b[\"input_ids\"].shape)\n",
    "c_in = b[\"input_ids\"].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n"
     ]
    }
   ],
   "source": [
    "net = model_cls(\n",
    "    model, tokenizer, lr=4e-3, weight_decay=0, total_steps=len(dl_train) * max_epochs, collection_layers=collection_layers\n",
    ")\n",
    "\n",
    "print(c_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# # net.half()\n",
    "# with torch.no_grad():\n",
    "#     o = net.training_step(b, None)\n",
    "# o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.predict_step(b, None)\n",
    "# o.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type                 | Params\n",
      "-----------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 1.5 B \n",
      "-----------------------------------------------\n",
      "573 K     Trainable params\n",
      "1.5 B     Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,087.864 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861a064a88304589a768211837dd4c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07c6b2cf078445fa517857ace75dfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c94acb84d04eae9f45482bf3406cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4ca7ee1c6d4afc82bb8be66e8ede6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    precision='16-mixed',\n",
    "\n",
    "    # gradient_clip_val=20,\n",
    "    devices=\"1\",\n",
    "    accelerator=\"gpu\",\n",
    "    accumulate_grad_batches=8,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    # plugins=precision,\n",
    "    # enable_model_summary=False,\n",
    ")\n",
    "trainer.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(trainer.log_dir) / \"checkpoint_last\"\n",
    "model.save_pretrained(checkpoint_path)\n",
    "checkpoint_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save config\n",
    "f = Path(trainer.log_dir) / 'config.yaml'\n",
    "cfg.save_yaml(f)\n",
    "f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffixes = list(set([c.split('/')[-1] for c in df_hist.columns if '/' in c]))\n",
    "# for suffix in suffixes:\n",
    "#     print(suffix)\n",
    "#     df_hist[[c for c in df_hist.columns if c.endswith(suffix) and '/' in c]].plot(title=suffix, style='.')\n",
    "#     plt.title(suffix)\n",
    "# plt.show()\n",
    "# df_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv, plot_hist\n",
    "\n",
    "df_histe, df_hist = read_metrics_csv(trainer.logger.experiment.metrics_file_path)\n",
    "# df_hist[[\"train/loss_step\", \"val/loss_step\"]].plot(style=\".\")\n",
    "plot_hist(df_hist)\n",
    "plot_hist(df_histe)\n",
    "df_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n",
    "\n",
    "This acts a QC to check of the trained adapter is still coherent while giving the opposite answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.gen import gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We need to reload it from checkpoint, since lightning seems to bug it after running\n",
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    # model_class=PhiForCausalLMWHS,\n",
    "    bnb=False,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.half()\n",
    ";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose a row where we will see the difference\n",
    "\n",
    "from src.eval.ds import ds2df\n",
    "df_tokens = ds2df(ds_tokens).reset_index()\n",
    "mask = (\n",
    "    (df_tokens['instructed_to_lie']==True) &\n",
    "    (df_tokens['label_true']==False)\n",
    ")\n",
    "bis = df_tokens[mask].index\n",
    "\n",
    "\n",
    "# # mask = (\n",
    "# #     (ds_tokens['instructed_to_lie']==True) &\n",
    "# #     (ds_tokens['label_true']==False)\n",
    "# # ).float()\n",
    "bi = int(np.random.choice(bis))\n",
    "bi\n",
    "# # TODO doesn't work if the model gets it wrong\n",
    "inputs = ds_tokens.with_format(\"torch\")[bi]\n",
    "df_tokens.iloc[bi]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.disable_adapter():\n",
    "    gen(model, inputs, tokenizer)\n",
    "\n",
    "gen(model, inputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.labels import ds2label_model_obey, ds2label_model_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_MULT = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2 = DeceptionDataModule(ds_tokens2, batch_size=cfg.batch_size * TEST_BATCH_MULT)\n",
    "dl_train2 = dm2.train_dataloader()\n",
    "dl_train2.shuffle = False\n",
    "\n",
    "dl_val2 = dm2.val_dataloader()\n",
    "dl_test2 = dm2.test_dataloader()\n",
    "\n",
    "dl_valtest2 = DataLoader(\n",
    "    torch.utils.data.ConcatDataset([dm.datasets[\"val\"], dm.datasets[\"test\"]]),\n",
    "    batch_size=cfg.batch_size * TEST_BATCH_MULT,\n",
    ")\n",
    "len(dl_valtest2.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_OOD = DataLoader(\n",
    "    ds_tokens2, batch_size=cfg.batch_size * TEST_BATCH_MULT, drop_last=False, shuffle=False\n",
    ")\n",
    "len(dl_OOD.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    # model_class=PhiForCausalLMWHS,\n",
    ")\n",
    "net = model_cls(model, tokenizer)\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import rename_pl_test_results\n",
    "\n",
    "rs1 = trainer.test(\n",
    "    net,\n",
    "    dataloaders=[\n",
    "        dl_train2,\n",
    "        dl_val2,\n",
    "        dl_test2,\n",
    "        dl_OOD,\n",
    "    ],\n",
    "    verbose=False\n",
    ")\n",
    "rs = rename_pl_test_results(rs1, [\"train\", \"val\", \"test\", \"OOD\"])\n",
    "df_testing = pd.DataFrame(rs)\n",
    "df_testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Here we want to see if we can do a probe on the hidden states to see if it's lying...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "\n",
    "- see how acc each was for instructions vs truth\n",
    "- see how a linear probe trained on the diff can do for truth, vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,  # bfloat can't be pickled\n",
    "    # model_class=PhiForCausalLMWHS,\n",
    "    bnb=False,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.collect import manual_collect2\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "from src.eval.labels import LABEL_MAPPING\n",
    "from src.eval.ds import qc_ds, ds2df, qc_dsdf\n",
    "from src.helpers.torch_helpers import batch_to_device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for single process DEBUGING\n",
    "# from src.eval.collect import generate_batches\n",
    "# o = next(iter(generate_batches(dl_OOD, model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=Path(trainer.log_dir)/'hidden_states'\n",
    "ds_out_OOD, f = manual_collect2(dl_OOD, model, dataset_name=\"OOD\", layers=collection_layers, dataset_dir=dataset_dir)\n",
    "ds_out_valtest, f = manual_collect2(dl_valtest2, model, dataset_name=\"valtest\", layers=collection_layers, dataset_dir=dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dfres2_pretty(styler):\n",
    "    styler.set_caption(\"Dataset metrics\")\n",
    "    styler.background_gradient(axis=1, vmin=0, vmax=1, cmap=\"RdYlGn\", \n",
    "                               subset=['auroc', 'lie_auroc', 'known_lie_auroc', 'choice_cov']\n",
    "                               )\n",
    "    styler.background_gradient(axis=1, vmin=0, vmax=0.5, cmap=\"RdYlGn\", \n",
    "                               subset=['balance']\n",
    "                               )\n",
    "    return styler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ds2df(ds_out_valtest)\n",
    "df_b = df1.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "res_b = qc_dsdf(df_b)\n",
    "df_a = df1.rename(columns=lambda x: x.replace(\"_adapt\", \"\")).copy()\n",
    "res_a = qc_dsdf(df_a)\n",
    "df_res_ab = pd.DataFrame([res_b, res_a], index=[\"base\", \"adapter\"])\n",
    "print(\" secondary metric: dataset quality: performance of base model and adapter\")\n",
    "display(df_res_ab.style.pipe(make_dfres2_pretty))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ds2df(ds_out_OOD)\n",
    "df_b = df1.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "res_b = qc_dsdf(df_b)\n",
    "df_a = df1.rename(columns=lambda x: x.replace(\"_adapt\", \"\")).copy()\n",
    "res_a = qc_dsdf(df_a)\n",
    "df_res_ab = pd.DataFrame([res_b, res_a], index=[\"base\", \"adapter\"])\n",
    "print(\" secondary metric: dataset quality: performance of base model and adapter\")\n",
    "display(df_res_ab.style.pipe(make_dfres2_pretty))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: code from 10_compare probes\n",
    "from src.eval.interventions import test_intervention_quality2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_intervention_quality2(ds_known, label_fn, title=\"\", skip=0, stride=1, model_kwargs={}):\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def analyse_intervention(ds_out, cfg, model_kwargs={}):\n",
    "    ds_known = filter_ds_to_known(ds_out, verbose=True)\n",
    "\n",
    "    print(\n",
    "        f\" primary metric: predictive power (of logistic regression on top of intervened hidden states of known question)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    The roc_auc should go up on the right given the intervented states\n",
    "    \"\"\"\n",
    "    )\n",
    "    for label_name, label_fn in LABEL_MAPPING.items():\n",
    "        try:\n",
    "            # fit probe\n",
    "            # print('='*80)\n",
    "            # print(f\"predicting label={label_name}\")\n",
    "            df_res = test_intervention_quality2(ds_known, label_fn, title=f\"predicting label={label_name}\",\n",
    "                                                skip=cfg.skip_layers, stride=cfg.stride_layers, model_kwargs=model_kwargs)\n",
    "            display(df_res)\n",
    "        except Exception as e:\n",
    "            raise\n",
    "            print(f\"Exception {e}\")\n",
    "\n",
    "    # df1 = ds2df(ds_out)\n",
    "    # df_b = df1.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "    # res_b = qc_dsdf(df_b)\n",
    "    # df_a = df1.rename(columns=lambda x: x.replace(\"_adapt\", \"\")).copy()\n",
    "    # res_a = qc_dsdf(df_a)\n",
    "    # df_res_ab = pd.DataFrame([res_b, res_a], index=[\"base\", \"adapter\"])\n",
    "    # print(\" secondary metric: dataset quality: performance of base model and adapter\")\n",
    "    # display(df_res_ab.style.pipe(make_dfres2_pretty))\n",
    "    # return df_res_ab, df_res\n",
    "\n",
    "# analyse_intervention(ds_out_OOD, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"valtest\")\n",
    "df_res_ab_v, df_res_v = analyse_intervention(ds_out_valtest, cfg)\n",
    "\n",
    "print(\"out of distribution\")\n",
    "df_res_ab_o, df_res_o = analyse_intervention(ds_out_OOD, cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: Probes on adapter are better than either probes or adapters.\n",
    "\n",
    "|model| val acc | OOD acc |\n",
    "|--|--|--|\n",
    "|base model  acc | 0.64  | 0.69 OOD |\n",
    "|adapter acc | 0.65  | 0.65 |\n",
    "|base+probe model residual auroc | 0.89 | 0.917|\n",
    "|adapter+probe residual auroc | **0.905** | **0.974** |\n",
    "\n",
    "So yes! Hypothesis confirmed\n",
    "mm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot labels vs each other\n",
    "\n",
    "to try and see why ranking is better\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
