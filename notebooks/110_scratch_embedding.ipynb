{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had the idea of using a pretrained embedding layer, inverted, as the first layer of a decoder. Forcing the latent space to use it's structure. Ideally it's a compressed but human interpretable latent space. If it works it should help probes. first lets prototype the embedding\n",
    "\n",
    "see also\n",
    "- https://keras.io/api/keras_nlp/modeling_layers/reversible_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "from loguru import logger\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.llms.load import load_model\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(\"paper\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebc6c8707f64b1d8b38aecf46bf8e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    # cfg.model,\n",
    "    device='cpu',\n",
    "    bnb=False,\n",
    "    trust_remote_code=True,\n",
    "    # model_class=PhiForCausalLMWHS, # ti add hidden states\n",
    "    # bnb=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51200, 2560)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get embedding layer\n",
    "embedding = model.model.embed_tokens.eval().cpu()\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, hidden_dim = embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "# vocab_size = 100\n",
    "# hidden_dim = 32\n",
    "seq_length = 50\n",
    "\n",
    "# Generate random inputs.\n",
    "token_ids = torch.tensor(np.random.randint(vocab_size, size=(batch_size, seq_length)))\n",
    "\n",
    "# embedding = keras_nlp.layers.ReversibleEmbedding(vocab_size, hidden_dim)\n",
    "\n",
    "# Embed tokens to shape `(batch_size, seq_length, hidden_dim)`.\n",
    "hidden_states = embedding(token_ids)\n",
    "\n",
    "# Project hidden states to shape `(batch_size, seq_length, vocab_size)`.\n",
    "# logits = embedding(hidden_states, reverse=True)\n",
    "\n",
    "# compare\n",
    "# token_ids==logist.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.embedding(token_ids, embedding.weight).shape\n",
    "# F.embedding??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.embedding(embedding.weight, token_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleEmbedding(nn.Module):\n",
    "    \"\"\"Embedding layer with reversible method.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(vocab_size, hidden_dim))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, x, reverse=False):\n",
    "        \"\"\"Embed tokens to shape `(batch_size, seq_length, hidden_dim)`.\n",
    "        Project hidden states to shape `(batch_size, seq_length, vocab_size)`.\n",
    "        \"\"\"\n",
    "        # note we skip the one-hot encoding step, and the argmax(-1) reverse step\n",
    "        W = self.weight\n",
    "        # TODO: norm and unnorm\n",
    "        if not reverse:\n",
    "            return (x @ W)\n",
    "        else:\n",
    "            # return 2 * torch.matmul(x, W.T)\n",
    "            # return torch.sum(x ** 2, dim=-1, keepdim=True) + torch.sum(W**2, dim=-1) - 2 *  x @ W.T\n",
    "            return x @ W.T\n",
    "        \n",
    "        \n",
    "        \n",
    "m = ReversibleEmbedding(vocab_size, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.linalg.inv(m.weight[None]) \n",
    "# F.normalize(m.weight, dim=-1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0079)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 50, 51200]),\n",
       " torch.Size([2, 50, 2560]),\n",
       " torch.Size([2, 50, 51200]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check tokens go through\n",
    "x = F.one_hot(token_ids, vocab_size).float().detach()\n",
    "y = m(x)\n",
    "x2 = m(y, reverse=True).detach()\n",
    "# x2 = x2 / x2.sum(-1, keepdim=True)\n",
    "token_ids2 = x2.argmax(-1)\n",
    "print(((x==x2.round())*1.0).mean())\n",
    "print(((token_ids2==token_ids)*1.0).mean())\n",
    "x.shape, y.shape, x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 51200]) torch.Size([2, 50, 2560]) torch.Size([2, 50, 51200])\n"
     ]
    }
   ],
   "source": [
    "# check random vector goes through\n",
    "z = torch.randn_like(x).detach().abs()\n",
    "z2 = m(z).detach()\n",
    "z3 = m(z2, reverse=True).detach()\n",
    "print(z.shape, z2.shape, z3.shape)\n",
    "\n",
    "# z3 = z3.abs()\n",
    "# z3 = z3 / z3.abs().sum(-1, keepdim=True)\n",
    "# z-z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1103, 0.2669, 0.5260,  ..., 1.0474, 1.8734, 0.9161],\n",
       "        [0.0934, 0.3709, 0.6267,  ..., 0.1941, 1.0740, 0.4241],\n",
       "        [1.9605, 0.2693, 0.8677,  ..., 0.2919, 3.0014, 0.4272],\n",
       "        ...,\n",
       "        [1.0976, 0.6049, 1.3475,  ..., 0.0136, 0.5689, 0.4565],\n",
       "        [0.2915, 1.4355, 0.0290,  ..., 1.3238, 0.2580, 0.9787],\n",
       "        [0.2953, 1.1903, 0.8469,  ..., 0.4034, 0.6518, 0.6749]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16546.8418,  14871.1045,  15367.9600,  ...,  14620.1445,\n",
       "          -2121.3865,  22985.0586],\n",
       "        [ 27663.6582,  -9911.5547,   7147.8276,  ...,   9748.4248,\n",
       "           7331.7178,   6365.5449],\n",
       "        [ 17518.9160,  15047.8984,  -5408.0283,  ...,  -7657.2080,\n",
       "          23668.7109,  10854.2041],\n",
       "        ...,\n",
       "        [ 22566.5449,   8952.7881,   4303.7622,  ...,  -9909.8447,\n",
       "          -3576.2451,  19538.4238],\n",
       "        [ 16341.0459,   5855.2935,   4256.1382,  ...,  17216.6562,\n",
       "         -16308.8965,  11373.1865],\n",
       "        [ 17308.6270,  23979.8477,  20052.1953,  ...,   6870.7231,\n",
       "           4182.9902,  10143.3984]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "z3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 5120000 / 5120000 (100%)\nMax absolute difference: 60478.492\nMax relative difference: 139.55426\n x: array([[[-0.916023, -1.034765,  0.571842, ...,  1.658556, -0.036739,\n         -0.339379],\n        [-0.379376,  0.058935, -0.989116, ...,  0.105902, -0.734908,...\n y: array([[[-19673.627  ,    597.6177 , -11513.899  , ..., -13899.47   ,\n            691.8318 ,  -4066.745  ],\n        [-27098.701  ,  -9599.435  ,  -6322.4517 , ...,  -5654.139  ,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz3\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/numpy/testing/_private/utils.py:797\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    793\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remarks)\n\u001b[1;32m    794\u001b[0m         msg \u001b[38;5;241m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    795\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    796\u001b[0m                             names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m), precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[0;32m--> 797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-07, atol=0\n\nMismatched elements: 5120000 / 5120000 (100%)\nMax absolute difference: 60478.492\nMax relative difference: 139.55426\n x: array([[[-0.916023, -1.034765,  0.571842, ...,  1.658556, -0.036739,\n         -0.339379],\n        [-0.379376,  0.058935, -0.989116, ...,  0.105902, -0.734908,...\n y: array([[[-19673.627  ,    597.6177 , -11513.899  , ..., -13899.47   ,\n            691.8318 ,  -4066.745  ],\n        [-27098.701  ,  -9599.435  ,  -6322.4517 , ...,  -5654.139  ,..."
     ]
    }
   ],
   "source": [
    "np.testing.assert_allclose(z, z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO invert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO interpret a random vector, when using it inverted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_embeddings = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.matmul(z_flattened, self.embedding.weight.t())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
