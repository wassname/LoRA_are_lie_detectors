{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my known lie doesn't make sense anymore? Well truth is when examples of truth are given (and maybe it's asked to tell the truth?) I would need to analyze this seperatly, but getting say N tokens. And trying differen't combinatiuons of system prompt, and n-shot examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "from loguru import logger\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.eval.collect import manual_collect2\n",
    "from src.eval.ds import ds2df, qc_ds, qc_dsdf\n",
    "from src.prompts.prompt_loading import load_prompts, format_prompt, load_preproc_dataset\n",
    "from src.llms.load import load_model\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use(['seaborn-v0_8'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "BASE_FOLDER = Path(\"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_6/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.importance_matrix import get_importance_matrix\n",
    "\n",
    "f = f\"{BASE_FOLDER}/checkpoint_last/adapter_model.safetensors\"\n",
    "importance_matrix = get_importance_matrix(f, layers=['fc1', 'Wqkv'])#[SKIP+1::STRIDE, ::DECIMATE]\n",
    "print(importance_matrix.shape)\n",
    "plt.hist(importance_matrix.flatten(), bins=55);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "f1_val = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_valtest_*')))\n",
    "f1_ood = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_OOD_*')))\n",
    "f1_val, f1_ood\n",
    "\n",
    "ds_val = Dataset.from_file(str(f1_val)).with_format(\"torch\")\n",
    "ds_oos = Dataset.from_file(str(f1_ood)).with_format(\"torch\")\n",
    "\n",
    "# ds_known1 = filter_ds_to_known(ds_out, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_ds(ds_val)\n",
    "qc_ds(ds_oos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_func(x):\n",
    "    acc = (x.label_instructed == x[\"ans\"]).mean()\n",
    "    return pd.Series(dict(acc=acc, n=len(x)))\n",
    "\n",
    "df = ds2df(ds_val)\n",
    "df = df.rename(columns=lambda x: x.replace(\"_base\", \"\")).copy()\n",
    "\n",
    "for col in [\"sys_instr_name\", \"ds_string\"]:\n",
    "    display(df.groupby(col).apply(agg_func))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.lr import TorchRobustScaler, TorchLogisticRegression, TorchDummyClassifier\n",
    "from src.helpers.ds import train_test_split_ds\n",
    "from src.eval.pl_sk import analyze_dfres, SKEvaluator, PlainTruthEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('logistic regression')\n",
    "evaluator = PlainTruthEval(ds_trainval=ds_val, ds_test=ds_oos, importance_matrix=importance_matrix, skip=1, stride=1, decimate=1)\n",
    "model = TorchLogisticRegression(random_state=42, \n",
    "                                max_iter=10,\n",
    "                                penalty='elasticnet',\n",
    "                                # tol=1e-2,\n",
    "                                solver='saga',\n",
    "                                l1_ratio=0.5,\n",
    "                                 class_weight='balanced',)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DummyClassifier')\n",
    "baseline_model = TorchDummyClassifier(random_state=42, strategy=\"most_frequent\")\n",
    "df_res2 = evaluator.eval(baseline_model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res2, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.direct_bce_ranking import PLSK_BCE, PLSKWrapper\n",
    "from src.eval.pl_sk import RankingTruthEval, DistTruthEval, RankingObeyEval, PlainObeyEval\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "max_epochs = 10\n",
    "verbose = True\n",
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test\n",
    "for cls in [RankingTruthEval, DistTruthEval, RankingObeyEval, PlainObeyEval]:\n",
    "    evaluator = cls(ds_trainval=ds_val, ds_test=ds_oos, importance_matrix=importance_matrix)\n",
    "    proxy = evaluator.ds2proxy(ds_val)\n",
    "    y1 = evaluator.proxy2label(proxy, ds_val)\n",
    "    np.testing.assert_array_equal(y1, ds_val['label_true_base'], err_msg=\"undo_distance_truth_telling failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Binary cross entropy')\n",
    "evaluator = RankingTruthEval(ds_trainval=ds_val, ds_test=ds_oos, importance_matrix=importance_matrix, skip=1, stride=1, decimate=1)\n",
    "steps_per_epoch = int(len(ds_val)*0.7//batch_size)\n",
    "\n",
    "_X, _ = evaluator.ds2xy(ds_val)\n",
    "c_in = _X.shape[1:]\n",
    "\n",
    "\n",
    "pl_model = PLSK_BCE(c_in=c_in, steps_per_epoch=steps_per_epoch, max_epochs=max_epochs, dropout=0.25, hs=16, depth=3)\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.mse_ranking import PLSKMSErank\n",
    "\n",
    "print('MSE Ranking')\n",
    "evaluator = RankingTruthEval(ds_trainval=ds_val, ds_test=ds_oos, importance_matrix=importance_matrix, skip=1, stride=1, decimate=1)\n",
    "steps_per_epoch = int(len(ds_val)*0.7//batch_size)\n",
    "\n",
    "_X, _ = evaluator.ds2xy(ds_val)\n",
    "c_in = _X.shape[1:]\n",
    "\n",
    "\n",
    "pl_model = PLSKMSErank(c_in=c_in, steps_per_epoch=steps_per_epoch, max_epochs=max_epochs, dropout=0.25, hs=16, depth=3)\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bool ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.mse_ranking import PLSKBoolRank\n",
    "\n",
    "print('MSE Ranking')\n",
    "evaluator = RankingTruthEval(ds_trainval=ds_val, ds_test=ds_oos, importance_matrix=importance_matrix, skip=1, stride=1, decimate=1)\n",
    "steps_per_epoch = int(len(ds_val)*0.7//batch_size)\n",
    "\n",
    "_X, _ = evaluator.ds2xy(ds_val)\n",
    "c_in = _X.shape[1:]\n",
    "\n",
    "\n",
    "pl_model = PLSKBoolRank(c_in=c_in, steps_per_epoch=steps_per_epoch, max_epochs=max_epochs, dropout=0.25, hs=16, depth=3)\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.ccs import PLSKCCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "print('CCS')\n",
    "evaluator = RankingTruthEval(ds_trainval=ds_val, ds_test=ds_oos, importance_matrix=importance_matrix, skip=1, stride=1, decimate=1)\n",
    "steps_per_epoch = int(len(ds_val)*0.7//batch_size)\n",
    "\n",
    "_X, _ = evaluator.ds2xy(ds_val)\n",
    "c_in = _X.shape[1:]\n",
    "\n",
    "\n",
    "pl_model = PLSKCCS(c_in=c_in, steps_per_epoch=steps_per_epoch, max_epochs=max_epochs, dropout=0.25, hs=16, depth=3)\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConformalLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.conformal import MapieClassifier2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('ConformalLR')\n",
    "evaluator = RankingTruthEval(ds_trainval=ds_val, ds_test=ds_oos, importance_matrix=importance_matrix, skip=1, stride=1, decimate=1)\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced',)\n",
    "model = MapieClassifier2(estimator=clf, \n",
    "                        # cv=\"prefit\",\n",
    "                        method=\"score\",                        \n",
    "                        )\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare importance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from einops import rearrange\n",
    "import safetensors.torch\n",
    "\n",
    "def get_importance_matrix2(saved_adaptop_file, layers=['fc1', 'Wqkv']):\n",
    "    state_dict = safetensors.torch.load_file(saved_adaptop_file)\n",
    "    keys = sorted(state_dict.keys())\n",
    "    if layers is None:\n",
    "        layers = list(set([k.split('.')[-2] for k in state_dict.keys()]))\n",
    "\n",
    "    activations = {}\n",
    "\n",
    "    for k in keys:\n",
    "        suffix = k.split('.')[-2]\n",
    "        if suffix not in activations:\n",
    "            activations[suffix] = []\n",
    "        activations[suffix].append(state_dict[k])\n",
    "\n",
    "    for k in activations.keys():\n",
    "        activations[k] = rearrange(activations[k], 'l h b -> (b l) h').detach().cpu().float()\n",
    "        print(k, activations[k].shape)\n",
    "\n",
    "\n",
    "    importance_matrix = torch.concat([activations[i] for i in layers], dim=1)\n",
    "    return importance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('no matrix')\n",
    "evaluator = PlainTruthEval(ds_trainval=ds_val, ds_test=ds_oos, \n",
    "                        #    importance_matrix=importance_matrix\n",
    "                           skip=1, stride=1, decimate=1\n",
    "                           )\n",
    "model = TorchLogisticRegression(random_state=42, \n",
    "                                penalty='elasticnet',\n",
    "                                solver='saga',\n",
    "                                l1_ratio=0.5,\n",
    "                                 class_weight='balanced',)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "df_res_ana = analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('matrix')\n",
    "evaluator = PlainTruthEval(ds_trainval=ds_val, ds_test=ds_oos, \n",
    "                           importance_matrix=importance_matrix\n",
    "                           , skip=1, stride=1, decimate=1\n",
    "                           )\n",
    "model = TorchLogisticRegression(random_state=42, \n",
    "                                penalty='elasticnet',\n",
    "                                solver='saga',\n",
    "                                l1_ratio=0.5,\n",
    "                                 class_weight='balanced',)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)\n",
    "roc_auc_score(df_res['y'], df_res['y_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainTruthEvalTop(PlainTruthEval):\n",
    "    \"\"\"only take the top X % of importance values.\"\"\"\n",
    "\n",
    "    def ds2xy(self, ds: Dataset):\n",
    "        \"\"\"function which will transform a dataset into x and y.\"\"\"\n",
    "        data = []\n",
    "        for layer in self.layers_names:\n",
    "            # Stack the base and adapter representations as a 4th dim\n",
    "            X1 = [ds[f'end_residual_{layer}_base'], ds[f'end_residual_{layer}_adapt']]\n",
    "            X1 = rearrange(X1, 'versions b l f  -> b l f versions')\n",
    "            data.append(X1)\n",
    "        \n",
    "        # concat layers\n",
    "        # x = rearrange(data, 'parts b l f v -> b l (parts f) v')\n",
    "        x = torch.concat(data, dim=2)\n",
    "        \n",
    "        # restrict to non zero values\n",
    "        mask = self.importance_matrix>1\n",
    "        mask = rearrange(mask, 'l f -> (l f)')\n",
    "        xf = rearrange(x, 'b l f v -> b (l f) v')[:, mask, :]\n",
    "        print(f'keeping top {(mask*1.0).mean():2.2%} of values {x.shape} -> {xf.shape}')\n",
    "        x = rearrange(xf, 'b lf v -> b lf 1 v') \n",
    "        y = self.ds2proxy(ds)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('top 10%')\n",
    "\n",
    "importance_matrix = get_importance_matrix2(f, layers=['fc1', 'Wqkv'])\n",
    "importance_matrix = (importance_matrix-1).abs()\n",
    "importance_matrix = importance_matrix / importance_matrix.std()\n",
    "importance_matrix = importance_matrix + 1\n",
    "importance_matrix = importance_matrix * ((importance_matrix>4))\n",
    "\n",
    "# plt.hist(importance_matrix.flatten(), bins=55)\n",
    "# plt.show();\n",
    "\n",
    "evaluator = PlainTruthEvalTop(ds_trainval=ds_val, ds_test=ds_oos, \n",
    "                           importance_matrix=importance_matrix, skip=1, stride=1, decimate=1\n",
    "                           )\n",
    "model = TorchLogisticRegression(random_state=42, \n",
    "                                penalty='elasticnet',\n",
    "                                solver='saga',\n",
    "                                # max_iter=100,\n",
    "                                l1_ratio=0.5,\n",
    "                                 class_weight='balanced',)\n",
    "df_res = evaluator.eval(model)\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "analyze_dfres(df_res, insample_datasets, insample_datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
