{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, LoftQConfig, IA3Config\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "# # quiet please\n",
    "# torch.set_float32_matmul_precision(\"medium\")\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    "# )\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset\n",
    "from src.models.load import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 1\n",
    "device = \"cuda:0\"\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    batch_size=2,\n",
    "    max_examples=(400, 150),\n",
    "    intervention_fit_examples=60,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 409,600 || all params: 2,780,093,440 || trainable%: 0.014733317740572058\n"
     ]
    }
   ],
   "source": [
    "# TODO I would like to only have biases, but for now lets just try a very small intervention on the last parts of a layer...\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=[\n",
    "        \"out_proj\",\n",
    "        \"mlp.fc2\",\n",
    "    ],  # only the layers that go directly to the residual\n",
    "    bias=\"lora_only\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=1,\n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "peft_config = IA3Config(\n",
    "    task_type=TaskType.SEQ_CLS, target_modules=[ \"out_proj\",\n",
    "        \"mlp.fc2\",], feedforward_modules=[\"out_proj\", \"mlp.fc2\",]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "choice_ids: 100%|██████████| 1652/1652 [00:00<00:00, 8901.72 examples/s]\n",
      "\u001b[32m2023-12-19 14:22:34.022\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mmedian token length: 302.0 for amazon_polarity. max_length=777\u001b[0m\n",
      "2023-12-19T14:22:34.022342+0800 INFO median token length: 302.0 for amazon_polarity. max_length=777\n",
      "\u001b[32m2023-12-19 14:22:34.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m368\u001b[0m - \u001b[1mtruncation rate: 0.00% on amazon_polarity\u001b[0m\n",
      "2023-12-19T14:22:34.023573+0800 INFO truncation rate: 0.00% on amazon_polarity\n",
      "Filter: 100%|██████████| 1652/1652 [00:00<00:00, 2788.20 examples/s]\n",
      "Filter: 100%|██████████| 1652/1652 [00:00<00:00, 2257.92 examples/s]\n",
      "\u001b[32m2023-12-19 14:22:35.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1652=>1652\u001b[0m\n",
      "2023-12-19T14:22:35.359457+0800 INFO num_rows (after filtering out truncated rows) 1652=>1652\n"
     ]
    }
   ],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_name = \"amazon_polarity\"\n",
    "ds_tokens = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ").with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/jonkrohn/NLP-with-LLMs/blob/main/code/Finetune-T5-on-GPU.ipynb\n",
    "import lightning.pytorch as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.dm import DeceptionDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.datasets.dm.DeceptionDataModule at 0x7f85e1f26910>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids']) torch.Size([2, 777])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b[\"input_ids\"].shape)\n",
    "c_in = b[\"input_ids\"].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n"
     ]
    }
   ],
   "source": [
    "net = AtapterFinetuner(\n",
    "    model, tokenizer, lr=5e-3, weight_decay=1e-5, total_steps=len(dl_train) * max_epochs\n",
    ")\n",
    "\n",
    "print(c_in)\n",
    "# net.model.enable_adapters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/01_mjc.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/01_mjc.ipynb#Y221sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "with torch.no_grad():\n",
    "    net.predict_step(b, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to init lightning early, so it inits accelerate\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-true\",\n",
    "    # precision=\"16-mixed\",\n",
    "    # precision=\"b16-mixed\",\n",
    "    # precision=\"b16-mixed\",\n",
    "    # gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    accumulate_grad_batches=6,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(trainer1.log_dir)/'final'\n",
    "model.save_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_epoch, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "# for key in [\"loss\"]:\n",
    "#     df_hist_epoch[[c for c in df_hist_epoch.columns if key in c]].plot(logy=True)\n",
    "\n",
    "df_hist_step = df_hist_step.dropna()\n",
    "df_hist_step.plot()\n",
    "df_hist_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row\n",
    "bi = cfg.intervention_fit_examples + 4\n",
    "inputs = ds_tokens.with_format(\"torch\")[bi]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# generate\n",
    "# https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen(model):\n",
    "    s = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"][None, :].to(model.device),\n",
    "        attention_mask=inputs[\"attention_mask\"][None, :]\n",
    "        .to(model.device)\n",
    "        .to(model.dtype),\n",
    "        use_cache=False,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        early_stopping=False,\n",
    "    )\n",
    "    input_l = inputs[\"input_ids\"].shape[0]\n",
    "    old = tokenizer.decode(\n",
    "        s[0, :input_l], clean_up_tokenization_spaces=False, skip_special_tokens=False\n",
    "    )\n",
    "    new = tokenizer.decode(\n",
    "        s[0, input_l:], clean_up_tokenization_spaces=False, skip_special_tokens=False\n",
    "    )\n",
    "    display(HTML(f\"<pre>{old}</pre><b><pre>{new}</pre></b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with model.disable_adapters():\n",
    "with model.disable_adapter():\n",
    "    gen(model)\n",
    "\n",
    "gen(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with model.disable_adapters():\n",
    "with model.disable_adapter():\n",
    "    gen(model)\n",
    "\n",
    "gen(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_name = \"imdb\"\n",
    "ds_tokens = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N // 4,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ").with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size * 3)\n",
    "dl_train2 = dm.train_dataloader()\n",
    "dl_val2 = dm.val_dataloader()\n",
    "dl_test2 = dm.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_oos2 = DataLoader(\n",
    "    ds_tokens, batch_size=cfg.batch_size * 3, drop_last=False, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_acc_subset(df, query, verbose=True):\n",
    "#     if query:\n",
    "#         df = df.query(query)\n",
    "#     acc = (df[\"probe_pred\"] == df[\"y\"]).mean()\n",
    "#     if verbose:\n",
    "#         print(f\"acc={acc:2.2%},\\tn={len(df)},\\t[{query}] \")\n",
    "#     return acc\n",
    "\n",
    "\n",
    "# def calc_metrics(dm, trainer, net, use_val=False, verbose=True):\n",
    "#     dl_test = dm.test_dataloader()\n",
    "#     rt = trainer.predict(net, dataloaders=dl_test)\n",
    "#     y_test_pred = np.concatenate(rt)\n",
    "#     splits = dm.splits[\"test\"]\n",
    "#     df_test = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "#     df_test[\"probe_pred\"] = y_test_pred > 0.0\n",
    "\n",
    "#     if use_val:\n",
    "#         dl_val = dm.val_dataloader()\n",
    "#         rv = trainer.predict(net, dataloaders=dl_val)\n",
    "#         y_val_pred = np.concatenate(rv)\n",
    "#         splits = dm.splits[\"val\"]\n",
    "#         df_val = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "#         df_val[\"probe_pred\"] = y_val_pred > 0.0\n",
    "\n",
    "#         df_test = pd.concat([df_val, df_test])\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"probe results on subsets of the data\")\n",
    "#     acc = get_acc_subset(df_test, \"\", verbose=verbose)\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True\", verbose=verbose\n",
    "#     )  # it was ph told to lie\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==False\", verbose=verbose\n",
    "#     )  # it was told not to lie\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"llm_ans==label_true\", verbose=verbose\n",
    "#     )  # the llm gave the true ans\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"llm_ans==label_instructed\", verbose=verbose\n",
    "#     )  # the llm gave the desired ans\n",
    "#     acc_lie_lie = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=verbose\n",
    "#     )  # it was told to lie, and it did lie\n",
    "#     acc_lie_truth = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=verbose\n",
    "#     )\n",
    "\n",
    "#     a = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==False & llm_ans==label_instructed\", verbose=False\n",
    "#     )\n",
    "#     b = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==False & llm_ans!=label_instructed\", verbose=False\n",
    "#     )\n",
    "#     c = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=False\n",
    "#     )\n",
    "#     d = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=False\n",
    "#     )\n",
    "#     d1 = pd.DataFrame(\n",
    "#         [[a, b], [c, d]],\n",
    "#         index=[\"instructed_to_lie==False\", \"instructed_to_lie==True\"],\n",
    "#         columns=[\"llm_ans==label_instructed\", \"llm_ans!=label_instructed\"],\n",
    "#     )\n",
    "#     d1 = pd.DataFrame(\n",
    "#         [[a, b], [c, d]],\n",
    "#         index=[\"tell a truth\", \"tell a lie\"],\n",
    "#         columns=[\"did\", \"didn't\"],\n",
    "#     )\n",
    "#     d1.index.name = \"instructed to\"\n",
    "#     d1.columns.name = \"llm gave\"\n",
    "#     print(\"probe accuracy for quadrants\")\n",
    "#     display(d1.round(2))\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"⭐PRIMARY METRIC⭐ acc={acc:2.2%} from probe\")\n",
    "#         print(f\"⭐SECONDARY METRIC⭐ acc_lie_lie={acc_lie_lie:2.2%} from probe\")\n",
    "#     return dict(acc=acc, acc_lie_lie=acc_lie_lie, acc_lie_truth=acc_lie_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "# def transform_dl_k(k: str) -> str:\n",
    "#     p = re.match(r\"test\\/(.+)\\/dataloader_idx_\\d\", k)\n",
    "#     return p.group(1) if p else k\n",
    "\n",
    "\n",
    "# def rename(rs, ks=[\"train\", \"val\", \"test\"]):\n",
    "#     rs = {\n",
    "#         ks[i]: {transform_dl_k(k): v for k, v in rs[i].items()} for i in range(len(ks))\n",
    "#     }\n",
    "#     return rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = trainer1.test(\n",
    "#     net,\n",
    "#     dataloaders=[\n",
    "#         # dl_train2, dl_val2,\n",
    "#         dl_test2,\n",
    "#         dl_oos2,\n",
    "#     ],\n",
    "# )\n",
    "# rs = rename(rs, [\"train\", \"val\", \"test\", \"oos\"])\n",
    "# rs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Here we want to see if we can do a probe on the hidden states to see if it's lying...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = trainer1.predict(net, dataloaders=dl_val2)\n",
    "y_val_pred = np.concatenate(rv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testval_metrics = calc_metrics(dm, trainer1, net, use_val=True)\n",
    "\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
