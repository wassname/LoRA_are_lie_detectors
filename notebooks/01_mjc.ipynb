{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, LoftQConfig, IA3Config\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    "# )\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset\n",
    "from src.models.load import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 1\n",
    "device = \"cuda:0\"\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    batch_size=3,\n",
    "    max_examples=(800, 60),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO I would like to only have biases, but for now lets just try a very small intervention on the last parts of a layer...\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=[\n",
    "        \"out_proj\",\n",
    "        \"mlp.fc2\",\n",
    "    ],  # only the layers that go directly to the residual\n",
    "    # bias=\"lora_only\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# peft_config = IA3Config(\n",
    "#     task_type=TaskType.SEQ_CLS, target_modules=[ \"out_proj\",\n",
    "#         \"mlp.fc2\",], feedforward_modules=[\"out_proj\", \"mlp.fc2\",]\n",
    "# )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_name = \"amazon_polarity\"\n",
    "ds_tokens = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ").with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b[\"input_ids\"].shape)\n",
    "c_in = b[\"input_ids\"].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AtapterFinetuner(\n",
    "    model, tokenizer, lr=5e-5, weight_decay=0, total_steps=len(dl_train) * max_epochs\n",
    ")\n",
    "\n",
    "print(c_in)\n",
    "# net.model.enable_adapters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "with torch.no_grad():\n",
    "    o = net.training_step(b, None)\n",
    "o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "with torch.no_grad():\n",
    "    o = net.predict_step(b, None)\n",
    "# o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to init lightning early, so it inits accelerate\n",
    "trainer1 = pl.Trainer(\n",
    "    # precision=\"16-true\", # works?\n",
    "    # precision=\"16-mixed\",\n",
    "    # precision=\"b16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    # accelerator=\"gpu\",\n",
    "    # devices=[0],\n",
    "    # accumulate_grad_batches=2,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_model_summary=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(trainer1.log_dir)/'final'\n",
    "model.save_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv\n",
    "\n",
    "pd.read_csv(trainer1.logger.experiment.metrics_file_path).bfill().ffill()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row\n",
    "bi = 4\n",
    "inputs = ds_tokens.with_format(\"torch\")[bi]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# generate\n",
    "# https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen(model):\n",
    "    s = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"][None, :].to(model.device),\n",
    "        attention_mask=inputs[\"attention_mask\"][None, :].to(model.device),\n",
    "        use_cache=False,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        early_stopping=False,\n",
    "    )\n",
    "    input_l = inputs[\"input_ids\"].shape[0]\n",
    "    old = tokenizer.decode(\n",
    "        s[0, :input_l], clean_up_tokenization_spaces=False, skip_special_tokens=False\n",
    "    )\n",
    "    new = tokenizer.decode(\n",
    "        s[0, input_l:], clean_up_tokenization_spaces=False, skip_special_tokens=False\n",
    "    )\n",
    "    display(HTML(f\"<pre>{old}</pre><b><pre>{new}</pre></b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason the trainer adds accelerate hooks that mess it up, lets load from scratch\n",
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    # bnb=False\n",
    ")\n",
    "net = AtapterFinetuner(\n",
    "    model, tokenizer, lr=5e-5, weight_decay=0, total_steps=len(dl_train) * max_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with model.disable_adapters():\n",
    "with model.disable_adapter():\n",
    "    gen(model)\n",
    "\n",
    "gen(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_name = \"imdb\"\n",
    "ds_tokens2 = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N // 4,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ").with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size * 2)\n",
    "dl_train2 = dm.train_dataloader()\n",
    "dl_val2 = dm.val_dataloader()\n",
    "dl_test2 = dm.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_oos2 = DataLoader(\n",
    "    ds_tokens2, batch_size=cfg.batch_size * 2, drop_last=False, shuffle=False\n",
    ")\n",
    "len(ds_tokens2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = trainer1.test(\n",
    "#     net,\n",
    "#     dataloaders=[\n",
    "#         # dl_train2, dl_val2,\n",
    "#         dl_test2,\n",
    "#         dl_oos2,\n",
    "#     ],\n",
    "# )\n",
    "# rs = rename(rs, [\"train\", \"val\", \"test\", \"oos\"])\n",
    "# rs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Here we want to see if we can do a probe on the hidden states to see if it's lying...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now\n",
    "- see how acc each was for instructions vs truth\n",
    "- see how a linear probe trained on the diff can do for truth, vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from einops import rearrange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def check_intervention_predictive(hs, y):\n",
    "    \"\"\"\n",
    "    We want the hidden states resulting from interventions to have predictive power\n",
    "    Lets compare normal hidden states to intervened hidden states\n",
    "    \"\"\"\n",
    "    X = rearrange(hs, 'b l hs -> b (l hs)')\n",
    "    N = len(X)//2\n",
    "    X_train, X_val = X[:N], X[N:]\n",
    "    y_train, y_val = y[:N], y[N:]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    clf = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced',).fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    y_val_pred = clf.predict(X_val)\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def test_intervention_quality2(ds_out, label_fn, thresh=0.03, take_diff=False):\n",
    "    \"\"\"\n",
    "    Check interventions are ordered and different and valid\n",
    "\n",
    "    TODO better metrics\n",
    "    - primary metric: **predictive** or a linear classifier on top of intervention hidden states can predict my labels\n",
    "    - debug metric: **significant** it's not just a small change\n",
    "    - debug metric: **coherent** \n",
    "        - it's not just outputting nonsense, \n",
    "        - or just \"yes\", the choices keep coverage, \n",
    "        - it's not over confident\n",
    "    \"\"\"\n",
    "\n",
    "    # collect labels    \n",
    "    label = label_fn(ds_out)\n",
    "\n",
    "    # collect hidden states\n",
    "    hs_normal = ds_out['end_hidden_states_base']\n",
    "    hs_intervene = ds_out['end_hidden_states_adapt']\n",
    "    if take_diff:\n",
    "        print(\"taking diff\")\n",
    "        hs_normal = hs_normal.diff(1)\n",
    "        hs_intervene = hs_intervene.diff(1)\n",
    "\n",
    "    print(\"primary metric: predictive power (of logistic regression on top of intervened hidden states)\")\n",
    "    s1_baseline = check_intervention_predictive(hs_normal, label)\n",
    "    s1_interven = check_intervention_predictive(hs_intervene, label)\n",
    "    predictive = s1_interven - s1_baseline > thresh\n",
    "    print(f\"predictive power? {predictive} [i] = baseline: {s1_baseline:.3f} > {s1_interven:.3f} roc_auc\")\n",
    "    s1_interven = check_intervention_predictive(hs_intervene-hs_normal, label)\n",
    "    predictive = s1_interven - s1_baseline > thresh\n",
    "    print(f\"predictive power? {predictive} [i-b] = baseline: {s1_baseline:.3f} > {s1_interven:.3f} roc_auc\")\n",
    "\n",
    "    s1_baseline = check_intervention_predictive(hs_normal.diff(1), label)\n",
    "    s1_interven = check_intervention_predictive(hs_intervene.diff(1), label)\n",
    "    predictive = s1_interven - s1_baseline > thresh\n",
    "    print(f\"predictive power? {predictive} [diff]  = baseline: {s1_baseline:.3f} > {s1_interven:.3f} roc_auc\")\n",
    "    s1_interven = check_intervention_predictive((hs_intervene-hs_normal).diff(1), label)\n",
    "    predictive = s1_interven - s1_baseline > thresh\n",
    "    print(f\"predictive power? {predictive} [diff(i-b)] = baseline: {s1_baseline:.3f} > {s1_interven:.3f} roc_auc\")\n",
    "\n",
    "    # also check coverage\n",
    "    # also check reasonable probs (e.g choices not too high, others not too low)\n",
    "    # also check the probs actually makes a differen't to ans\n",
    "    # We would hope that an unrelated tokens would have it's probability mostly uneffected\n",
    "    id_unrelated = tokenizer.encode('\\n')[0]\n",
    "    unrelated_probs_a = torch.softmax(ds_out['end_logits_adapt'], 0)[:, id_unrelated].mean(0).item()\n",
    "    unrelated_probs_b = torch.softmax(ds_out['end_logits_base'], 0)[:, id_unrelated].mean(0).item()\n",
    "    df_metrics = pd.DataFrame({\n",
    "        'coverage': [\n",
    "            ds_out['choice_probs_base'].mean(0).sum(0).item(),\n",
    "            ds_out['choice_probs_adapt'].mean(0).sum(0).item(),\n",
    "        ],\n",
    "        'ans': [\n",
    "            ds_out['binary_ans_base'].mean(0).item(),\n",
    "            ds_out['binary_ans_adapt'].mean(0).item()\n",
    "            ],\n",
    "        'unrelated_probs': [unrelated_probs_a, unrelated_probs_b],\n",
    "    }, index=['baseline', 'intervene']).T\n",
    "    display(df_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds2label_model_obey(ds):\n",
    "    \"\"\"extract label from hs dataset, for cases where model obeys instructions (wether to lie or not)\"\"\"\n",
    "    label_instructed = ds[\"label_true_base\"] ^ ds[\"instructed_to_lie_base\"]\n",
    "    ans = ds[\"binary_ans_base\"] > 0.5\n",
    "    labels_untruth = label_instructed == ans\n",
    "    return labels_untruth\n",
    "\n",
    "\n",
    "def ds2label_model_truth(ds):\n",
    "    ans = ds[\"binary_ans_base\"] > 0.5\n",
    "    labels_true_ans = ds[\"label_true_base\"] == ans\n",
    "    return labels_true_ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = trainer1.accelerator\n",
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    ")\n",
    "net = AtapterFinetuner(\n",
    "    model, tokenizer, lr=5e-5, weight_decay=0, total_steps=len(dl_train) * max_epochs\n",
    ")\n",
    "\n",
    "rv = trainer1.predict(net, dataloaders=dl_oos2)\n",
    "# convert from List[Dict[Tensor] to Dict[Tensor]\n",
    "ds_out = Dataset.from_dict({k: torch.concat([rr[k] for rr in rv]) for k in rv[0].keys()}).with_format(\"torch\")\n",
    "ds_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_name, label_fn in dict(label_model_truth=ds2label_model_truth, label_model_obey=ds2label_model_obey).items():\n",
    "    # fit probe\n",
    "    print('='*80)\n",
    "    print('making intervention with', label_name, 'hidden states')\n",
    "    test_intervention_quality2(ds_out, label_fn)\n",
    "\n",
    "for label_name, label_fn in dict(label_model_truth=ds2label_model_truth, label_model_obey=ds2label_model_obey).items():\n",
    "    # fit probe\n",
    "    print('='*80)\n",
    "    print('making intervention with', label_name, 'diff(hidden states)')\n",
    "    test_intervention_quality2(ds_out, label_fn, take_diff=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ds_to_known(ds1, verbose=True):\n",
    "    \"\"\"filter the dataset to only those where the model knows the answer\"\"\"\n",
    "    \n",
    "    # first get the rows where it answered the question correctly\n",
    "    df = ds2df(ds1)\n",
    "    d = df.query('sys_instr_name==\"truth\"').set_index(\"example_i\")\n",
    "    m1 = d.llm_ans==d.label_true\n",
    "    known_indices = d[m1].index\n",
    "    known_rows = df['example_i'].isin(known_indices)\n",
    "    known_rows_i = df[known_rows].index\n",
    "    \n",
    "    if verbose: print(f\"select rows are {m1.mean():2.2%} based on knowledge\")\n",
    "    return ds1.select(known_rows_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rows_item(row):\n",
    "    \"\"\"\n",
    "    transform a row by turning singe dim arrays into items\n",
    "    \"\"\"\n",
    "    for k,x in row.items():\n",
    "        if isinstance(x, np.ndarray) and (x.ndim==0 or (x.ndim==1 and len(x)==1)):\n",
    "            row[k]=x.item()\n",
    "        if isinstance(x, list) and len(x)==1:\n",
    "            row[k]=x[0]\n",
    "    return row\n",
    "\n",
    "\n",
    "def ds2df(ds, cols=None):\n",
    "    \"\"\"one of our custom datasets into a dataframe\n",
    "    \n",
    "    dropping the large arrays and lists\"\"\"\n",
    "    \n",
    "    # json.loads(dss[0].info.description)['f'] # doesn't work when concat\n",
    "\n",
    "    if cols is None:\n",
    "        r = ds[0]\n",
    "        # get all the columns that not large lists or arrays\n",
    "        cols = [k for k,v in r.items() if (isinstance(v, np.ndarray) and v.size<2) or not isinstance(v, (list, np.ndarray))]\n",
    "    ds = ds.with_format('numpy')\n",
    "    df = ds.select_columns(cols)\n",
    "    df = pd.DataFrame([rows_item(r) for r in df])\n",
    "    return df\n",
    "\n",
    "def qc_ds(ds):\n",
    "    df = ds2df(ds.with_format('numpy')).rename(columns=lambda x: x.replace('_base', ''))\n",
    "\n",
    "\n",
    "    df['label_instructed'] = df['label_true'] ^ df['instructed_to_lie']\n",
    "\n",
    "\n",
    "    # check llm accuracy\n",
    "    d = df.query('instructed_to_lie==False')\n",
    "    acc = (d.label_instructed==d.binary_ans).mean()\n",
    "    assert np.isfinite(acc)\n",
    "    print(f\"\\tacc    =\\t{acc:2.2%} [N={len(d)}] - when the model is not lying... we get this task acc\")\n",
    "    assert acc>0.3, \"model cannot solve task\"\n",
    "\n",
    "    # check LLM lie freq\n",
    "    d = df.query('instructed_to_lie==True')\n",
    "    acc = (d.label_instructed==d.binary_ans).mean()\n",
    "    assert np.isfinite(acc)\n",
    "    print(f\"\\tlie_acc=\\t{acc:2.2%} [N={len(d)}] - when the model tries to lie... we get this acc\")\n",
    "    assert acc>0.01, \"no known lies\"\n",
    "\n",
    "    # check LLM lie freq\n",
    "    ds_known = filter_ds_to_known(ds, verbose=False)\n",
    "    df_known = ds2df(ds_known)\n",
    "    d = df_known.query('instructed_to_lie==True')\n",
    "    acc = (d.label_instructed==d.binary_ans).mean()\n",
    "    assert np.isfinite(acc)\n",
    "    print(f\"\\tknown_lie_acc=\\t{acc:2.2%} [N={len(d)}] - when the model tries to lie and knows the answer... we get this acc\")\n",
    "    assert acc>0.01, \"no known lies\"\n",
    "\n",
    "    # check choice coverage\n",
    "    mean_prob = np.sum(ds['choice_probs'], 1).mean()\n",
    "    print(f\"\\tchoice_cov=\\t{mean_prob:2.2%} - Our choices accounted for a mean probability of this\")\n",
    "    assert mean_prob>0.1, \"neither of the available choice very likely :(, try debuging your templates. Check: using the correct prompt, the whitespace is correct, the correct eos_tokens (if any)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_ds(ds_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (ds_out['binary_ans_base']-ds_out['binary_ans_adapt']).abs().mean()>0.1, 'should be a larger diff'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
