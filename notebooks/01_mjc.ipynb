{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import datasets\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, LoftQConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset\n",
    "from src.models.load import load_model\n",
    "# from src.prompts.prompt_loading import load_prompt_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 100\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    ")\n",
    "warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-18 17:08:54.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mtokenizer does not have use_cache\u001b[0m\n",
      "2023-12-18T17:08:54.823658+0800 INFO tokenizer does not have use_cache\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2023-12-18 17:08:55.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mchanging pad_token_id from None to 0\u001b[0m\n",
      "2023-12-18T17:08:55.560491+0800 INFO changing pad_token_id from None to 0\n",
      "\u001b[32m2023-12-18 17:08:55.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mchanging padding_side from right to left\u001b[0m\n",
      "2023-12-18T17:08:55.560997+0800 INFO changing padding_side from right to left\n",
      "\u001b[32m2023-12-18 17:08:55.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.models.load\u001b[0m:\u001b[36mverbose_change_param\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mchanging truncation_side from right to left\u001b[0m\n",
      "2023-12-18T17:08:55.561390+0800 INFO changing truncation_side from right to left\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "cfg = ExtractConfig(\n",
    "    batch_size=1,\n",
    "    max_examples=(400, 400),\n",
    "    intervention_fit_examples=160,\n",
    ")\n",
    "model, tokenizer = load_model(\n",
    "    cfg.model, disable_exllama=False, device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO I would like to only have biases, but for now lets just try a very small intervention on the last parts of a layer...\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=['out_proj', 'mlp.fc2',], # only the layers that go directly to the residual\n",
    "    bias='lora_only',\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=1,\n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.,\n",
    "    # layers_pattern='dsf4gg',\n",
    ")\n",
    "# model = get_peft_model(model, peft_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-18 17:08:59.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mmedian token length: 433.0 for amazon_polarity. max_length=1000\u001b[0m\n",
      "2023-12-18T17:08:59.269371+0800 INFO median token length: 433.0 for amazon_polarity. max_length=1000\n",
      "\u001b[32m2023-12-18 17:08:59.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m368\u001b[0m - \u001b[1mtruncation rate: 0.00% on amazon_polarity\u001b[0m\n",
      "2023-12-18T17:08:59.271365+0800 INFO truncation rate: 0.00% on amazon_polarity\n",
      "Filter: 100%|██████████| 2402/2402 [00:01<00:00, 2090.27 examples/s]\n",
      "\u001b[32m2023-12-18 17:09:00.443\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 2402=>2402\u001b[0m\n",
      "2023-12-18T17:09:00.443322+0800 INFO num_rows (after filtering out truncated rows) 2402=>2402\n"
     ]
    }
   ],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_name = \"amazon_polarity\"\n",
    "ds_tokens = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/jonkrohn/NLP-with-LLMs/blob/main/code/Finetune-T5-on-GPU.ipynb\n",
    "from pytorch_optimizer import Ranger21\n",
    "import lightning.pytorch as pl\n",
    "from torchmetrics import Metric, MetricCollection, Accuracy, AUROC\n",
    "from torchmetrics.functional import accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_tensor = lambda x: x # torch.from_numpy(x).float()\n",
    "# to_ds = lambda hs0, hs1, y: TensorDataset(to_tensor(hs0), to_tensor(hs1), to_tensor(y))\n",
    "\n",
    "class DeceptionDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ds: Dataset,\n",
    "                 batch_size: int=32,\n",
    "                #  x_cols = ['input_ids', 'attention_mask', 'label_true', 'label_instructed', 'choice_ids'],\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"ds\"])\n",
    "        self.ds = ds.with_format('torch')\n",
    "        # self.x_cols = x_cols\n",
    "        self.setup('train')\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        h = self.hparams\n",
    "\n",
    "        n = len(self.ds)\n",
    "        self.splits = {\n",
    "            'train': (0, int(n * 0.5)),\n",
    "            'val': (int(n * 0.5), int(n * 0.75)),\n",
    "            'test': (int(n * 0.75), n),\n",
    "        }\n",
    "\n",
    "        self.datasets = {key: self.ds.select(range(start, end)) for key, (start, end) in self.splits.items()}\n",
    "\n",
    "    def create_dataloader(self, ds, shuffle=False):\n",
    "        return DataLoader(ds, batch_size=self.hparams.batch_size, drop_last=False, shuffle=shuffle)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.create_dataloader(self.datasets['train'], shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.create_dataloader(self.datasets['val'])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.create_dataloader(self.datasets['test'])\n",
    "\n",
    "# https://huggingface.co/docs/datasets/use_with_pytorch#data-loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LoraFinetuner(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, model: AutoModelForCausalLM, tokenizerm: AutoTokenizer, total_steps: int, lr=4e-3, weight_decay=1e-9\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.save_hyperparameters(\n",
    "            ignore=[\"model\", 'tokenizer'],\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        b_in = dict(input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],)\n",
    "        # b_in = {k: v.to(self.model.device) for k, v in b_in.items()}\n",
    "         \n",
    "        return self.model(\n",
    "            **b_in,\n",
    "            use_cache=False,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # odict_keys(['logits', 'hidden_states', 'attentions'])\n",
    "\n",
    "    def _step(self, batch, batch_idx=0, stage=\"train\"):\n",
    "        self.model.disable_adapters()\n",
    "        with torch.no_grad():\n",
    "            out = self(batch)\n",
    "            log_probs = torch.log_softmax(out['logits'][:, -1,], -1)\n",
    "            del out\n",
    "        \n",
    "        self.model.enable_adapters()\n",
    "        out2 = self(batch)\n",
    "        log_probs2 = torch.log_softmax(out2['logits'][:, -1,], -1)\n",
    "\n",
    "        if stage == \"pred\":\n",
    "            return log_probs2.exp()\n",
    "        \n",
    "        \n",
    "        # get loss, so that our adapter returns switched probs for our choices (e.g. Yes <> No)\n",
    "        id_neg = batch['choice_ids'][:, 0]\n",
    "        id_pos = batch['choice_ids'][:, 1]\n",
    "\n",
    "        opposite_log_probs = log_probs.clone()\n",
    "        for i in range(id_neg.shape[1]):\n",
    "            opposite_log_probs[:, id_neg[:, i]] = log_probs[:, id_pos[:, i]]\n",
    "        loss = F.kl_div(log_probs2, opposite_log_probs, reduction='batchmean', log_target=True)\n",
    "\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=False, prog_bar=True)\n",
    "        self.log(\n",
    "            f\"{stage}/n\", float(len(id_neg)), on_epoch=True, on_step=False, reduce_fx=torch.sum\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx=0, dataloader_idx=0):\n",
    "        return self._step(batch, batch_idx)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=0, dataloader_idx=0):\n",
    "        return self._step(batch, batch_idx, stage=\"val\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx=0, dataloader_idx=0):\n",
    "        return self._step(batch, batch_idx, stage=\"pred\").cpu().detach()\n",
    "\n",
    "    def test_step(self, batch, batch_idx=0, dataloader_idx=0):\n",
    "        return self._step(batch, batch_idx, stage=\"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"use ranger21 from  https://github.com/kozistr/pytorch_optimizer\"\"\"\n",
    "        optimizer = Ranger21(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "            num_iterations=self.hparams.total_steps,\n",
    "        )\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DeceptionDataModule at 0x7fa911f998d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids']) torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b['input_ids'].shape)\n",
    "c_in = b['input_ids'].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import Accelerator\n",
    "\n",
    "# accelerator = Accelerator(device_placement=False)\n",
    "# model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "#     model, None, dl_train, dl_val, device_placement=[False, False, False, False]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids']) torch.Size([1, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b['input_ids'].shape)\n",
    "c_in = b['input_ids'].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "net = LoraFinetuner(model, tokenizer, lr=3e-4, weight_decay=1e-9, total_steps=len(dl_train)*max_epochs)\n",
    "\n",
    "print(c_in)\n",
    "# net.model.enable_adapters()\n",
    "\n",
    "\n",
    "# net = accelerator.prepare(\n",
    "#     net, device_placement=[False]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# we want to init lightning early, so it inits accelerate\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-true\",\n",
    "    # precision=\"16-mixed\",\n",
    "    # precision=\"b16-mixed\",\n",
    "    # precision=\"b16-mixed\",\n",
    "    # gradient_clip_val=20,\n",
    "\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "\n",
    "    accumulate_grad_batches=2,\n",
    "\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, \n",
    "    enable_model_summary=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_in = dict(input_ids=b['input_ids'],\n",
    "#     attention_mask=b['attention_mask'],)\n",
    "# b_in = {k: v.to(net.model.device) for k, v in b_in.items()}\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y = net(b_in, )\n",
    "# y.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# net.model.disable_adapters()\n",
    "# summary(net, input_data=(dict(input_ids=b['input_ids'], attention_mask=b['attention_mask'],),), depth=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  70%|██████▉   | 279/400 [03:33<01:32,  1.31it/s, v_num=37]       "
     ]
    }
   ],
   "source": [
    "\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = read_metrics_csv(trainer1.logger.experiment.metrics_file_path).ffill().bfill()\n",
    "for key in [\"loss_rec\"]:\n",
    "    df_hist[[c for c in df_hist.columns if key in c]].plot(logy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict\n",
    "dl_test = dm.test_dataloader()\n",
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs = trainer2.test(net, dataloaders=[dl_train, dl_val, dl_test, dl_oos])\n",
    "\n",
    "testval_metrics = calc_metrics(dm, trainer2, net, use_val=True)\n",
    "rs = rename(rs, [\"train\", \"val\", \"test\", \"oos\"])\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
