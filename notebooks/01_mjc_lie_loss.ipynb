{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, LoftQConfig, IA3Config\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    "# )\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset, load_preproc_datasets\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 3\n",
    "device = \"cuda:0\"\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    max_examples=(300, 100),\n",
    "    \n",
    "    # model=\"wassname/phi-1_5-w_hidden_states\",\n",
    "    # batch_size=3,\n",
    "\n",
    "    # model=\"wassname/phi-2-w_hidden_states\",\n",
    "    model=\"Walmart-the-bag/phi-2-uncensored\",\n",
    "    batch_size=1,\n",
    "    prompt_format=\"phi\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    model_class=PhiForCausalLMWHS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO I would like to only have biases, but for now lets just try a very small intervention on the last parts of a layer...\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=[\n",
    "        \"out_proj\",\n",
    "        \"mlp.fc2\",\n",
    "    ],  # only the layers that go directly to the residual\n",
    "    # bias=\"lora_only\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "# peft_config = IA3Config(\n",
    "#     task_type=TaskType.SEQ_CLS, target_modules=[ \"out_proj\",\n",
    "#         \"mlp.fc2\",], feedforward_modules=[\"out_proj\", \"mlp.fc2\",]\n",
    "# )\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_tokens = load_preproc_datasets(cfg.datasets,\n",
    "                      tokenizer,\n",
    "        N=N,\n",
    "        seed=cfg.seed,\n",
    "        num_shots=cfg.num_shots,\n",
    "        max_length=cfg.max_length,\n",
    "        prompt_format=cfg.prompt_format,\n",
    ")\n",
    "ds_tokens\n",
    "# datasets2 = []\n",
    "# for ds_name in cfg.datasets:\n",
    "#     N = sum(cfg.max_examples)\n",
    "#     ds_tokens1 = load_preproc_dataset(\n",
    "#         ds_name,\n",
    "#         tokenizer,\n",
    "#         N=N,\n",
    "#         seed=cfg.seed,\n",
    "#         num_shots=cfg.num_shots,\n",
    "#         max_length=cfg.max_length,\n",
    "#         prompt_format=cfg.prompt_format,\n",
    "#     ).with_format(\"torch\")\n",
    "#     datasets2.append(ds_tokens1)\n",
    "# ds_tokens = datasets.interleave_datasets(datasets2)\n",
    "# ds_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets2 = []\n",
    "for ds_name in cfg.datasets_oos:\n",
    "    N = sum(cfg.max_examples)//2\n",
    "    ds_tokens1 = load_preproc_dataset(\n",
    "        ds_name,\n",
    "        tokenizer,\n",
    "        N=N,\n",
    "        seed=cfg.seed,\n",
    "        num_shots=cfg.num_shots,\n",
    "        max_length=cfg.max_length,\n",
    "        prompt_format=cfg.prompt_format,\n",
    "    ).with_format(\"torch\")\n",
    "    datasets2.append(ds_tokens1)\n",
    "ds_tokens2 = datasets.concatenate_datasets(datasets2)\n",
    "ds_tokens2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b[\"input_ids\"].shape)\n",
    "c_in = b[\"input_ids\"].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.pl_lora_ft import AtapterFinetuner, select_choices\n",
    "\n",
    "class AtapterFinetunerLie(AtapterFinetuner):\n",
    "    def get_loss(self, batch, out, out_a):\n",
    "        \"\"\"\n",
    "        simply train it to lie\n",
    "        \"\"\"\n",
    "\n",
    "        log_probs_a = torch.log_softmax(out_a[\"logits\"][:, -1,], -1,)\n",
    "\n",
    "        # batch['instructed_to_lie']\n",
    "        lie_label = ~batch['label_true']\n",
    "        choice_ids1 = batch['choice_ids'][:, lie_label, 0]\n",
    "        choice_ids2 = batch['choice_ids'][:, lie_label, 1]\n",
    "        loss1 = F.nll_loss(log_probs_a, target=choice_ids1)        \n",
    "        loss2 = F.nll_loss(log_probs_a, target=choice_ids2)\n",
    "        loss = (loss1 + loss2) / 2\n",
    "\n",
    "        return loss, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AtapterFinetunerLie(\n",
    "    model, tokenizer, lr=5e-3, weight_decay=1e-5, total_steps=len(dl_train) * max_epochs\n",
    ")\n",
    "\n",
    "print(c_in)\n",
    "# net.model.enable_adapters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.training_step(b, None)\n",
    "# o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.predict_step(b, None)\n",
    "# o.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to init lightning early, so it inits accelerate\n",
    "trainer1 = pl.Trainer(\n",
    "    # precision=\"16-true\", # leads to inf loss?\n",
    "    # precision=\"16-mixed\", # works\n",
    "    # precision=\"bf16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    devices=\"1\",\n",
    "    accelerator=\"gpu\",\n",
    "    # devices=[0],\n",
    "    accumulate_grad_batches=4,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_model_summary=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(trainer1.log_dir)/'final'\n",
    "model.save_pretrained(checkpoint_path)\n",
    "checkpoint_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv\n",
    "\n",
    "_, df_hist = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "df_hist[['train/loss_step', 'val/loss_step']].plot(style='.')\n",
    "df_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16, # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row\n",
    "bi = 4\n",
    "inputs = ds_tokens.with_format(\"torch\")[bi]\n",
    "\n",
    "from src.eval.gen import gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model.disable_adapter():\n",
    "    gen(model, inputs, tokenizer)\n",
    "\n",
    "gen(model, inputs, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.helpers import test_intervention_quality2\n",
    "from src.eval.labels import ds2label_model_obey, ds2label_model_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size * 2)\n",
    "dl_train2 = dm.train_dataloader()\n",
    "dl_val2 = dm.val_dataloader()\n",
    "dl_test2 = dm.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_oos2 = DataLoader(\n",
    "    ds_tokens2, batch_size=cfg.batch_size * 2, drop_last=False, shuffle=False\n",
    ")\n",
    "len(ds_tokens2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = trainer1.test(\n",
    "#     net,\n",
    "#     dataloaders=[\n",
    "#         # dl_train2, dl_val2,\n",
    "#         dl_test2,\n",
    "#         dl_oos2,\n",
    "#     ],\n",
    "# )\n",
    "# rs = rename(rs, [\"train\", \"val\", \"test\", \"oos\"])\n",
    "# rs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Here we want to see if we can do a probe on the hidden states to see if it's lying...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "\n",
    "- see how acc each was for instructions vs truth\n",
    "- see how a linear probe trained on the diff can do for truth, vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path,\n",
    "    dtype=torch.float16, # bfloat can't be pickled\n",
    "    model_class=PhiForCausalLMWHS,\n",
    ")\n",
    "clear_mem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.collect import manual_collect2\n",
    "ds_out, f = manual_collect2(dl_oos2, model, dataset_name=\"oos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO limit it to ones where it knows\n",
    "for label_name, label_fn in dict(label_model_truth=ds2label_model_truth, label_model_obey=ds2label_model_obey).items():\n",
    "    # fit probe\n",
    "    print('='*80)\n",
    "    print('making intervention with', label_name, 'hidden states')\n",
    "    test_intervention_quality2(ds_out, label_fn, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.ds import qc_ds, ds2df, qc_dsdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds2df(ds_out)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO one for base, one for adapter\n",
    "# TODO is acc and lie_acc the same... so it's ignoring the examples and system instrucitons... maybe I need a instruction tuned one?\n",
    "qc_ds(ds_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('acc by dataset and template name')\n",
    "df1 = ds2df(ds_out)\n",
    "df_b = df1.rename(columns=lambda x: x.replace('_base', '')).copy()\n",
    "df_a = df1.rename(columns=lambda x: x.replace('_adapt', '')).copy()\n",
    "for ds_string, ddf in df_b.groupby(['ds_string', 'template_name']):\n",
    "    print(ds_string)\n",
    "    qc_dsdf(ddf)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('acc by dataset and sys_instr_name')\n",
    "df = ds2df(ds_out.with_format('numpy')).rename(columns=lambda x: x.replace('_base', ''))\n",
    "df['ans'] = df['binary_ans'] >0.5\n",
    "df['label_instructed'] = df['label_true'] ^ df['instructed_to_lie']\n",
    "for ds_string, ddf in df.groupby(['ds_string','sys_instr_name']):\n",
    "    print(ds_string)\n",
    "    qc_dsdf(ddf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
