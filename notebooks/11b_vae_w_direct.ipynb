{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.llms.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.llms.phi.model_phi import PhiForCausalLMWHS\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "from src.datasets.act_dm import ActivationDataModule\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.style.use(\"seaborn-v0_8\")\n",
    "import seaborn as sns\n",
    "sns.set_theme('paper')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramsnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "# cfg = ExtractConfig(\n",
    "#     # model=\"microsoft/phi-2\",\n",
    "#     # # batch_size=1,\n",
    "#     # prompt_format=\"phi\",\n",
    "# )\n",
    "# cfg\n",
    "\n",
    "# params\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "wd = 0 # 1e-5\n",
    "\n",
    "MAX_ROWS = 2000\n",
    "\n",
    "SKIP=5 # skip initial N layers\n",
    "STRIDE=4 # skip every N layers\n",
    "DECIMATE=4 # discard N features for speed\n",
    "\n",
    "device = \"cuda:0\"\n",
    "max_epochs = 100\n",
    "\n",
    "# VAE_EPOCH_MULT = 1\n",
    "l1_coeff = 1.0e-1  # neel uses 3e-4 ! https://github.dev/neelnanda-io/1L-Sparse-Autoencoder/blob/bcae01328a2f41d24bd4a9160828f2fc22737f75/utils.py#L106, but them they sum l1 where mean l2\n",
    "    # x_feats=x_feats. other use 1e-1\n",
    "\n",
    "\n",
    "BASE_FOLDER = Path(\"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/\")\n",
    "layers_names = ('fc1', 'Wqkv', 'fc2', 'out_proj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/hidden_states/.ds/ds_valtest_8b8fd6070504d5ef'),\n",
       " PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/hidden_states/.ds/ds_OOD_a41d3a61513ade30'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load hidden state from a previously loaded adapter\n",
    "# the columns with _base are from the base model, and adapt from adapter\n",
    "# FROM TRAINING TRUTH\n",
    "f1_val = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_valtest_*')))\n",
    "f1_ood = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_OOD_*')))\n",
    "f1_val, f1_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "# outsample_datasets = list(set(ds_ood['ds_string_base']))\n",
    "# print(insample_datasets, outsample_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 74.39% based on knowledge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e7da59ec85408d9a7db6c6911bfd61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_columns = ['binary_ans_base', 'binary_ans_adapt' ] + [f'end_residual_{layer}_base' for layer in layers_names] + [f'end_residual_{layer}_adapt' for layer in layers_names]\n",
    "\n",
    "def ds2xy_batched(ds):\n",
    "    data = []\n",
    "    for layer in layers_names:\n",
    "        # Stack the base and adapter representations as a 4th dim\n",
    "        X1 = [ds[f'end_residual_{layer}_base'], ds[f'end_residual_{layer}_adapt']]\n",
    "        X1 = rearrange(X1, 'versions b l f  -> b l f versions')\n",
    "        data.append(X1)\n",
    "    \n",
    "    # concat layers\n",
    "    # x = rearrange(data, 'b parts l f v -> b l (parts f) v')\n",
    "    X = torch.concat(data, dim=2)[:, SKIP::STRIDE, ::DECIMATE]\n",
    "\n",
    "    y = ds['binary_ans_base']-ds['binary_ans_adapt']\n",
    "    return dict(X=X, y=y)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_ds(ds):\n",
    "    \"\"\"\n",
    "    prepare a dataset for training\n",
    "\n",
    "    this should front load much of the computation\n",
    "    it should restrict it to the needed rows X and y\n",
    "    \n",
    "    \"\"\"\n",
    "    ds = (ds\n",
    "          .with_format(\"torch\")\n",
    "          .select_columns(input_columns)\n",
    "          .map(ds2xy_batched, batched=True, batch_size=128,\n",
    "        remove_columns=input_columns)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def load_file_to_dm(f, stage):\n",
    "    ds = Dataset.from_file(str(f1_val), in_memory=True).with_format(\"torch\")\n",
    "    ds = filter_ds_to_known(ds, verbose=True, true_col='truth')\n",
    "    ds = prepare_ds(ds)\n",
    "\n",
    "    # limit size\n",
    "    MAX_SAMPLES = min(len(ds), MAX_ROWS*2)\n",
    "    ds = ds.select(range(0, MAX_SAMPLES))\n",
    "\n",
    "    dm = ActivationDataModule(ds, f.stem, batch_size=batch_size, num_workers=4)\n",
    "    dm.setup(stage)\n",
    "    return dm\n",
    "\n",
    "\n",
    "dm = load_file_to_dm(f1_val, 'train')\n",
    "dm_ood = load_file_to_dm(f1_ood, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow... takes 12 mins. It's the accessing\n",
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "dl_test = dm.test_dataloader()\n",
    "dl_ood = dm_ood.all_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with dataloading speeds:\n",
    "- does it help to save the Xy dataset to disc, then load, while keeping in mem?. no not faster at all\n",
    "- does it help to use num_workers > 0? yes 3x faster\n",
    "- the shared dataset wrapper is 10x faster, and less mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get importance matrix from adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.importance_matrix import get_importance_matrix\n",
    "\n",
    "\n",
    "f = f\"{BASE_FOLDER}/checkpoint_last/adapter_model.safetensors\"\n",
    "importance_matrix = get_importance_matrix(f, layers=layers_names)[SKIP::STRIDE, ::DECIMATE]\n",
    "plt.hist(importance_matrix.flatten(), bins=155);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test2 = dm.datasets['test']\n",
    "shape1 = ds_test2[0][0][:, :, 0].shape\n",
    "shape2= importance_matrix.shape\n",
    "np.testing.assert_equal(shape1, shape2, err_msg=\"shape mismatch between ds and importance matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.vae.conv_inception import AutoEncoder, PLAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_subset(df, query, verbose=True):\n",
    "    if query:\n",
    "        df = df.query(query)\n",
    "    acc = (df[\"probe_pred\"] == df[\"y\"]).mean()\n",
    "    if verbose:\n",
    "        print(f\"acc={acc:2.2%},\\tn={len(df)},\\t[{query}] \")\n",
    "    return acc\n",
    "\n",
    "\n",
    "def calc_metrics(dm, trainer, net, use_val=False, verbose=True):\n",
    "    dl_test = dm.test_dataloader()\n",
    "    rt = trainer.predict(net, dataloaders=dl_test)\n",
    "    y_test_pred = np.concatenate(rt)\n",
    "    splits = dm.splits[\"test\"]\n",
    "    df_test = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "    df_test[\"probe_pred\"] = y_test_pred > 0.0\n",
    "\n",
    "    if use_val:\n",
    "        dl_val = dm.val_dataloader()\n",
    "        rv = trainer.predict(net, dataloaders=dl_val)\n",
    "        y_val_pred = np.concatenate(rv)\n",
    "        splits = dm.splits[\"val\"]\n",
    "        df_val = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "        df_val[\"probe_pred\"] = y_val_pred > 0.0\n",
    "\n",
    "        df_test = pd.concat([df_val, df_test])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"probe results on subsets of the data\")\n",
    "    acc = get_acc_subset(df_test, \"\", verbose=verbose)\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True\", verbose=verbose\n",
    "    )  # it was ph told to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False\", verbose=verbose\n",
    "    )  # it was told not to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"llm_ans==label_true\", verbose=verbose\n",
    "    )  # the llm gave the true ans\n",
    "    get_acc_subset(\n",
    "        df_test, \"llm_ans==label_instructed\", verbose=verbose\n",
    "    )  # the llm gave the desired ans\n",
    "    acc_lie_lie = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=verbose\n",
    "    )  # it was told to lie, and it did lie\n",
    "    acc_lie_truth = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    a = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & llm_ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    b = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & llm_ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "    c = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    d = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "    d1 = pd.DataFrame(\n",
    "        [[a, b], [c, d]],\n",
    "        index=[\"instructed_to_lie==False\", \"instructed_to_lie==True\"],\n",
    "        columns=[\"llm_ans==label_instructed\", \"llm_ans!=label_instructed\"],\n",
    "    )\n",
    "    d1 = pd.DataFrame(\n",
    "        [[a, b], [c, d]],\n",
    "        index=[\"tell a truth\", \"tell a lie\"],\n",
    "        columns=[\"did\", \"didn't\"],\n",
    "    )\n",
    "    d1.index.name = \"instructed to\"\n",
    "    d1.columns.name = \"llm gave\"\n",
    "    print(\"probe accuracy for quadrants\")\n",
    "    display(d1.round(2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"⭐PRIMARY METRIC⭐ acc={acc:2.2%} from probe\")\n",
    "        print(f\"⭐SECONDARY METRIC⭐ acc_lie_lie={acc_lie_lie:2.2%} from probe\")\n",
    "    return dict(acc=acc, acc_lie_lie=acc_lie_lie, acc_lie_truth=acc_lie_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(len(dl_train), len(dl_val))\n",
    "b = next(iter(dl_train))\n",
    "x, y = b # b['X'], b['y']\n",
    "print(x.shape, \"x\")\n",
    "if x.ndim == 3:\n",
    "    x = x.unsqueeze(-1)\n",
    "c_in = x.shape[1:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# for b in tqdm(dl_train):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # oh no, this is very slow\n",
    "# g = iter(dl_train)\n",
    "# b = next(g)\n",
    "# b = next(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = PLAE(\n",
    "    c_in=c_in,\n",
    "    steps_per_epoch=len(dl_train),\n",
    "    max_epochs=max_epochs,\n",
    "    lr=lr,\n",
    "    weight_decay=wd,\n",
    "    hs=64,\n",
    "    depth=3,\n",
    "    dropout=0,\n",
    "    n_latent=16,\n",
    "    l1_coeff=l1_coeff, \n",
    "    importance_matrix=importance_matrix,\n",
    ")\n",
    "print(c_in)\n",
    "x1= x[..., 0]\n",
    "with torch.no_grad():\n",
    "    y = net(x1)\n",
    "{k: v.abs().mean() for k, v in y.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(net, input_data=x1, depth=4)  # input_size=(batch_size, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for b in tqdm(dl_train):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.isfinite(x).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in tqdm(dl_val):\n",
    "    x,y=b\n",
    "    # o = net.validation_step(b, 0)\n",
    "    x = x[..., 0]\n",
    "    x = rearrange(x, \"b l h -> b h l\")\n",
    "    o = net.ae(x)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv, plot_hist, rename_pl_test_results\n",
    "\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(0)\n",
    " \n",
    "lr_logger = LearningRateMonitor(logging_interval='step')\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    # devices=2,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"1\",\n",
    "    max_epochs=max_epochs,# * VAE_EPOCH_MULT,\n",
    "    log_every_n_steps=1,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    "    callbacks=[lr_logger],\n",
    ")\n",
    "\n",
    "# LOAD_CHECKPONT = Path('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_52/1_ae.ckpt')\n",
    "LOAD_CHECKPONT = None\n",
    "if LOAD_CHECKPONT:\n",
    "    PLAE.load_from_checkpoint(LOAD_CHECKPONT)\n",
    "else:\n",
    "    trainer1.fit(model=net, train_dataloaders=dl_train, \n",
    "                 val_dataloaders=dl_val # FIXME why does this slow it down with multiple processes?\n",
    "                );\n",
    "\n",
    "    df_hist, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "    plot_hist(df_hist, ['l2_loss', 'l1_loss', 'loss_rec'], logy=True)\n",
    "    plt.show()\n",
    "    plot_hist(df_hist_step, ['loss_rec_step'], logy=True)\n",
    "\n",
    "    display(df_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the LR\n",
    "df_hist, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "df_hist['lr-AdamW'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = net.configure_optimizers()[1][0]\n",
    "total_steps = net.hparams.steps_per_epoch * net.hparams.max_epochs\n",
    "data = []\n",
    "for i in range(total_steps):\n",
    "    s.step()\n",
    "    l_rate = s.get_last_lr()[0]\n",
    "    data.append(l_rate)\n",
    "    # if i % 100 == 0:\n",
    "    #     print(l_rate)\n",
    "\n",
    "pd.Series(data).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/10 [00:04<00:04, 1.10it/s, v_num=295, val/loss_pred=0.0849, val/loss_rec=9.58e+5, train/loss_pred=0.350, train/loss_rec=9.8e+5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize latent space\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def plot_latent(latent):\n",
    "\n",
    "    # plot image of latent space\n",
    "    vmax = latent.abs().max()\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        vmax = latent[i].abs().max()\n",
    "        plt.imshow(\n",
    "            latent[i],\n",
    "            cmap=cm.coolwarm,\n",
    "            interpolation=\"none\",\n",
    "            aspect=\"auto\",\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        plt.xlabel(\"layer\")\n",
    "        plt.ylabel(\"neuron\")\n",
    "        if i < 2:\n",
    "            plt.xlabel(\"\")\n",
    "            plt.xticks([])\n",
    "        if i % 2 == 1:\n",
    "            plt.ylabel(\"\")\n",
    "            plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.colorbar()\n",
    "    # plt.colorbar()\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "    # histogram\n",
    "    latentf = rearrange(latent, \"b n l -> (b n) l\").flatten()\n",
    "    vmax = (latentf.abs().mean() + 5 * latentf.abs().std()).item()\n",
    "    plt.hist(latentf, bins=55, range=[-vmax, vmax], histtype=\"step\")\n",
    "    plt.title(\"latents by layer\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "latent = y[\"latent\"].cpu()  # .reshape(64, 24, 12) # [Batch, Latent, Layer]\n",
    "plot_latent(latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent.shape, latent.diff(dim=1).std(), latent.std(), latent.diff(dim=2).std(), 16*7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # net.save_checkpoint\n",
    "# f = Path(trainer1.log_dir)/\"1_ae.ckpt\"\n",
    "# trainer1.save_checkpoint(f)\n",
    "# # PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_52/1_ae.ckpt')\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "with torch.no_grad():\n",
    "    b = next(iter(dl_train))\n",
    "    y = net.predict_step(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(1)\n",
    "trainer2 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    "    callbacks=[lr_logger],\n",
    ")\n",
    "trainer2.fit(model=net, train_dataloaders=dl_train, \n",
    "            #  val_dataloaders=dl_val\n",
    "             );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist, _ = read_metrics_csv(trainer2.logger.experiment.metrics_file_path)\n",
    "plot_hist(df_hist, ['loss_pred_epoch', 'auroc'])\n",
    "df_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict\n",
    "\n",
    "\n",
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs = trainer2.test(net, dataloaders=[dl_train, dl_val, dl_test, dl_ood])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testval_metrics = calc_metrics(dm, trainer2, net, use_val=True)\n",
    "rs = rename_pl_test_results(rs, [\"train\", \"val\", \"test\", \"ood\"])\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how well does it generalize to other datasets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs2 = trainer1.test(net, dataloaders=[dl_ood])\n",
    "rs2 = rename_pl_test_results(rs2, ks=[\"ood\"])\n",
    "\n",
    "testval_metrics2 = calc_metrics(dm_ood, trainer1, net, use_val=True)\n",
    "rs[\"ood\"][\"acc_lie_lie\"] = testval_metrics2[\"acc_lie_lie\"]\n",
    "rs[\"ood_metrics\"] = rs2[\"ood\"]\n",
    "rs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(2)\n",
    "trainer3 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    ")\n",
    "trainer3.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at hist\n",
    "df_hist, _ = read_metrics_csv(trainer3.logger.experiment.metrics_file_path)\n",
    "plot_hist(df_hist, ['loss_pred', 'acc'])\n",
    "\n",
    "# predict\n",
    "# dl_test = dm.test_dataloader()\n",
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs = trainer3.test(net, dataloaders=[dl_train, dl_val, dl_test, dl_ood])\n",
    "\n",
    "testval_metrics = calc_metrics(dm, trainer3, net, use_val=True)\n",
    "rs = rename_pl_test_results(rs, [\"train\", \"val\", \"test\", \"ood\"])\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"training with x_feats={x_feats} with c={c}\")\n",
    "rs2 = trainer3.test(net, dataloaders=[dl_ood])\n",
    "rs2 = rename_pl_test_results(rs2, ks=[\"ood\"])\n",
    "\n",
    "testval_metrics2 = calc_metrics(dm_ood, trainer3, net, use_val=True)\n",
    "rs[\"ood\"][\"acc_lie_lie\"] = testval_metrics2[\"acc_lie_lie\"]\n",
    "rs[\"ood_metrics\"] = rs2[\"ood\"]\n",
    "rs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
