{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a tokenized autoencoder\n",
    "\n",
    "E.g. from IRIS https://github.dev/eloialonso/iris where sparsity is enforced by the tokenization/embedding quantisation bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "# from safetensors.torch import save_file, safe_open\n",
    "import safetensors.torch\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.llms.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.llms.phi.model_phi import PhiForCausalLMWHS\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "from src.datasets.act_dm import ActivationDataModule\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.style.use(\"seaborn-v0_8\")\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(\"paper\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    ")\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TQDM_MININTERVAL\"] = \"9\"\n",
    "# os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramsnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# params\n",
    "batch_size = 32\n",
    "lr = 4e-3\n",
    "wd = 0  # 1e-5\n",
    "\n",
    "MAX_ROWS = 2000\n",
    "\n",
    "SKIP = 15  # skip initial N layers\n",
    "STRIDE = 4  # skip every N layers\n",
    "DECIMATE = 1  # discard N features for speed\n",
    "\n",
    "device = \"cuda:0\"\n",
    "max_epochs = 840\n",
    "\n",
    "l1_coeff = 1e-2  # 0.5  # neel uses 3e-4 ! https://github.dev/neelnanda-io/1L-Sparse-Autoencoder/blob/bcae01328a2f41d24bd4a9160828f2fc22737f75/utils.py#L106, but them they sum l1 where mean l2\n",
    "# x_feats=x_feats. other use 1e-1\n",
    "# in ai saftey foundation. They use l1_coefficient=Parameter(max=0.03, min=0.008),\n",
    "\n",
    "\n",
    "BASE_FOLDER = Path(\n",
    "    \"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/\"\n",
    ")\n",
    "layers_names = (\"fc1\", \"Wqkv\",\n",
    "                #  \"fc2\", \"out_proj\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load hidden state from a previously loaded adapter\n",
    "# the columns with _base are from the base model, and adapt from adapter\n",
    "# FROM TRAINING TRUTH\n",
    "f1_val = next(iter(BASE_FOLDER.glob(\"hidden_states/.ds/ds_valtest_*\")))\n",
    "f1_ood = next(iter(BASE_FOLDER.glob(\"hidden_states/.ds/ds_OOD_*\")))\n",
    "f1_val, f1_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = (\n",
    "    [\"binary_ans_base\", \"binary_ans_adapt\"]\n",
    "    + [f\"end_residual_{layer}_base\" for layer in layers_names]\n",
    "    + [f\"end_residual_{layer}_adapt\" for layer in layers_names]\n",
    ")\n",
    "\n",
    "y_thresh = 0.5\n",
    "def get_label(ds):\n",
    "    # What are we predicting? Here it's whether the adapter is more truthfull than the base\n",
    "    # return ds[\"binary_ans_base\"] - ds[\"binary_ans_adapt\"]\n",
    "\n",
    "    # if base is truthfull. relative token prob, flipped for truth\n",
    "    return ds[\"binary_ans_base\"]\n",
    "\n",
    "\n",
    "def ds2xy_batched(ds):\n",
    "    data = []\n",
    "    for layer in layers_names:\n",
    "        # Stack the base and adapter representations as a 4th dim\n",
    "        X1 = [ds[f\"end_residual_{layer}_base\"], ds[f\"end_residual_{layer}_adapt\"]]\n",
    "        X1 = rearrange(X1, \"versions b l f  -> b l f versions\")[..., 0]\n",
    "        data.append(X1)\n",
    "\n",
    "    # concat layers\n",
    "    # x = rearrange(data, 'b parts l f v -> b l (parts f) v')\n",
    "    X = torch.concat(data, dim=2)[:, SKIP::STRIDE, ::DECIMATE]\n",
    "\n",
    "    \n",
    "    y = get_label(ds)\n",
    "    return dict(X=X, y=y)\n",
    "\n",
    "\n",
    "def prepare_ds(ds):\n",
    "    \"\"\"\n",
    "    prepare a dataset for training\n",
    "\n",
    "    this should front load much of the computation\n",
    "    it should restrict it to the needed rows X and y\n",
    "\n",
    "    \"\"\"\n",
    "    ds = (\n",
    "        ds.with_format(\"torch\")\n",
    "        .select_columns(input_columns)\n",
    "        .map(ds2xy_batched, batched=True, batch_size=128, remove_columns=input_columns)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_file_to_dm(f, stage):\n",
    "    ds1 = Dataset.from_file(str(f1_val), in_memory=True).with_format(\"torch\")\n",
    "    ds1 = filter_ds_to_known(ds1, verbose=True, true_col=\"truth\")\n",
    "    ds = prepare_ds(ds1)\n",
    "\n",
    "    # limit size\n",
    "    MAX_SAMPLES = min(len(ds), MAX_ROWS * 2)\n",
    "    ds = ds.select(range(0, MAX_SAMPLES))\n",
    "    ds1 = ds1.select(range(0, MAX_SAMPLES))\n",
    "\n",
    "    dm = ActivationDataModule(ds, f.stem, batch_size=batch_size, num_workers=0)\n",
    "    dm.setup(stage)\n",
    "    dm.ds_orig = ds1\n",
    "    assert (get_label(dm.ds_orig)==dm.ds['y']).all()\n",
    "    return dm\n",
    "\n",
    "\n",
    "dm = load_file_to_dm(f1_val, \"train\")\n",
    "dm_ood = load_file_to_dm(f1_ood, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_ood.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "dl = dm.test_dataloader()\n",
    "dl_ood = dm_ood.all_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with dataloading speeds:\n",
    "- does it help to save the Xy dataset to disc, then load, while keeping in mem?. no not faster at all\n",
    "- does it help to use num_workers > 0? yes 3x faster\n",
    "- the shared dataset wrapper is 10x faster, and less mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get importance matrix from adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.importance_matrix import get_importance_matrix\n",
    "\n",
    "\n",
    "f = f\"{BASE_FOLDER}/checkpoint_last/adapter_model.safetensors\"\n",
    "importance_matrix = get_importance_matrix(f, layers=layers_names)[\n",
    "    SKIP::STRIDE, ::DECIMATE\n",
    "]\n",
    "\n",
    "\n",
    "# importance_matrix = importance_matrix ** 3 # square to make it positive\n",
    "importance_matrix = (importance_matrix - 1).abs() ** 2\n",
    "\n",
    "\n",
    "# importance_matrix = importance_matrix / (0.1*importance_matrix.std())\n",
    "# importance_matrix = importance_matrix + 1\n",
    "\n",
    "# square to make it positive\n",
    "# importance_matrix = importance_matrix.clamp(0, None)\n",
    "# importance_matrix -= importance_matrix.mean() - 1\n",
    "\n",
    "s = importance_matrix.std()\n",
    "importance_matrix = (importance_matrix > s * 3) * 1.0\n",
    "print(f\"keeping top {importance_matrix.mean():2.2%} of features\")\n",
    "importance_matrix = importance_matrix / importance_matrix.mean()\n",
    "\n",
    "plt.hist(importance_matrix.flatten(), bins=155)\n",
    "\n",
    "importance_matrix.mean()\n",
    "\n",
    "importance_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((importance_matrix>0)*1.0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test2 = dm.datasets['test']\n",
    "# shape1 = ds_test2[0][0].shape\n",
    "# shape2= importance_matrix.shape\n",
    "# np.testing.assert_equal(shape1, shape2, err_msg=\"shape mismatch between ds and importance matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.ds import ds2df\n",
    "from src.helpers.pandas_classification_report import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def get_acc_subset(df, query, verbose=True):\n",
    "    # assert (df[\"y\"].mean() < 0).any(), \"y should be [-1, 1]\"\n",
    "    assert (df[\"y\"].min() >= 0).all(), \"y should be [-1, 1]\"\n",
    "    assert (df[\"y\"].max() <= 1).all(), \"y should be [-1, 1]\"\n",
    "    assert (df[\"probe_pred\"].min() >= 0).all(), \"pred should be [0,1]\"\n",
    "    assert (df[\"probe_pred\"].max() <= 1).all(), \"pred should be [0,1]\"\n",
    "\n",
    "    if query:\n",
    "        df = df.query(query)\n",
    "    acc = ((df[\"probe_pred\"] > 0.5) == (df[\"y\"] > y_thresh)).mean()\n",
    "    # print(df['probe_pred'])\n",
    "    # print(df['y'])\n",
    "    if verbose:\n",
    "        print(f\"acc={acc:2.2%},\\tn={len(df)},\\t[{query}] \")\n",
    "    return acc\n",
    "\n",
    "\n",
    "def calc_metrics(dm, net, trainer=None, split=\"test\", verbose=True):\n",
    "    # predict\n",
    "    dl = dm.create_dataloader(split)\n",
    "    if trainer is None:\n",
    "        trainer = pl.Trainer(\n",
    "            logger=False, enable_progress_bar=False, enable_model_summary=False, barebones=True,\n",
    "        )\n",
    "    rt = trainer.predict(net, dataloaders=dl)\n",
    "    y_test_pred = np.concatenate(rt)\n",
    "\n",
    "    # get original df\n",
    "    splits = dm.splits[split]\n",
    "    df = ds2df(dm.ds_orig).rename(columns=lambda s: s.replace(\"_base\", \"\"))\n",
    "    df[\"y\"] = dm.ds[\"y\"]\n",
    "    df[\"label_true\"]\n",
    "    df_test = df.iloc[splits[0] : splits[1]].copy()\n",
    "    df_test[\"probe_pred\"] = y_test_pred\n",
    "    \n",
    "    if verbose:\n",
    "        target_names = ['true','lie']\n",
    "        df_cls = classification_report(df_test[\"y\"]>y_thresh, df_test[\"probe_pred\"] > 0.5, target_names=target_names)\n",
    "        display(df_cls.format(precision=2))\n",
    "        display(confusion_matrix(df_test[\"y\"]>y_thresh, df_test[\"probe_pred\"] > 0.5, target_names=target_names, normalize='all',).format(precision=2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"probe results on subsets of the data for {split}\")\n",
    "    acc = get_acc_subset(df_test, \"\", verbose=verbose)\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True\", verbose=verbose\n",
    "    )  # it was ph told to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False\", verbose=verbose\n",
    "    )  # it was told not to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"ans==label_true\", verbose=verbose\n",
    "    )  # the llm gave the true ans\n",
    "    get_acc_subset(\n",
    "        df_test, \"ans==label_instructed\", verbose=verbose\n",
    "    )  # the llm gave the desired ans\n",
    "    acc_lie_lie = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans==label_instructed\", verbose=verbose\n",
    "    )  # it was told to lie, and it did lie\n",
    "    acc_lie_truth = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans!=label_instructed\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    a = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    b = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "    c = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    d = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "\n",
    "    df_quad = pd.DataFrame(\n",
    "        [[a, b], [c, d]],\n",
    "        index=[\"tell a truth\", \"tell a lie\"],\n",
    "        columns=[\"did\", \"didn't\"],\n",
    "    )\n",
    "    df_quad.index.name = \"instructed to\"\n",
    "    df_quad.columns.name = \"llm gave\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"⭐PRIMARY METRIC⭐ acc={acc:2.2%} from probe on {split}\")\n",
    "        print(\n",
    "            f\"⭐SECONDARY METRIC⭐ acc_lie_lie={acc_lie_lie:2.2%} from probe on {split}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n\\nprobe accuracy for quadrants (doesn't need to add up):\")\n",
    "    print(df_quad.round(2).to_markdown())\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    return dict(\n",
    "        acc=acc,\n",
    "        acc_lie_lie=acc_lie_lie,\n",
    "        acc_lie_truth=acc_lie_truth,\n",
    "        df_test=df_test,\n",
    "        df_confusion=df_quad,\n",
    "    )\n",
    "\n",
    "\n",
    "# r = testval_metrics = calc_metrics(dm, net)\n",
    "# r['df_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize latent space\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def plot_latent(latent):\n",
    "    # plot image of latent space \n",
    "    vmax = latent.abs().max()\n",
    "    for i in range(4): # first 4 batches\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        vmax = latent[i].max()\n",
    "        plt.imshow(\n",
    "            latent[i],\n",
    "            cmap=cm.coolwarm,\n",
    "            interpolation=\"none\",\n",
    "            aspect=\"auto\",\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        plt.title(f\"batch {i}\")\n",
    "        plt.ylabel(\"layer\")\n",
    "        plt.xlabel(\"neuron\")\n",
    "        if i < 2:\n",
    "            plt.xlabel(\"\")\n",
    "            plt.xticks([])\n",
    "        if i % 2 == 1:\n",
    "            plt.ylabel(\"\")\n",
    "            plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.colorbar()\n",
    "    # plt.colorbar()\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "    # histogram\n",
    "    latentf = rearrange(latent, \"b l n -> (b n) l\").T#.flatten()\n",
    "    print(latentf.shape)\n",
    "    for i in range(latent.shape[1]):\n",
    "        plt.hist(latentf[i], bins=25,\n",
    "                histtype=\"step\", log=True, label=f\"layer {i}\", density=True)\n",
    "    plt.title(\"latents by layer\")\n",
    "    plt.xlabel('latent magnitude')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# plot_latent(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv, plot_hist, rename_pl_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vae.tae import PL_TAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dl_train), len(dl_val))\n",
    "b = next(iter(dl_train))\n",
    "x, y = b  # b['X'], b['y']\n",
    "print(x.shape, \"x\")\n",
    "if x.ndim == 3:\n",
    "    x = x.unsqueeze(-1)\n",
    "c_in = x.shape[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PL_TAE(\n",
    "    c_in=c_in,\n",
    "    steps_per_epoch=len(dl_train),\n",
    "    max_epochs=max_epochs,\n",
    "    lr=lr,\n",
    "    dropout=0.2,\n",
    "    encoder_sizes=[64,64, 64, 64],\n",
    "    weight_decay=wd,\n",
    "    n_latent=512,  # there will be layers * n_latent latent features\n",
    "    importance_matrix=importance_matrix,\n",
    "    tokens_per_layer=6,\n",
    "    probe_embedd_dim=512,\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_in)\n",
    "x1 = x[..., 0]\n",
    "with torch.no_grad():\n",
    "    y = net(x1)\n",
    "# {k: v.abs().mean() for k, v in y.items()}, {k: v.shape for k, v in y.items()}\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(net, input_data=x1, depth=4, col_names=(\"input_size\", \"output_size\", \"num_params\",))  # input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     o = net.predict_step(b)\n",
    "#     l1_loss, l2_loss, loss, acts, h_reconstructed = net.ae(b[0])\n",
    "#     # l1_loss, l2_loss, loss, latent, h_rec = self.ae(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scratch\n",
    "\n",
    "# d = safetensors.torch.load_file(LOAD_AE_SAFETENSORS)\n",
    "# d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(0)\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"1\",\n",
    "    max_epochs=max_epochs,  # * VAE_EPOCH_MULT,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose,\n",
    "    # enable_model_summary=verbose,\n",
    ")\n",
    "\n",
    "# LOAD_CHECKPONT = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_362/checkpoints/epoch=839-step=8400.ckpt'\n",
    "LOAD_CHECKPONT = None\n",
    "\n",
    "LOAD_AE_SAFETENSORS='/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_362/ae.safetensors'\n",
    "# LOAD_AE_SAFETENSORS = None # good for when probe changed but no ae\n",
    "if LOAD_CHECKPONT:\n",
    "    print(f\"loading from {LOAD_CHECKPONT}\")\n",
    "    PL_TAE.load_from_checkpoint(LOAD_CHECKPONT)\n",
    "elif LOAD_AE_SAFETENSORS is not None:\n",
    "    print(f\"loading ae from {LOAD_AE_SAFETENSORS}\")\n",
    "    net.ae.load_state_dict(safetensors.torch.load_file(LOAD_AE_SAFETENSORS))\n",
    "else:\n",
    "    trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    df_hist, df_hist_step = read_metrics_csv(\n",
    "        trainer1.logger.experiment.metrics_file_path\n",
    "    )\n",
    "    plot_hist(df_hist, [\"reconstruction_loss\", \"commitment_loss\", \"loss_rec\"], logy=True)\n",
    "    # plt.show()\n",
    "    # plot_hist(df_hist_step, ['loss_rec_step'], logy=True)\n",
    "\n",
    "    display(df_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(df_hist, [\"reconstruction_loss\", \"commitment_loss\", \"loss_rec\"], logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs3r = trainer1.test(\n",
    "    net, dataloaders=[dl_train, dl_val, dl, dl_ood], verbose=False\n",
    ")\n",
    "rs3 = rename_pl_test_results(rs3r, [\"train\", \"val\", \"test\", \"ood\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x[..., 0]\n",
    "with torch.no_grad():\n",
    "    y = net(x1)\n",
    "\n",
    "print('QC: view latent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('z before quant')\n",
    "plot_latent(y['z'][..., 0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer1.checkpoint_callback.best_model_path\n",
    "print(best_model_path)\n",
    "\n",
    "\n",
    "\n",
    "f = trainer1.log_dir + \"/ae.safetensors\"\n",
    "safetensors.torch.save_file(net.ae.state_dict(), f)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('z after quant')\n",
    "plot_latent(y['z_q'][..., 0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('z_q after embed(z.argmin())')\n",
    "# latent = y[\"latent_probe\"].cpu()  # .reshape(64, 24, 12) # [Batch, Latent, Layer]\n",
    "# plot_latent(latent[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity = ((latent!=0) * 1.0).mean()\n",
    "# print(f'QC: latent usage {sparsity:2.2%}')\n",
    "# # sparsity_mult = y['l1_losses']/y['l1_raw']\n",
    "# # print(f'QC: sparsity multiplier per layer {sparsity_mult.mean(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(1)\n",
    "trainer2 = pl.Trainer(\n",
    "    # precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose,\n",
    "    # enable_model_summary=verbose,\n",
    "    # callbacks=[lr_logger],\n",
    ")\n",
    "trainer2.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist, _ = read_metrics_csv(trainer2.logger.experiment.metrics_file_path)\n",
    "plot_hist(df_hist, [\"loss_pred_epoch\", \"auroc\"])\n",
    "df_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs3r = trainer2.test(\n",
    "    net, dataloaders=[dl_train, dl_val, dl, dl_ood], verbose=False\n",
    ")\n",
    "rs3 = rename_pl_test_results(rs3r, [\"train\", \"val\", \"test\", \"ood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(dm.ds['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO wrong?\n",
    "a = calc_metrics(dm, net, split=\"test\", verbose=False)\n",
    "b = calc_metrics(dm, net, split=\"val\", verbose=False)\n",
    "c = calc_metrics(dm_ood, net, split=\"all\", verbose=False)\n",
    "df_metrics = pd.DataFrame([a, b, c], index=[\"test\", \"val\", \"ood\"]).iloc[:, :3]\n",
    "print(df_metrics.round(2).to_markdown())\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.ae_mode(2)\n",
    "trainer3 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False, enable_model_summary=False\n",
    ")\n",
    "trainer3.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds2df(dm.ds_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at hist\n",
    "df_hist, _ = read_metrics_csv(trainer3.logger.experiment.metrics_file_path)\n",
    "plot_hist(df_hist, [\"loss_pred\", \"acc\"])\n",
    "\n",
    "rs3r = trainer3.test(\n",
    "    net, dataloaders=[dl_train, dl_val, dl, dl_ood], verbose=False\n",
    ")\n",
    "rs3 = rename_pl_test_results(rs3r, [\"train\", \"val\", \"test\", \"ood\"])\n",
    "\n",
    "# predict\n",
    "a = calc_metrics(dm, net, trainer1, \"test\")\n",
    "b = calc_metrics(dm, net, trainer1, \"val\")\n",
    "pd.DataFrame([a, b], index=[\"test\", \"val\"])\n",
    "\n",
    "c = calc_metrics(dm_ood, net, trainer1, \"all\")\n",
    "pd.DataFrame([a, b, c], index=[\"test\", \"val\", \"ood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = calc_metrics(dm, net, trainer1, \"test\")\n",
    "b = calc_metrics(dm, net, trainer1, \"val\")\n",
    "c = calc_metrics(dm_ood, net, trainer1, \"all\")\n",
    "df_metrics = pd.DataFrame([a, b, c], index=[\"test\", \"val\", \"ood\"]).iloc[:, :3]\n",
    "print(df_metrics.round(3).to_markdown())\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
