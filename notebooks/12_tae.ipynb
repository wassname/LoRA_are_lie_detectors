{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a tokenized autoencoder\n",
    "\n",
    "E.g. from IRIS https://github.dev/eloialonso/iris where sparsity is enforced by the tokenization/embedding quantisation bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "# from safetensors.torch import save_file, safe_open\n",
    "import safetensors.torch\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    LoftQConfig,\n",
    "    IA3Config,\n",
    ")\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.llms.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.llms.phi.model_phi import PhiForCausalLMWHS\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "from src.datasets.act_dm import ActivationDataModule\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.style.use(\"seaborn-v0_8\")\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(\"paper\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    ")\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TQDM_MININTERVAL\"] = \"9\"\n",
    "# os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramsnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "# params\n",
    "batch_size = 32\n",
    "lr = 4e-3\n",
    "wd = 0  # 1e-5\n",
    "\n",
    "MAX_ROWS = 2000\n",
    "\n",
    "SKIP = 15  # skip initial N layers\n",
    "STRIDE = 4  # skip every N layers\n",
    "DECIMATE = 1  # discard N features for speed\n",
    "\n",
    "device = \"cuda:0\"\n",
    "max_epochs = 84\n",
    "max_ae_epochs = 84\n",
    "\n",
    "l1_coeff = 1e-2  # 0.5  # neel uses 3e-4 ! https://github.dev/neelnanda-io/1L-Sparse-Autoencoder/blob/bcae01328a2f41d24bd4a9160828f2fc22737f75/utils.py#L106, but them they sum l1 where mean l2\n",
    "# x_feats=x_feats. other use 1e-1\n",
    "# in ai saftey foundation. They use l1_coefficient=Parameter(max=0.03, min=0.008),\n",
    "\n",
    "\n",
    "BASE_FOLDER = Path(\n",
    "    \"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/\"\n",
    ")\n",
    "layers_names = (\"fc1\", \"Wqkv\",\n",
    "                 \"fc2\", \"out_proj\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814a7519a8be4fa9b451de2a6c654afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(51200, 2560)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    # cfg.model,\n",
    "    device='cpu',\n",
    "    bnb=False,\n",
    "    trust_remote_code=True,\n",
    "    # model_class=PhiForCausalLMWHS, # ti add hidden states\n",
    "    # bnb=False,\n",
    ")\n",
    "# get embedding layer\n",
    "embedding = model.model.embed_tokens.eval().cpu()\n",
    "del model\n",
    "vocab_size, embed_dim = embedding.weight.shape\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/hidden_states/.ds/ds_valtest_8b8fd6070504d5ef'),\n",
       " PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_24/hidden_states/.ds/ds_OOD_a41d3a61513ade30'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load hidden state from a previously loaded adapter\n",
    "# the columns with _base are from the base model, and adapt from adapter\n",
    "# FROM TRAINING TRUTH\n",
    "f1_val = next(iter(BASE_FOLDER.glob(\"hidden_states/.ds/ds_valtest_*\")))\n",
    "f1_ood = next(iter(BASE_FOLDER.glob(\"hidden_states/.ds/ds_OOD_*\")))\n",
    "f1_val, f1_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 74.39% based on knowledge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709251cac9564758a229cda65eea3023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-02 12:52:36.432\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.act_dm\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mconverting datasets this may take a while... ds_valtest_8b8fd6070504d5ef train\u001b[0m\n",
      "2024-02-02T12:52:36.432514+0800 INFO converting datasets this may take a while... ds_valtest_8b8fd6070504d5ef train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup train\n",
      "select rows are 74.39% based on knowledge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e50ed0894e418fb833267ae5d5b14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-02 12:54:07.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.datasets.act_dm\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mconverting datasets this may take a while... ds_OOD_a41d3a61513ade30 all\u001b[0m\n",
      "2024-02-02T12:54:07.991327+0800 INFO converting datasets this may take a while... ds_OOD_a41d3a61513ade30 all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup all\n"
     ]
    }
   ],
   "source": [
    "input_columns = (\n",
    "    [\"binary_ans_base\", \"binary_ans_adapt\"]\n",
    "    + [f\"end_residual_{layer}_base\" for layer in layers_names]\n",
    "    + [f\"end_residual_{layer}_adapt\" for layer in layers_names]\n",
    ")\n",
    "\n",
    "y_thresh = 0.5\n",
    "def get_label(ds):\n",
    "    # What are we predicting? Here it's whether the adapter is more truthfull than the base\n",
    "    # return ds[\"binary_ans_base\"] - ds[\"binary_ans_adapt\"]\n",
    "\n",
    "    # if base is truthfull. relative token prob, flipped for truth\n",
    "    return ds[\"binary_ans_base\"]\n",
    "\n",
    "\n",
    "def ds2xy_batched(ds):\n",
    "    data = []\n",
    "    for layer in layers_names:\n",
    "        # Stack the base and adapter representations as a 4th dim\n",
    "        X1 = [ds[f\"end_residual_{layer}_base\"], ds[f\"end_residual_{layer}_adapt\"]]\n",
    "        X1 = rearrange(X1, \"versions b l f  -> b l f versions\")[..., 0]\n",
    "        data.append(X1)\n",
    "\n",
    "    # concat layers\n",
    "    # x = rearrange(data, 'b parts l f v -> b l (parts f) v')\n",
    "    X = torch.concat(data, dim=2)[:, SKIP::STRIDE, ::DECIMATE]\n",
    "\n",
    "    \n",
    "    y = get_label(ds)\n",
    "    return dict(X=X, y=y)\n",
    "\n",
    "\n",
    "def prepare_ds(ds):\n",
    "    \"\"\"\n",
    "    prepare a dataset for training\n",
    "\n",
    "    this should front load much of the computation\n",
    "    it should restrict it to the needed rows X and y\n",
    "\n",
    "    \"\"\"\n",
    "    ds = (\n",
    "        ds.with_format(\"torch\")\n",
    "        .select_columns(input_columns)\n",
    "        .map(ds2xy_batched, batched=True, batch_size=128, remove_columns=input_columns)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def load_file_to_dm(f, stage):\n",
    "    ds1 = Dataset.from_file(str(f1_val), in_memory=True).with_format(\"torch\")\n",
    "    ds1 = filter_ds_to_known(ds1, verbose=True, true_col=\"truth\")\n",
    "    ds = prepare_ds(ds1)\n",
    "\n",
    "    # limit size\n",
    "    MAX_SAMPLES = min(len(ds), MAX_ROWS * 2)\n",
    "    ds = ds.select(range(0, MAX_SAMPLES))\n",
    "    ds1 = ds1.select(range(0, MAX_SAMPLES))\n",
    "\n",
    "    dm = ActivationDataModule(ds, f.stem, batch_size=batch_size, num_workers=0)\n",
    "    dm.setup(stage)\n",
    "    dm.ds_orig = ds1\n",
    "    assert (get_label(dm.ds_orig)==dm.ds['y']).all()\n",
    "    return dm\n",
    "\n",
    "\n",
    "# HACK this is so we can rerun all cells notebook without rerunning this slow step\n",
    "try:\n",
    "    print(dm)\n",
    "except NameError:\n",
    "    dm = load_file_to_dm(f1_val, \"train\")\n",
    "    dm_ood = load_file_to_dm(f1_ood, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': <torch.utils.data.dataset.TensorDataset at 0x7fe0a509db50>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_ood.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n",
    "dl = dm.test_dataloader()\n",
    "dl_ood = dm_ood.all_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with dataloading speeds:\n",
    "- does it help to save the Xy dataset to disc, then load, while keeping in mem?. no not faster at all\n",
    "- does it help to use num_workers > 0? yes 3x faster\n",
    "- the shared dataset wrapper is 10x faster, and less mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get importance matrix from adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wqkv torch.Size([32, 7680])\n",
      "out_proj torch.Size([32, 2560])\n",
      "fc1 torch.Size([32, 10240])\n",
      "fc2 torch.Size([32, 2560])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEQCAYAAABLMTQcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgSklEQVR4nO3de3BU9f3/8efZTcLNkCahktvvK5aK10mFqdVgau2wCYxLqjQBkmHGMdUyQLS1KggoaLgJWK/YkaZN0XaKiQgZSbjYUDGD2HGmKuNkHKvjZTSwbiDZEAFj9nJ+f2BWrppNFjb57Osxs5PZ8zmf9f32hNeefPbkxLJt20ZERIziiHUBIiISfQp3EREDKdxFRAykcBcRMZDCXUTEQAp3EREDKdxFRAykcBcRMZDCXUTEQAmxLuB86Oz8imAwFNGc1NQR+HxHz1FFA1u89h6vfYN6H0y9O50ORo4c9r37xUW4B4MhAoHeh7tlfTsv3m7OEK+9x2vfoN7BzN61LCMiYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUrh/h55rYEVEBhuF+1n4AyHS05NJSRke61JERCKmcD8Dy4LEBAdz1/ybpCSnzuBFZNBRuH+Hr74OxLoEEZE+UbiLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGijjcN2/eTFFREUVFRdx666188sknAFRXVzNlyhQKCgqorKzE7/cDEAqFWLNmDZMnT8blcvHMM89g2zYAXV1dLFy4MDyvpqYm/N/x+XzMmTOHm266icmTJ7Nr165o9CsiEhciCvePP/6Yxx57jOeee476+noKCgpYunQpTU1NvPTSS2zatImdO3fS3t7Ohg0bAKipqaG5uZn6+noaGhrYu3cv27dvB2DdunUEAgF27NjBiy++yIYNG9i3bx8AlZWVjBs3ju3bt1NdXc1DDz2Ex+OJbvciIoaKKNx/9KMf0dTURHp6OoFAgAMHDpCamkpjYyNut5vk5GScTidlZWXU1dUB0NjYSHFxMUlJSQwdOpSSkpKTxmbOnIllWaSmpuJ2u6mrqyMQCLB7925KS0sByMnJIT8/n/r6+j43almRPfozd7A/4rHneO5bvce+hkjr7Y2ESEMyMTGR//73v/z+97/nq6++orq6mmeeeYbx48eH98nIyAifZXs8HjIyMno91tzcjM/no6ur66zzIpWaOqJP83qkpyf3a/5gFI89Q/z2DerdNBGHO8BPf/pT9u7dy65du5g9eza5ublYp7yl9Dy3bfu0MYfD8Z1jPWvyZ5sXKZ/vKMFgqNf7OxyQlvbtwW5r+5JvSjKeZR3/Ro+nniF++wb1Pth6dzodvTphjSgtW1pa+M9//hN+7nK5SExMJBQK4fV6w9u9Xi9ZWVkAZGdnnzaWmZn5nWPp6ekMGTKE1tbWM87rC9uO7NGfuYP9EY89x3Pf6j32NURab29EFO6HDx/m7rvv5osvvgCgqakJh8PBbbfdxrZt2+js7CQUClFTU0NhYSEABQUFbNmyhe7ubrq6uti8efNJY7W1tYRCITo6OmhoaKCwsBCn08mkSZPYuHEjAPv372fPnj24XK5IyhURiVsRLctceeWV3H///fz2t7/F4XAwcuRI/vKXv3D55Zfz8ccfU1ZWRiAQYMKECcydOxeAGTNm0NLSwrRp0/D7/bhcLoqLiwGoqKhgxYoVFBUV4ff7KSsrIy8vD4AlS5awdOlSpk6dSiAQYOHChYwZMya63YuIGMqy7UhO9Acnn+8ogUBka+7p6cnctuwVnls6mUOHBs96XH9ZFowalRxXPUP89g3qfbD1npBwDtbcRURkcFC4i4gYSOEuImIghbuIiIEU7iIiBlK4i4gYSOEuImIghbuIiIEU7iIiBlK4i4gYSOEuImIghbuIiIEU7iIiBlK4i4gYSOEuImIghbuIiIEU7iIiBlK4i4gYSOEuImIghbuIiIEU7iIiBlK4i4gYSOEuImIghbuIiIESIp3wwgsvsHHjRizLYtiwYTzwwAPk5uYyadIkhg0bhtPpBMDtdjN79my6urp4+OGH2bdvH8FgkNtvv53S0lIAfD4fixYt4rPPPiMYDDJ//nxcLhcAn3/+OYsXL6a9vR2Hw8GyZcsYP358FFsXETFXROH+9ttvU1VVxebNm0lLS2P37t3MmzePzZs3c+zYMXbt2oVlWSfNWbduHYFAgB07dtDR0UFpaSmXXXYZV199NZWVlYwbN47169fT0tLCzJkzufLKK8nMzOSee+6hpKSEmTNn0tzczJw5c/jXv/7F8OHDo/o/QETERBGFe0pKCsuXLyctLQ2A3Nxc2traeOONNxg+fDjl5eW0tbVx3XXXcc899zBs2DAaGxtZuXIllmWRmpqK2+2mrq6Oq666it27d7Njxw4AcnJyyM/Pp76+nptvvpn333+fX//61wBcddVVjBkzhtdee42bbrqpT42e8p4T0b6RzB3senqNp54hfvsG9X7iV5NEFO5jx45l7NixAIRCIVatWsWNN94IwMSJE1m8eDGWZXHfffexevVqKisr8Xg8ZGRkhF8jIyOD5uZmfD4fXV1dp415PB48Hg+jRo0iMTExPDZ69GgOHDjQpyZTU0f0aV6P9PTkfs0fjOKxZ4jfvkG9mybiNXeAI0eOsGDBAtrb26mqqmLkyJFMmzYtPD5nzhxmz55NZWUltm2ftlTjcDiwbRvgrGOnbu8Z6wuf7yjBYKjX+zsckJb27cFua/uSb8o1nmUd/0aPp54hfvsG9T7Yenc6Hb06YY04LT/55BNKSkq44IILeP755xk5ciQNDQ28++674X1s2yYh4fj7RnZ2Nl6vNzzm9XrJzMwkPT2dIUOG0NraetpYVlYWhw4dIhAIhMdaW1vJzMyMtNwTaors0Z+5g/0Rjz3Hc9/qPfY1RFpvb0QU7gcOHGDWrFlMnz6dtWvXMmTIEAA+/fRTHn/8cbq7uwkEAlRXV+N2uwEoKCigtraWUChER0cHDQ0NFBYW4nQ6mTRpEhs3bgRg//797NmzB5fLxejRo7nsssuoq6sD4L333uPDDz9k4sSJkZQrIhK3IlqWqa6uprOzk61bt7J169bw9meffZZDhw5x8803EwgEyMvL4+677wagoqKCFStWUFRUhN/vp6ysjLy8PACWLFnC0qVLmTp1KoFAgIULFzJmzBgAHnvsMZYsWcLf//53AB5//HFSUlKi0LKIiPks247kRH9w8vmOEghEtuaenp7Mbcte4bmlkzl0aPCsx/WXZcGoUclx1TPEb9+g3gdb7wkJ52jNXUREBj6Fu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBgoonB/4YUXKCoq4le/+hUzZ87k3XffBaC6upopU6ZQUFBAZWUlfr8fgFAoxJo1a5g8eTIul4tnnnkG27YB6OrqYuHCheF5NTU14f+Oz+djzpw53HTTTUyePJldu3ZFq18RkbjQ63B/++23qaqq4vnnn2fr1q3MmTOHefPm0dTUxEsvvcSmTZvYuXMn7e3tbNiwAYCamhqam5upr6+noaGBvXv3sn37dgDWrVtHIBBgx44dvPjii2zYsIF9+/YBUFlZybhx49i+fTvV1dU89NBDeDye6HcvImKoXod7SkoKy5cvJy0tDYDc3Fza2tpobGzE7XaTnJyM0+mkrKyMuro6ABobGykuLiYpKYmhQ4dSUlJy0tjMmTOxLIvU1FTcbjd1dXUEAgF2795NaWkpADk5OeTn51NfX9+vRi0rskd/5g72Rzz2HM99q/fY1xBpvb2R0Nsdx44dy9ixY4Hjyy2rVq3ixhtvxOPxMH78+PB+GRkZ4bNsj8dDRkZGr8eam5vx+Xx0dXWddV5fpKaO6PNcgPT05H7NH4zisWeI375BvZum1+He48iRIyxYsID29naqqqr4wx/+gHXK20nPc9u2TxtzOBzfOdazJn+2eX3h8x0lGAz1en+HA9LSvj3YbW1f8k1ZxrOs49/o8dQzxG/foN4HW+9Op6NXJ6wRhfsnn3zC3Llzyc3N5YknnmDIkCFkZWXh9XrD+3i9XrKysgDIzs4+bSwzM/OksZycnJPG0tPTGTJkCK2trYwePTo89uMf/ziSUk8TyYE7dV/bjmy+CeKxZ4jfvkG9m9Z7r0+HDxw4wKxZs5g+fTpr165lyJAhABQUFLBt2zY6OzsJhULU1NRQWFgYHtuyZQvd3d10dXWxefPmk8Zqa2sJhUJ0dHTQ0NBAYWEhTqeTSZMmsXHjRgD279/Pnj17cLlc0e5dRMRYvT5zr66uprOzk61bt7J169bw9qqqKoqLiykrKyMQCDBhwgTmzp0LwIwZM2hpaWHatGn4/X5cLhfFxcUAVFRUsGLFCoqKivD7/ZSVlZGXlwfAkiVLWLp0KVOnTiUQCLBw4ULGjBkTxbZFRMxm2bZpP4yczuc7SiAQ2Zp7enoyty17heeWTubQocGzHtdflgWjRiXHVc8Qv32Deh9svSck9G7NXb+hKiJiIIW7iIiBFO4iIgZSuIuIGEjhLiJiIIW7iIiBFO4iIgZSuIuIGEjhLiJiIIW7iIiBFO4iIgZSuIuIGEjhLiJiIIW7iIiBFO4iIgZSuIuIGEjhLiJiIIW7iIiBFO4iIgZSuIuIGEjhLiJiIIW7iIiBFO4iIgZSuIuIGCihL5OefPJJDh48yMqVKwG49dZb8Xq9DB06FIBrrrmGBx98kFAoxKOPPsqrr75KMBjklltuoaKiAsuy6Orq4uGHH2bfvn0Eg0Fuv/12SktLAfD5fCxatIjPPvuMYDDI/PnzcblcUWpZRMR8EYV7S0sLjzzyCK+//jpTp04FwO/309zczGuvvcbIkSNP2r+mpobm5mbq6+sJhUKUl5dz8cUX43a7WbduHYFAgB07dtDR0UFpaSmXXXYZV199NZWVlYwbN47169fT0tLCzJkzufLKK8nMzIxe5yIiBoso3Gtra5k4cSKXXHIJBw8eBOC9994jKSmJ++67D4/Hw1VXXcWCBQtITU2lsbGR4uJikpKSACgpKaGurg63201jYyMrV67EsixSU1Nxu93U1dVx1VVXsXv3bnbs2AFATk4O+fn51NfXM3v27D43all93zeSuYNdT6/x1DPEb9+g3k/8apKIwv3ee+8FYN26deFthw8f5rrrruOhhx4iOTmZRx55hPnz5/PXv/4Vj8dDRkZGeN+MjAw8Hg/AGceam5vx+Xx0dXWddV5fpKaO6PNcgPT05H7NH4zisWeI375BvZumT2vuJ7rhhhu44YYbws/vvPNO8vLyOHbsGLZtY53yluhwHP8M92xjtm0DnHVeX/h8RwkGQ73e3+GAtLRvD3Zb25d8U5bxLOv4N3o89Qzx2zeo98HWu9Pp6NUJa7/DvampiYSEBK6//nrgeGg7HA4SEhLIzs7G6/WG9/V6veF1856xnJyck8bS09MZMmQIra2tjB49Ojz24x//uF91RnLgTt3XtiObb4J47Bnit29Q76b13u9LIdvb21m1ahVHjhwBoKqqCpfLRVJSEgUFBWzZsoXu7m66urrYvHkzhYWFABQUFFBbW0soFKKjo4OGhgYKCwtxOp1MmjSJjRs3ArB//3727Nmjq2VERCLQ7zP3adOm8dlnnzF9+nRCoRCXXnopy5YtA2DGjBm0tLQwbdo0/H4/LpeL4uJiACoqKlixYgVFRUX4/X7KysrIy8sDYMmSJSxdupSpU6cSCARYuHAhY8aM6W+pIiJxw7Jt034YOZ3Pd5RAILI19/T0ZG5b9grPLZ3MoUODZz2uvywLRo1KjqueIX77BvU+2HpPSOjdmrt+Q1VExEAKdxERAyncRUQMpHAXETGQwl1ExEAKdxERAyncRUQMpHAXETGQwr0XTLwdqIiYTeH+HRKdDvyBEOnpyaSkDI91OSIivaZw/w5Op0VigoO5a/5NUpJTZ/AiMmgo3Hvhq68DsS5BRCQiCncREQMp3EVEDKRwFxExkMJdRMRACncREQMp3EVEDKRwFxExkMJdRMRACncREQMp3EVEDKRwFxExkMJdRMRAfQr3J598kgceeCD8/OWXX8btdjN58mTuvvtujhw5Eh6rrq5mypQpFBQUUFlZid/vByAUCrFmzRomT56My+XimWeewbZtALq6uli4cGF4Xk1NTX96FBGJOxGFe0tLCxUVFWzYsCG87cMPP2TNmjX87W9/45VXXiEjI4O1a9cC0NTUxEsvvcSmTZvYuXMn7e3t4bk1NTU0NzdTX19PQ0MDe/fuZfv27QCsW7eOQCDAjh07ePHFF9mwYQP79u2LUssiIuaLKNxra2uZOHEi5eXl4W27du3iF7/4BaNHjwZg1qxZ1NfXEwqFaGxsxO12k5ycjNPppKysjLq6OgAaGxspLi4mKSmJoUOHUlJSctLYzJkzsSyL1NRU3G53eKyvLCuyRzReY7A+4qlX9a3eB1vvvZUQSUDee++9wPEz6x4ej4fMzMzw84yMDI4dO0ZHRwcej4fx48efNObxeMLzMjIyej3W3NwcSaknSU0d0ee5J0pPT47K6wwG8dTrieK1b1Dvpoko3M/GOsPbSc+2U8d6ntu2fdqYw+H43rG+8PmOEgyGer2/wwFpaacf7La2L/nmYwFjWdbxb/R46PVE8do3qPfB1rvT6ejVCWu/wz0rK4uWlpbwc6/Xy4gRI0hJSSErKwuv13vSWFZWFgDZ2dmnjfX8BNAzlpOTc9pYX0Vy4M62r21H9jqDWTz1eqJ47RvUu2m99/tSyEmTJtHU1BQO6n/+85+4XC4cDgcFBQVs27aNzs5OQqEQNTU1FBYWAlBQUMCWLVvo7u6mq6uLzZs3nzRWW1tLKBSio6ODhoaG8JiIiHy/fp+5X3LJJSxYsIA77rgDv9/PxRdfzOrVqwG44YYb+OijjygrKyMQCDBhwgTmzp0LwIwZM2hpaWHatGn4/X5cLhfFxcUAVFRUsGLFCoqKivD7/ZSVlZGXl9ffUkVE4oZl26b9MHI6n+8ogUBka+7p6cnMWb2L9Qtd3LbsFZ5bOplDhwbPulxfWRaMGpUcF72eKF77BvU+2HpPSOjdmrt+Q7UXEp0O/IEQI1OGx7oUEZFeicrVMqZzOi0SE46/D1qWeR+8iIh5dOYuImIghbuIiIEU7iIiBlK4i4gYSOEuImIghbuIiIEU7iIiBlK4i4gYSOEuImIghbuIiIEU7iIiBlK4RyiSv2EoIhIrCvcI+AMh0tOTSdHdIUVkgFO4RyAxwcHcNf8mKcmpM3gRGdAU7hH66utArEsQEfleCncREQMp3EVEDKRwFxExkMK9j/SBqogMZAr3CPX8sWxdEikiA5nCPUI9fyxbl0SKyECmcO8jXRIpIgNZQrReaNGiRbz55pskJycDcNFFF/H0009TXV3Npk2bCAaD5Ofns3jxYhITEwmFQjz66KO8+uqrBINBbrnlFioqKrAsi66uLh5++GH27dtHMBjk9ttvp7S0NFqliogYL2rh/tZbb7F+/XrGjRsX3tbU1MRLL73Epk2bGD58OPfccw8bNmxg9uzZ1NTU0NzcTH19PaFQiPLyci6++GLcbjfr1q0jEAiwY8cOOjo6KC0t5bLLLuPqq6+OVrkiIkaLyrLMoUOHOHDgAE8//TRFRUXcdddd7N+/n8bGRtxuN8nJyTidTsrKyqirqwOgsbGR4uJikpKSGDp0KCUlJSeNzZw5E8uySE1Nxe12h8f6yrIie5yr1x0MD1P7Ut/q3YTeeysqZ+6tra3k5+ezaNEisrKy+Otf/8qcOXMYNWoU48ePD++XkZGBx+MBwOPxkJGR0eux5ubmPteXmjqiz3O/T3p68jl77Vgyta/vE699g3o3TVTC/YorrmD9+vXh53fccQfPPvssF154IdYpbzU9z23bPm3M4XB871hf+HxHCQZDvd7f4YC0tO8+2D2XRCYmOOj2B+k8fKzP9Q0klnX8G72t7UtsO9bVnD/x2jeo98HWu9Pp6NUJa1TC/Z133sHr9TJlypTwNtu28fv9eL3e8Dav10tWVhYA2dnZp41lZmaeNJaTk3PaWF9FcuB6s++Jl0Q+e/+kiP8bA51tm9VPb8Vr36DeTes9Kmvu3d3dLF++nNbWVgD+8Y9/MHbsWG6//Xa2bdtGZ2cnoVCImpoaCgsLASgoKGDLli10d3fT1dXF5s2bTxqrra0lFArR0dFBQ0NDeGyg0SWRIjIQReXM/dprr2XevHmUl5cTDAbJysriqaeeIjs7m48//piysjICgQATJkxg7ty5AMyYMYOWlhamTZuG3+/H5XJRXFwMQEVFBStWrKCoqAi/309ZWRl5eXnRKPWcsSzz3vlFZPCK2qWQs2bNYtasWadtLy8vp7y8/LTtTqeT+fPnM3/+/NPGhg0bxsqVK6NV2jl14u0IuruDHDZk7V1EBjf9hmo/6XYEIjIQKdyjpGftXeEuIgOBwj1KdLdIERlIFO5RouUZERlIFO5RpksjRWQgULiLiBhI4R5lPWvvI7XuLiIxFLXr3OW4nrV3OH6PmlDvb2kjIhI1OnM/R3TljIjEksL9HDnxypl+3NBSRKRPFDvnUCAQOukMXpdHisj5onA/h3rW3+/6424sh8WoUVqmEZHzQ+F+HvgDQf2Ck4icVwr380j3nxGR80Xhfh7p/jMicr4o3M+jU9fgU36gD1lF5NxQuMdAzxq8ZX37IatCXkSiSeEeQ2e6kkYhLyLRoHCPsZ6z+LOFvMJeRPpC4T5AnCnkf5A6XNfGi0ifKNwHmJ6Q/8MTr5GY4Dzp2nidxYtIb+mukANUzzXxPbcwSPnBcBITnHR3B+nsPIZtHw/7E7+KiPTQmfsA13P5ZGKC84xLNqcu3ejsXkRA4T6onLpk0/O1J/RPDHp9ICsS3wZ0uO/Zs4ebb76ZKVOmUF5eTmtra6xLGhB6lmx6vvaE/tnO7s8W9gp+EXMN2HBvb2/nvvvuY+3atezcuZNf/vKXLFq0KNZlDXinnt2fbSmnt2f5p47pDUFkcBiwH6i+/vrrXHrppVx66aUAlJaWsnbtWg4ePMgPf/jDiF7L6YzsPaznj2tc8v9+cNLXM20729do7dPX18u58AIAxmQmk5jgYN2L73DXjPHhrwBP1bzNvJKrGTUq+Zs3BSf+QBDg+Bq+RXjbSV/9QY4e7Trtw9xTP+A90we+37dPrF6vR2Kig1Bo4NV3Ll+vR8I3fx5yoNV3Ll+v59/6qcf9fNYXqd7mmWXbA/M6i6qqKj766CPWrFkT3vbzn/+cP/3pT+Tm5sawMhGRgW/ALsvYto11hjUAh/5mnYjI9xqwSZmVlYXX6w0/7+7uxufzkZWVFcOqREQGhwEb7tdffz3vvfceH3zwAQCbNm3iJz/5CWlpaTGuTERk4Buwa+4Ab7zxBo8++ihff/016enprF69muzs7FiXJSIy4A3ocBcRkb4ZsMsyIiLSdwp3EREDKdxFRAykcBcRMZDCXUTEQAp3EREDKdxFRAw0YO8KGSt79uzhj3/8I19//TWZmZmsWbOGCy+8MNZlnReLFi3izTffJDk5GYCLLrqIp59+OsZVnVtPPvkkBw8eZOXKlQC8/PLLVFVVEQgEuPzyy1mxYgUXXHBBjKs8N07t/dZbb8Xr9TJ06FAArrnmGh588MFYlhhVL7zwAhs3bsSyLIYNG8YDDzxAbm4u1dXVbNq0iWAwSH5+PosXLyYxMTHW5fafLWFtbW32z372M/v999+3bdu2n3/+efs3v/lNjKs6fwoKCuz//e9/sS7jvPj888/tefPm2bm5ufbixYtt27btDz74wM7Ly7O/+OIL27Zt+5FHHrGXLFkSyzLPiTP13t3dbY8fP94+fPhwjKs7N9566y37xhtvtNva2mzbtu1XX33Vvv766+3XXnvNnjJlit3Z2WkHAgH7d7/7nf3nP/85xtVGh5ZlTnCme8i/+eabHDx4MMaVnXuHDh3iwIEDPP300xQVFXHXXXexf//+WJd1ztTW1jJx4kTKy8vD23bt2sUvfvELRo8eDcCsWbOor68nFArFqsxz4ky9v/feeyQlJXHfffdRVFTEokWL8Pl8MawyulJSUli+fHn43lS5ubm0tbXR2NiI2+0mOTkZp9NJWVkZdXV1Ma42OhTuJ/jiiy/IzMwMP09KSiI1NRWPxxPDqs6P1tZW8vPzWbRoEVu3biU3N5c5c+YQDAZjXdo5ce+99zJr1iycTmd4m8fjOen4Z2RkcOzYMTo6OmJQ4blzpt4PHz7Mddddx5o1a6irq2P48OHMnz8/hlVG19ixY8nPzwcgFAqxatUqbrzxxjMec1P+vSvcT2DH8T3kr7jiCtavX092djaWZXHHHXewf/9+Pv3001iXdl6d6fifaZtpbrjhBp588klSU1NJSEjgzjvv5PXXX+fYsWOxLi2qjhw5wp133sn+/fvDfwjo1ONryvE2P7UiEM/3kH/nnXfYuXPnSdts2yYhIX4+cz/1+Hu9XkaMGEFKSkoMqzo/mpqa2Lt3b/i5bds4HA6jjv8nn3xCSUkJF1xwAc8//zwjR4484zE35d+7wv0E8XwP+e7ubpYvX05raysA//jHPxg7diz/93//F+PKzp9JkybR1NQU/sf+z3/+E5fLFRc/ubW3t7Nq1SqOHDkCHP8zly6Xi6SkpBhXFh0HDhxg1qxZTJ8+nbVr1zJkyBAACgoK2LZtG52dnYRCIWpqaigsLIxxtdFhzttyFKSlpfHEE09w//33h+8hv3bt2liXdV5ce+21zJs3j/LycoLBIFlZWTz11FPG/IjaG5dccgkLFizgjjvuwO/3c/HFF7N69epYl3VeTJs2jc8++4zp06cTCoW49NJLWbZsWazLiprq6mo6OzvZunUrW7duDW+vqqqiuLiYsrIyAoEAEyZMYO7cuTGsNHp0P3cREQOZ//OmiEgcUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBhI4S4iYiCFu4iIgRTuIiIGUriLiBjo/wOH7IQXV3GtaAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.probes.importance_matrix import get_importance_matrix\n",
    "\n",
    "\n",
    "f = f\"{BASE_FOLDER}/checkpoint_last/adapter_model.safetensors\"\n",
    "importance_matrix = get_importance_matrix(f, layers=layers_names)[\n",
    "    SKIP::STRIDE, ::DECIMATE\n",
    "]\n",
    "\n",
    "\n",
    "# importance_matrix = importance_matrix ** 3 # square to make it positive\n",
    "importance_matrix = (importance_matrix - 1).abs() ** 2\n",
    "\n",
    "\n",
    "# importance_matrix = importance_matrix / (0.1*importance_matrix.std())\n",
    "# importance_matrix = importance_matrix + 1\n",
    "\n",
    "# square to make it positive\n",
    "# importance_matrix = importance_matrix.clamp(0, None)\n",
    "# importance_matrix -= importance_matrix.mean() - 1\n",
    "\n",
    "s = importance_matrix.std()\n",
    "# importance_matrix = (importance_matrix > s * 3) * 1.0\n",
    "# print(f\"keeping top {importance_matrix.mean():2.2%} of features\")\n",
    "importance_matrix = importance_matrix / importance_matrix.mean()\n",
    "\n",
    "plt.hist(importance_matrix.flatten(), bins=155)\n",
    "\n",
    "importance_matrix.mean()\n",
    "\n",
    "# importance_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((importance_matrix>0)*1.0).sum()\n",
    "\n",
    "# importance_matrix.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test2 = dm.datasets['test']\n",
    "# shape1 = ds_test2[0][0].shape\n",
    "# shape2= importance_matrix.shape\n",
    "# np.testing.assert_equal(shape1, shape2, err_msg=\"shape mismatch between ds and importance matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.ds import ds2df\n",
    "from src.helpers.pandas_classification_report import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def get_acc_subset(df, query, verbose=True):\n",
    "    # assert (df[\"y\"].mean() < 0).any(), \"y should be [-1, 1]\"\n",
    "    assert (df[\"y\"].min() >= 0).all(), \"y should be [-1, 1]\"\n",
    "    assert (df[\"y\"].max() <= 1).all(), \"y should be [-1, 1]\"\n",
    "    assert (df[\"probe_pred\"].min() >= 0).all(), \"pred should be [0,1]\"\n",
    "    assert (df[\"probe_pred\"].max() <= 1).all(), \"pred should be [0,1]\"\n",
    "\n",
    "    if query:\n",
    "        df = df.query(query)\n",
    "    acc = ((df[\"probe_pred\"] > 0.5) == (df[\"y\"] > y_thresh)).mean()\n",
    "    # print(df['probe_pred'])\n",
    "    # print(df['y'])\n",
    "    if verbose:\n",
    "        print(f\"acc={acc:2.2%},\\tn={len(df)},\\t[{query}] \")\n",
    "    return acc\n",
    "\n",
    "\n",
    "def calc_metrics(dm, net, trainer=None, split=\"test\", verbose=True):\n",
    "    # predict\n",
    "    dl = dm.create_dataloader(split)\n",
    "    if trainer is None:\n",
    "        trainer = pl.Trainer(\n",
    "            logger=False, enable_progress_bar=False, enable_model_summary=False, barebones=True,\n",
    "        )\n",
    "    rt = trainer.predict(net, dataloaders=dl)\n",
    "    y_test_pred = np.concatenate(rt)\n",
    "\n",
    "    # get original df\n",
    "    splits = dm.splits[split]\n",
    "    df = ds2df(dm.ds_orig).rename(columns=lambda s: s.replace(\"_base\", \"\"))\n",
    "    df[\"y\"] = dm.ds[\"y\"]\n",
    "    df[\"label_true\"]\n",
    "    df_test = df.iloc[splits[0] : splits[1]].copy()\n",
    "    df_test[\"probe_pred\"] = y_test_pred\n",
    "    \n",
    "    if verbose:\n",
    "        target_names = ['true','lie']\n",
    "        df_cls = classification_report(df_test[\"y\"]>y_thresh, df_test[\"probe_pred\"] > 0.5, target_names=target_names)\n",
    "        display(df_cls.format(precision=2))\n",
    "        display(confusion_matrix(df_test[\"y\"]>y_thresh, df_test[\"probe_pred\"] > 0.5, target_names=target_names, normalize='all',).format(precision=2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"probe results on subsets of the data for {split}\")\n",
    "    acc = get_acc_subset(df_test, \"\", verbose=verbose)\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True\", verbose=verbose\n",
    "    )  # it was ph told to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False\", verbose=verbose\n",
    "    )  # it was told not to lie\n",
    "    get_acc_subset(\n",
    "        df_test, \"ans==label_true\", verbose=verbose\n",
    "    )  # the llm gave the true ans\n",
    "    get_acc_subset(\n",
    "        df_test, \"ans==label_instructed\", verbose=verbose\n",
    "    )  # the llm gave the desired ans\n",
    "    acc_lie_lie = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans==label_instructed\", verbose=verbose\n",
    "    )  # it was told to lie, and it did lie\n",
    "    acc_lie_truth = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans!=label_instructed\", verbose=verbose\n",
    "    )\n",
    "\n",
    "    a = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    b = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==False & ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "    c = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans==label_instructed\", verbose=False\n",
    "    )\n",
    "    d = get_acc_subset(\n",
    "        df_test, \"instructed_to_lie==True & ans!=label_instructed\", verbose=False\n",
    "    )\n",
    "\n",
    "    df_quad = pd.DataFrame(\n",
    "        [[a, b], [c, d]],\n",
    "        index=[\"tell a truth\", \"tell a lie\"],\n",
    "        columns=[\"did\", \"didn't\"],\n",
    "    )\n",
    "    df_quad.index.name = \"instructed to\"\n",
    "    df_quad.columns.name = \"llm gave\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"⭐PRIMARY METRIC⭐ acc={acc:2.2%} from probe on {split}\")\n",
    "        print(\n",
    "            f\"⭐SECONDARY METRIC⭐ acc_lie_lie={acc_lie_lie:2.2%} from probe on {split}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\n\\nprobe accuracy for quadrants (doesn't need to add up):\")\n",
    "    print(df_quad.round(2).to_markdown())\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    return dict(\n",
    "        acc=acc,\n",
    "        acc_lie_lie=acc_lie_lie,\n",
    "        acc_lie_truth=acc_lie_truth,\n",
    "        df_test=df_test,\n",
    "        df_confusion=df_quad,\n",
    "    )\n",
    "\n",
    "\n",
    "# r = testval_metrics = calc_metrics(dm, net)\n",
    "# r['df_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize latent space\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def plot_latent(latent):\n",
    "    # plot image of latent space \n",
    "    vmax = latent.abs().max()\n",
    "    for i in range(4): # first 4 batches\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        vmax = latent[i].max()\n",
    "        plt.imshow(\n",
    "            latent[i],\n",
    "            cmap=cm.coolwarm,\n",
    "            interpolation=\"none\",\n",
    "            aspect=\"auto\",\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        plt.title(f\"batch {i}\")\n",
    "        plt.ylabel(\"layer\")\n",
    "        plt.xlabel(\"neuron\")\n",
    "        if i < 2:\n",
    "            plt.xlabel(\"\")\n",
    "            plt.xticks([])\n",
    "        if i % 2 == 1:\n",
    "            plt.ylabel(\"\")\n",
    "            plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.colorbar()\n",
    "    # plt.colorbar()\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "    # histogram\n",
    "    latentf = rearrange(latent, \"b l n -> (b n) l\").T#.flatten()\n",
    "    print(latentf.shape)\n",
    "    for i in range(latent.shape[1]):\n",
    "        plt.hist(latentf[i], bins=25,\n",
    "                histtype=\"step\", log=True, label=f\"layer {i}\", density=True)\n",
    "    plt.title(\"latents by layer\")\n",
    "    plt.xlabel('latent magnitude')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# plot_latent(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def partial_load_from_pl_checkpoint(net, f, allowed_prefixes = ['ae.', 'norm.']):\n",
    "    sd = torch.load(open(f, 'rb'))['state_dict']\n",
    "    \n",
    "    sd2 = {k: v for k, v in sd.items() if any(k.startswith(p) for p in allowed_prefixes)}\n",
    "    print(net.load_state_dict(sd2, strict=False))\n",
    "    \n",
    "# partial_load_from_pl_checkpoint(net2, LOAD_CHECKPONT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv, plot_hist, rename_pl_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vae.tae import PL_TAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5\n",
      "torch.Size([32, 5, 23040]) x\n"
     ]
    }
   ],
   "source": [
    "print(len(dl_train), len(dl_val))\n",
    "b = next(iter(dl_train))\n",
    "x, y = b  # b['X'], b['y']\n",
    "print(x.shape, \"x\")\n",
    "if x.ndim == 3:\n",
    "    x = x.unsqueeze(-1)\n",
    "c_in = x.shape[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PL_TAE(\n",
      "  (tae): TokenizedAutoEncoder(\n",
      "    (tokenizer): Tokenizer(\n",
      "      (encoder): Encoder(\n",
      "        (encoder): Sequential(\n",
      "          (0): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=23040, out_features=16, bias=False)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (1): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=16, out_features=16, bias=False)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (2): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (3): Rearrange('b l f -> b (l f)')\n",
      "          (4): Linear(in_features=80, out_features=80, bias=True)\n",
      "          (5): Rearrange('b (l f) -> b l f', l=5)\n",
      "          (6): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=16, out_features=30720, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (embedding): Embedding(51200, 2560)\n",
      "      (decoder): Decoder(\n",
      "        (decoder): Sequential(\n",
      "          (0): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=30720, out_features=16, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (1): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (2): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (3): Rearrange('b l f -> b (l f)')\n",
      "          (4): Linear(in_features=80, out_features=80, bias=True)\n",
      "          (5): Rearrange('b (l f) -> b l f', l=5)\n",
      "          (6): NormedLinears(\n",
      "            (linears): ModuleList(\n",
      "              (0-4): 5 x NormedLinear(in_features=16, out_features=23040, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): Affines(\n",
      "        (affines): ModuleList(\n",
      "          (0-4): 5 x AffineInstanceNorm1d(23040, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = PL_TAE(\n",
    "    c_in=c_in,\n",
    "    steps_per_epoch=len(dl_train),\n",
    "    max_epochs=max_ae_epochs,\n",
    "    lr=lr,\n",
    "    encoder_sizes=[16,16, 16],\n",
    "    weight_decay=wd,\n",
    "    embed_dim=embed_dim,  # there will be layers * n_latent latent features\n",
    "    vocab_size=vocab_size,\n",
    "    # importance_matrix=importance_matrix,\n",
    "    tokens_per_layer=12,\n",
    "    # probe_embedd_dim=128,\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51200, 2560])\n",
      "torch.Size([51200, 2560])\n"
     ]
    }
   ],
   "source": [
    "# class ReversibleEmbedding(nn.Module):\n",
    "#     \"\"\"Embedding layer with reversible method.\"\"\"\n",
    "    \n",
    "#     def __init__(self, vocab_size, hidden_dim):\n",
    "#         super().__init__()\n",
    "#         self.weight = nn.Parameter(torch.randn(vocab_size, hidden_dim))\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.hidden_dim = hidden_dim\n",
    "    \n",
    "#     def forward(self, x, reverse=False):\n",
    "#         \"\"\"Embed tokens to shape `(batch_size, seq_length, hidden_dim)`.\n",
    "#         Project hidden states to shape `(batch_size, seq_length, vocab_size)`.\n",
    "#         \"\"\"\n",
    "#         # note we skip the one-hot encoding step, and the argmax(-1) reverse step\n",
    "#         W = self.weight\n",
    "#         # TODO: norm and unnorm\n",
    "#         if not reverse:\n",
    "#             return (x @ W)\n",
    "#         else:\n",
    "#             return x @ W.T\n",
    "        \n",
    "        \n",
    "# m = ReversibleEmbedding(vocab_size, hidden_dim)\n",
    "\n",
    "w = embedding.weight.to(dtype=torch.float) * 1.0\n",
    "w = nn.Parameter(w, requires_grad=False)\n",
    "print(net.tae.tokenizer.embedding.weight.shape)\n",
    "net.tae.tokenizer.embedding.weight = w\n",
    "print(net.tae.tokenizer.embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 23040])\n"
     ]
    }
   ],
   "source": [
    "print(c_in)\n",
    "x1 = x[..., 0]\n",
    "with torch.no_grad():\n",
    "    y = net(x1)\n",
    "# {k: v.abs().mean() for k, v in y.items()}, {k: v.shape for k, v in y.items()}\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #\n",
       "==================================================================================================================================\n",
       "PL_TAE                                                  [32, 5, 23040]            --                        --\n",
       "├─TokenizedAutoEncoder: 1-1                             --                        --                        --\n",
       "│    └─Tokenizer: 2-1                                   --                        --                        (recursive)\n",
       "│    │    └─Encoder: 3-1                                [32, 5, 23040]            [32, 5, 30720]            2,620,320\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Decoder: 3-16                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "│    │    └─Encoder: 3-17                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Decoder: 3-16                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "│    │    └─Encoder: 3-17                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Decoder: 3-16                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "│    │    └─Encoder: 3-17                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Embedding: 3-8                              [1920]                    [1920, 2560]              (131,072,000)\n",
       "│    │    └─Decoder: 3-9                                [32, 5, 30720]            [32, 5, 23040]            --\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "│    └─Tokenizer: 2-2                                   [32, 5, 23040]            [32, 5, 2560, 12]         139,960,800\n",
       "│    │    └─Affines: 3-10                               [32, 5, 23040]            [32, 5, 23040]            --\n",
       "│    │    │    └─ModuleList: 4-9                        --                        --                        --\n",
       "│    │    └─Encoder: 3-11                               [32, 5, 23040]            [32, 5, 30720]            (recursive)\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Decoder: 3-16                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "│    │    └─Encoder: 3-17                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Decoder: 3-16                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "│    │    └─Encoder: 3-17                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Decoder: 3-16                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "│    │    └─Encoder: 3-17                               --                        --                        (recursive)\n",
       "│    │    │    └─Sequential: 4-16                       --                        --                        (recursive)\n",
       "│    │    └─Embedding: 3-18                             [1920]                    [1920, 2560]              (recursive)\n",
       "│    │    └─Decoder: 3-19                               [32, 5, 30720]            [32, 5, 23040]            (recursive)\n",
       "│    │    │    └─Sequential: 4-17                       --                        --                        (recursive)\n",
       "==================================================================================================================================\n",
       "Total params: 282,541,920\n",
       "Trainable params: 20,397,920\n",
       "Non-trainable params: 262,144,000\n",
       "Total mult-adds (Units.GIGABYTES): 503.89\n",
       "==================================================================================================================================\n",
       "Input size (MB): 14.75\n",
       "Forward/backward pass size (MB): 216.60\n",
       "Params size (MB): 559.84\n",
       "Estimated Total Size (MB): 791.19\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(net, input_data=x1, depth=4, col_names=(\"input_size\", \"output_size\", \"num_params\",))  # input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     o = net.predict_step(b)\n",
    "#     l1_loss, l2_loss, loss, acts, h_reconstructed = net.ae(b[0])\n",
    "#     # l1_loss, l2_loss, loss, latent, h_rec = self.ae(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name | Type                 | Params\n",
      "----------------------------------------------\n",
      "0 | tae  | TokenizedAutoEncoder | 139 M \n",
      "----------------------------------------------\n",
      "8.9 M     Trainable params\n",
      "131 M     Non-trainable params\n",
      "139 M     Total params\n",
      "559.843   Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"1\",\n",
    "    max_epochs=max_ae_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose,\n",
    "    # enable_model_summary=verbose,\n",
    ")\n",
    "\n",
    "# LOAD_CHECKPONT='/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_377/checkpoints/epoch=83-step=840.ckpt'\n",
    "# LOAD_AE_SAFETENSORS = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_377/ae.safetensors'\n",
    "\n",
    "# LOAD_CHECKPONT='/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_529/checkpoints/epoch=83-step=840.ckpt'\n",
    "# LOAD_AE_SAFETENSORS = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_529/tae.safetensors'\n",
    "\n",
    "LOAD_CHECKPONT = None\n",
    "LOAD_AE_SAFETENSORS = None # good for when probe changed but no ae\n",
    "if LOAD_AE_SAFETENSORS is not None:\n",
    "    print(f\"loading ae from {LOAD_AE_SAFETENSORS}\")\n",
    "    # net.ae.load_state_dict(safetensors.torch.load_file(LOAD_AE_SAFETENSORS))\n",
    "    partial_load_from_pl_checkpoint(net, LOAD_CHECKPONT)\n",
    "elif LOAD_CHECKPONT:\n",
    "    print(f\"loading from {LOAD_CHECKPONT}\")\n",
    "    net = PL_TAE.load_from_checkpoint(LOAD_CHECKPONT)\n",
    "\n",
    "else:\n",
    "    trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    df_hist, df_hist_step = read_metrics_csv(\n",
    "        trainer1.logger.experiment.metrics_file_path\n",
    "    )\n",
    "    plot_hist(df_hist, [\"reconstruction_loss\", \"commitment_loss\", \"loss_rec\"], logy=True)\n",
    "    # plt.show()\n",
    "    # plot_hist(df_hist_step, ['loss_rec_step'], logy=True)\n",
    "    display(df_hist)\n",
    "    df_hist, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hist(df_hist, [\"reconstruction_loss\", \"commitment_loss\", \"loss_rec\"], logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, loss, should be <1\n",
    "rs3r = trainer1.test(\n",
    "    net, dataloaders=[dl_train, dl_val, dl, dl_ood], verbose=False\n",
    ")\n",
    "rs3 = rename_pl_test_results(rs3r, [\"train\", \"val\", \"test\", \"ood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer1.checkpoint_callback.best_model_path\n",
    "print(f\"LOAD_CHECKPONT='{best_model_path}'\")\n",
    "\n",
    "f = trainer1.log_dir + \"/tae.safetensors\"\n",
    "safetensors.torch.save_file(net.tae.state_dict(), f)\n",
    "print(f\"LOAD_AE_SAFETENSORS = '{f}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC: check loading\n",
    "# net2 = PL_TAE(\n",
    "#     c_in=c_in,\n",
    "#     steps_per_epoch=len(dl_train),\n",
    "#     max_epochs=max_epochs,\n",
    "#     lr=lr,\n",
    "#     dropout=0.2,\n",
    "#     encoder_sizes=[64,64, 64, 64],\n",
    "#     weight_decay=wd,\n",
    "#     n_latent=512,  # there will be layers * n_latent latent features\n",
    "#     importance_matrix=importance_matrix,\n",
    "#     tokens_per_layer=6,\n",
    "#     probe_embedd_dim=512,\n",
    "# )\n",
    "# net2.ae_mode(0)\n",
    "# partial_load_from_pl_checkpoint(net2, LOAD_CHECKPONT)\n",
    "\n",
    "# # note, loss, should be <1\n",
    "# rs3r = trainer1.test(\n",
    "#     net2, dataloaders=[dl_train, dl_val, dl, dl_ood], verbose=False\n",
    "# )\n",
    "# rs3 = rename_pl_test_results(rs3r, [\"train\", \"val\", \"test\", \"ood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x[..., 0]\n",
    "with torch.no_grad():\n",
    "    y = net(x1)\n",
    "\n",
    "print('QC: view latent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('z before quant')\n",
    "plot_latent(y['z'][..., 0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('z after quant')\n",
    "plot_latent(y['z_q'][..., 0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y.keys()\n",
    "# y['z_q'].shape, y['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('z_q after embed(z.argmin())')\n",
    "# latent = y[\"latent_probe\"].cpu()  # .reshape(64, 24, 12) # [Batch, Latent, Layer]\n",
    "# plot_latent(latent[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity = ((latent!=0) * 1.0).mean()\n",
    "# print(f'QC: latent usage {sparsity:2.2%}')\n",
    "# # sparsity_mult = y['l1_losses']/y['l1_raw']\n",
    "# # print(f'QC: sparsity multiplier per layer {sparsity_mult.mean(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y['tokens'].shape)\n",
    "im = y['tokens']/net.tae.tokenizer.vocab_size\n",
    "# batch, layers, tokens_per_layer\n",
    "\n",
    "plt.imshow(im[0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = next(iter(dl_train))\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import reduce\n",
    "from src.vae.conv_inception import LinBnDrop\n",
    "from src.vae.tae import PL_TAEProbeBase\n",
    "from src.helpers.layers import HeadBlock\n",
    "    \n",
    "\n",
    "\n",
    "class PL_TAEProbeZQ(PL_TAEProbeBase):    \n",
    "    \"\"\"probe which takes in the quantized latent space.\"\"\"\n",
    "    \n",
    "    def __init__(self, c_in, *args, layers=[], dropout=0, **kwargs):\n",
    "        super().__init__( *args, **kwargs)\n",
    "        tokens_per_layer = self.tae.tokenizer.tokens_per_layer\n",
    "        encoder_vocab_size = self.tae.tokenizer.vocab_size\n",
    "        vae_embed_size  = self.tae.embed_dim\n",
    "        n_layers, n_channels = c_in\n",
    "        n_feats = n_layers * tokens_per_layer * vae_embed_size\n",
    "        self.head = nn.Sequential(\n",
    "            Rearrange(\"b l h v -> b (l h v)\"),\n",
    "            HeadBlock(n_feats, layers=layers, dropout=dropout),\n",
    "            Rearrange(\"b l -> (b l)\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            o = self.tae.tokenizer.encode(x)        \n",
    "        z, z_q, tokens = o.z, o.z_quantized, o.tokens\n",
    "        logits = self.head(z_q)\n",
    "        y_probs = F.sigmoid(logits)\n",
    "        return y_probs\n",
    "    \n",
    "\n",
    "class PL_TAEProbeToken(PL_TAEProbeBase):\n",
    "    \"\"\"\n",
    "    Probe which embeds and projects tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, *args, layers=[], vocab_size=32, dropout=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        tokens_per_layer = self.tae.tokenizer.tokens_per_layer\n",
    "        encoder_vocab_size = self.tae.tokenizer.vocab_size\n",
    "        n_layers, n_channels = c_in\n",
    "        n_feats = n_layers * tokens_per_layer * vocab_size\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Embedding(encoder_vocab_size, vocab_size, \n",
    "                                        # max_norm=1.0\n",
    "                                        ),\n",
    "            Rearrange(\"b l h v -> b (l h v)\"),  \n",
    "            HeadBlock(n_feats, layers=layers, dropout=dropout),\n",
    "            Rearrange(\"b l -> (b l)\"), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            o = self.tae.tokenizer.encode(x)\n",
    "        \n",
    "        z, z_q, tokens = o.z, o.z_quantized, o.tokens\n",
    "        logits = self.head(tokens)\n",
    "        y_probs = F.sigmoid(logits)\n",
    "        return y_probs\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class PL_TAEProbeEmbed(PL_TAEProbeBase):\n",
    "    \"\"\"\n",
    "    Probe which embeds tokens only\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, vocab_size=32, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        encoder_vocab_size = self.tae.tokenizer.vocab_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Embedding(encoder_vocab_size, vocab_size, \n",
    "                                        max_norm=1.0\n",
    "                                        ),\n",
    "            Reduce(\"b l h v -> b\", \"max\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            o = self.tae.tokenizer.encode(x)\n",
    "        \n",
    "        z, z_q, tokens = o.z, o.z_quantized, o.tokens\n",
    "        \n",
    "        y_probs = self.head(tokens)\n",
    "        return y_probs\n",
    "    \n",
    "\n",
    "        \n",
    "class PL_TAEProbeZ(PL_TAEProbeBase):    \n",
    "    \"\"\"probe which takes in the latent space.\"\"\"\n",
    "    \n",
    "    def __init__(self, c_in, *args, layers=[], dropout=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        tokens_per_layer = self.tae.tokenizer.tokens_per_layer\n",
    "        vae_embed_size = self.tae.embed_dim\n",
    "        n_layers, n_channels = c_in\n",
    "        n_feats = n_layers * tokens_per_layer * vae_embed_size\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            Rearrange(\"b l h v -> b (l h v)\"),\n",
    "            HeadBlock(n_feats, layers=layers, dropout=dropout),\n",
    "            Rearrange(\"b l -> (b l)\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            o = self.tae.tokenizer.encode(x)        \n",
    "        z, z_q, tokens = o.z, o.z_quantized, o.tokens\n",
    "        logits = self.head(z)\n",
    "        y_probs = F.sigmoid(logits)\n",
    "        return y_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def get_df_results(dm, net, trainer=None, split=\"test\", verbose=True):\n",
    "    # predict\n",
    "    dl = dm.create_dataloader(split)\n",
    "    dl.shuffle = False\n",
    "    if trainer is None:\n",
    "        trainer = pl.Trainer(\n",
    "            logger=False, enable_progress_bar=False, enable_model_summary=False, barebones=True,\n",
    "        )\n",
    "    rt = trainer.predict(net, dataloaders=dl)\n",
    "    y_test_pred = np.concatenate(rt)\n",
    "\n",
    "    # get original df\n",
    "    splits = dm.splits[split]\n",
    "    print(splits, split)\n",
    "    df = ds2df(dm.ds_orig).rename(columns=lambda s: s.replace(\"_base\", \"\"))\n",
    "    df[\"y\"] = dm.ds[\"y\"]\n",
    "    # print(dm.ds[\"y\"], df[\"binary_ans\"])\n",
    "    # assert (dm.ds[\"y\"]==df[\"binary_ans\"]).all()\n",
    "    np.testing.assert_allclose(dm.ds[\"y\"], df[\"binary_ans\"])\n",
    "    df_test = df.iloc[splits[0] : splits[1]].copy()\n",
    "    df_test[\"probe_pred\"] = y_test_pred\n",
    "    return df_test\n",
    "\n",
    "def get_score(dm, probe, trainer=None, split=\"test\", verbose=True):\n",
    "    df_test = get_df_results(dm, probe, trainer, split, verbose)    \n",
    "    # print(df_test[['y', 'probe_pred']])\n",
    "    roc_auc = sklearn.metrics.roc_auc_score(df_test[\"y\"]>0.5, df_test[\"probe_pred\"])\n",
    "    acc = sklearn.metrics.accuracy_score(df_test[\"y\"]>0.5, df_test[\"probe_pred\"]>0.5)\n",
    "    return dict(roc_auc=roc_auc, acc=acc, n=len(df_test), balance=df_test[\"y\"].mean())\n",
    "\n",
    "def get_scores(dm, probe, trainer=None, verbose=True):\n",
    "    scores = {s:get_score(dm, probe, trainer, split=s, verbose=verbose) for s in ['train', 'val', 'test']}\n",
    "    scores['ood'] = get_score(dm_ood, probe, trainer, split='all', verbose=verbose)\n",
    "    return pd.DataFrame(scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.tae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for cls in [\n",
    "    PL_TAEProbeEmbed, \n",
    "    PL_TAEProbeToken, PL_TAEProbeZ, PL_TAEProbeZQ]:\n",
    "    model_name = cls.__name__\n",
    "    print('training', model_name)\n",
    "    probe = cls(c_in=c_in, layers=[128, 64, 32], tae=net.tae, steps_per_epoch=len(dl_train), max_epochs=max_epochs, dropout=0.,)\n",
    "    print(probe.head)\n",
    "    trainer2 = pl.Trainer(\n",
    "        precision=\"16-mixed\",\n",
    "        gradient_clip_val=20,\n",
    "        max_epochs=max_epochs,\n",
    "        log_every_n_steps=1,\n",
    "        enable_progress_bar=verbose,\n",
    "        # enable_model_summary=verbose,\n",
    "        # callbacks=[lr_logger],\n",
    "    )\n",
    "    trainer2.fit(model=probe, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    \n",
    "    df_hist, _ = read_metrics_csv(trainer2.logger.experiment.metrics_file_path)\n",
    "    plot_hist(df_hist, [\"loss_pred_epoch\", \"auroc\"])\n",
    "    \n",
    "    # rs3r = trainer2.test(\n",
    "    #     probe, dataloaders=[dl_train, dl_val, dl, dl_ood], verbose=False\n",
    "    # )\n",
    "    # print(model_name)\n",
    "    # rs3 = rename_pl_test_results(rs3r, [\"train\", \"val\", \"test\", \"ood\"])\n",
    "    # display(rs3)\n",
    "    \n",
    "    # just in case the torchmetrics one isn't working, or is adding batch scores in a weird way\n",
    "    df_scores = get_scores(dm, probe)\n",
    "    print(model_name)\n",
    "    print(df_scores.round(3).to_markdown())\n",
    "    \n",
    "    results[model_name] = df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.concat(results).unstack(0).T\n",
    "df_res.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.concat(results)['roc_auc'].unstack().sort_values('ood', ascending=False)\n",
    "d.index.name = 'probe'\n",
    "print('results: roc_auc')\n",
    "print(d.round(3).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
