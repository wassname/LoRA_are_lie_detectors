{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This nb is to prototype and compare probes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TQDM_DISABLE'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset, load_preproc_datasets\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n",
    "from src.eval.interventions import check_lr_intervention_predictive\n",
    "from src.probes.utils import preproc, postproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.style.use(['seaborn-v0_8', 'seaborn-v0_8-paper'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "batch_size=16\n",
    "verbose = False\n",
    "MAX_SAMPLES = 400\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load previously made datasets of hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.5G\n",
      "drwxrwxr-x 9 wassname wassname 4.0K Dec 24 22:53 ..\n",
      "-rw-rw-r-- 1 wassname wassname 1.9G Dec 24 23:23 ds_OOD_4a1b0db1fd6f7026\n",
      "-rw-rw-r-- 1 wassname wassname 1.9G Dec 24 23:52 ds_valtest_7bf5202bdaa0342b\n",
      "-rw-rw-r-- 1 wassname wassname 470M Dec 25 11:16 ds_OOD_e160117f450c53eb\n",
      "-rw-rw-r-- 1 wassname wassname 470M Dec 25 11:23 ds_valtest_0e952754b5d5b69d\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:34 ds__5f60c63f9936a355\n",
      "-rw-rw-r-- 1 wassname wassname 4.7M Dec 25 16:36 ds__1847c1588016eac0\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:37 ds__6381067ec23829dd\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:37 ds__717e4bababcb831f\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:37 ds__5fc32636946d5cc4\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:37 ds__ad02b954c542f142\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:38 ds__143c9c8f939f9cd0\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:38 ds__84a3a13cd30ac1c3\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:38 ds__c688d3860d4e612c\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:39 ds__285f16bf1ddf8496\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:39 ds__1f40d42d8ce8d056\n",
      "-rw-rw-r-- 1 wassname wassname 5.9M Dec 25 16:39 ds__3800291996d737ea\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 18:59 ds__fbbf3b9524dc05ba\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:02 ds__e7fbba5be777e667\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:06 ds__b8fd18ad5d5ba447\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:09 ds__a1f22eb25e1e6760\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:12 ds__00136c9ca2a82d74\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:15 ds__b90de14b6580f7f9\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:18 ds__792a7d3313d88d8f\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:22 ds__b52bc63561199047\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:25 ds__db2d956bc9b9d82c\n",
      "-rw-rw-r-- 1 wassname wassname  59M Dec 25 19:27 ds__28a4ae9cea92d30b\n",
      "-rw-rw-r-- 1 wassname wassname 1.7G Dec 26 06:50 ds_OOD_6d3ece46c44f6c3b\n",
      "drwxrwxr-x 2 wassname wassname 4.0K Dec 26 06:50 .\n",
      "-rw-rw-r-- 1 wassname wassname 1.7G Dec 26 07:19 ds_valtest_73b754e8fdff9f2f\n"
     ]
    }
   ],
   "source": [
    "!ls -altrh '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/'\n",
    "\n",
    "# !ls -altrh '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_valtest_8c031b4aa03ae4d2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_ood = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_OOD_6d3ece46c44f6c3b'\n",
    "f1_val = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_valtest_73b754e8fdff9f2f'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['end_logits_base', 'choice_probs_base', 'binary_ans_base', 'label_true_base', 'label_instructed_base', 'instructed_to_lie_base', 'sys_instr_name_base', 'example_i_base', 'ds_string_base', 'template_name_base', 'correct_truth_telling_base', 'correct_instruction_following_base', 'end_residual_stream_base', 'end_logits_adapt', 'choice_probs_adapt', 'binary_ans_adapt', 'label_true_adapt', 'label_instructed_adapt', 'instructed_to_lie_adapt', 'sys_instr_name_adapt', 'example_i_adapt', 'ds_string_adapt', 'template_name_adapt', 'correct_truth_telling_adapt', 'correct_instruction_following_adapt', 'end_residual_stream_adapt'],\n",
       "     num_rows: 3204\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['end_logits_base', 'choice_probs_base', 'binary_ans_base', 'label_true_base', 'label_instructed_base', 'instructed_to_lie_base', 'sys_instr_name_base', 'example_i_base', 'ds_string_base', 'template_name_base', 'correct_truth_telling_base', 'correct_instruction_following_base', 'end_residual_stream_base', 'end_logits_adapt', 'choice_probs_adapt', 'binary_ans_adapt', 'label_true_adapt', 'label_instructed_adapt', 'instructed_to_lie_adapt', 'sys_instr_name_adapt', 'example_i_adapt', 'ds_string_adapt', 'template_name_adapt', 'correct_truth_telling_adapt', 'correct_instruction_following_adapt', 'end_residual_stream_adapt'],\n",
       "     num_rows: 400\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_val = Dataset.from_file(f1_val).with_format(\"torch\")\n",
    "\n",
    "ds_oos = Dataset.from_file(f1_ood).with_format(\"torch\")\n",
    "\n",
    "ds_out1 = datasets.interleave_datasets([ds_val, ds_val])\n",
    "ds_out = ds_out1.select(range(MAX_SAMPLES))\n",
    "ds_out1, ds_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.interventions import check_lr_intervention_predictive\n",
    "from src.eval.labels import ranking_truth_telling, ds2label_model_truth\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "\n",
    "from src.probes.pl_ranking_probe import PLConvProbeLinear\n",
    "from src.helpers.lightning import read_metrics_csv\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from src.helpers.pandas_classification_report import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 72.00% based on knowledge\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_known = filter_ds_to_known(ds_out, verbose=True)\n",
    "hs_normal = ds_known['end_residual_stream_base']\n",
    "hs_intervene = ds_known['end_residual_stream_adapt']\n",
    "label_fn = ranking_truth_telling\n",
    "hs = hs_normal - hs_intervene\n",
    "y = label_fn(ds_known)\n",
    "# y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = check_lr_intervention_predictive(hs, y, verbose=True)\n",
    "r['cr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = check_lr_intervention_predictive(hs, y, verbose=True, scale=False)\n",
    "# r['cr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv mse ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_convrank_intervention_predictive(hs, y, verbose=True):\n",
    "\n",
    "\n",
    "def dist_truth_telling(ds):\n",
    "    \"\"\"label whether the adapter or the base model were more truthfull.\"\"\"\n",
    "    return ds['correct_truth_telling_adapt'] - ds['correct_truth_telling_base']\n",
    "\n",
    "y = dist_truth_telling(ds_known)\n",
    "X = torch.stack([hs_normal,hs_intervene], 1)\n",
    "X_train0, X_val0, X_train1, X_val1, y_train, y_val = train_test_split(hs_normal, hs_intervene, y, test_size=0.5, random_state=42)\n",
    "\n",
    "to_ds = lambda hs0, hs1, y: TensorDataset(hs0, hs1, y)\n",
    "dl_train = DataLoader(to_ds(X_train0, X_train1, y_train), batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(to_ds(X_val0, X_val1, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x0, x1,y1 = next(iter(dl_train))\n",
    "c_in = x1.shape[1:]\n",
    "net = PLConvProbeLinear(c_in, total_steps=max_epochs * len(dl_train), depth=3, lr=4e-3, weight_decay=1e-5, hs=32, dropout=0.1)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(net, input_data=x1) # input_size=(batch_size, 1, 28, 28))\n",
    "\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose,\n",
    "      enable_model_summary=verbose\n",
    ")\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n",
    "\n",
    "df_hist, df_hist_step  = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "\n",
    "df_hist[['val/acc', 'train/acc']].plot()\n",
    "df_hist[['val/loss', 'train/loss']].plot()\n",
    "# df_hist_step[['val/acc', 'train/acc']].plot(style=\".\")\n",
    "# df_hist_step[['val/loss', 'train/loss']].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_hist[['val/acc', 'train/acc']].plot()\n",
    "# df_hist[['val/loss', 'train/loss']].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = trainer1.predict(net, dataloaders=dl_val)\n",
    "y_pred_raw = torch.cat(r).flatten()\n",
    "# y_pred_prob = (y_pred_raw+1)/2\n",
    "y_pred_prob = (torch.tanh(y_pred_raw)+1)/2\n",
    "y_pred = y_pred_raw > 0.\n",
    "y_val2 = y_val > 0.\n",
    "\n",
    "score = roc_auc_score(y_val2, y_pred_prob)\n",
    "print(score)\n",
    "target_names = [0, 1]\n",
    "cm = confusion_matrix(y_val2, y_pred, target_names=target_names, normalize='true')\n",
    "cr = classification_report(y_val2, y_pred, target_names=target_names)\n",
    "print(cm)\n",
    "print(cr)\n",
    "# return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_kwargs = dict(lw=2, alpha=0.75, histtype=\"step\", bins=26, range=(-2,2))\n",
    "# plt.hist(y_pred_prob, label='prob', **hist_kwargs, )\n",
    "# plt.hist(y_val2,label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_pred, label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(y_pred_raw, label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_val,  label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv bool ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy, auroc, f1_score, jaccard_index, dice\n",
    "from random import random as rand\n",
    "\n",
    "class PLConvProbeBoolRank(PLConvProbeLinear):\n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x0, x1, y = batch\n",
    "\n",
    "        if rand()>0.5:\n",
    "            x0, x1 = x1, x0\n",
    "            y = 1-y\n",
    "            \n",
    "        ypred0 = self(x0)\n",
    "        ypred1 = self(x1)\n",
    "        \n",
    "        if stage=='pred':\n",
    "            return (ypred1-ypred0).float()\n",
    "        \n",
    "        ranking_y = (y>0)*2-1 # from 0,1 to -1,1\n",
    "        loss = F.margin_ranking_loss(ypred1, ypred0, ranking_y, margin=1)\n",
    "        # loss = F.smooth_l1_loss(ypred1-ypred0, y)\n",
    "        # self.log(f\"{stage}/loss\", loss)\n",
    "        \n",
    "        y_cls = ypred1>ypred0 # switch2bool(ypred1-ypred0)\n",
    "        self.log(f\"{stage}/acc\", accuracy(y_cls, y>0, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def check_convrank_intervention_predictive(hs, y, verbose=True):\n",
    "\n",
    "\n",
    "def dist_truth_telling(ds):\n",
    "    \"\"\"label whether the adapter or the base model were more truthfull.\"\"\"\n",
    "    return ds['correct_truth_telling_adapt'] > ds['correct_truth_telling_base']\n",
    "\n",
    "y = dist_truth_telling(ds_known)\n",
    "X = torch.stack([hs_normal,hs_intervene], 1)\n",
    "X_train0, X_val0, X_train1, X_val1, y_train, y_val = train_test_split(hs_normal, hs_intervene, y, test_size=0.5, random_state=42)\n",
    "\n",
    "to_ds = lambda hs0, hs1, y: TensorDataset(hs0, hs1, y)\n",
    "dl_train = DataLoader(to_ds(X_train0, X_train1, y_train), batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(to_ds(X_val0, X_val1, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x0, x1,y1 = next(iter(dl_train))\n",
    "c_in = x1.shape[1:]\n",
    "net = PLConvProbeBoolRank(c_in, total_steps=max_epochs * len(dl_train), depth=3, lr=4e-3, weight_decay=1e-5, hs=16, dropout=0.2)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(net, input_data=x1) # input_size=(batch_size, 1, 28, 28))\n",
    "\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose,\n",
    "      enable_model_summary=verbose\n",
    ")\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n",
    "\n",
    "\n",
    "\n",
    "df_hist, df_hist_step  = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "\n",
    "df_hist[['val/acc', 'train/acc']].plot()\n",
    "df_hist[['val/loss', 'train/loss']].plot()\n",
    "# df_hist_step[['val/acc', 'train/acc']].plot(style=\".\")\n",
    "# df_hist_step[['val/loss', 'train/loss']].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = trainer1.predict(net, dataloaders=dl_val)\n",
    "y_pred_raw = torch.cat(r).flatten()\n",
    "y_pred_prob = (torch.tanh(y_pred_raw)+1)/2\n",
    "y_pred = y_pred_raw > 0.\n",
    "y_val2 = y_val > 0.\n",
    "\n",
    "score = roc_auc_score(y_val2, y_pred_prob)\n",
    "print(score)\n",
    "target_names = [0, 1]\n",
    "cm = confusion_matrix(y_val2, y_pred, target_names=target_names, normalize='true')\n",
    "cr = classification_report(y_val2, y_pred, target_names=target_names)\n",
    "print(cm)\n",
    "print(cr)\n",
    "\n",
    "# return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG dist\n",
    "hist_kwargs = dict(lw=2, alpha=0.75, histtype=\"step\", bins=26, range=(-2,2))\n",
    "plt.hist(y_pred_prob, label='prob', **hist_kwargs, )\n",
    "plt.hist(y_val2,label='pred',  **hist_kwargs,)\n",
    "plt.hist(y_pred, label='truth',  **hist_kwargs,)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(y_pred_raw, label='pred',  **hist_kwargs,)\n",
    "plt.hist(y_val,  label='truth',  **hist_kwargs,)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv direct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy, auroc, f1_score, jaccard_index, dice\n",
    "from src.probes.pl_ranking_probe import LinBnDrop, InceptionBlock, PLRankingBase\n",
    "\n",
    "class PLConvProbeLinearCls(PLRankingBase):\n",
    "\n",
    "    def __init__(self, c_in, total_steps, depth=0, lr=4e-3, weight_decay=1e-9, hs=8, dropout=0, **kwargs):\n",
    "        super().__init__(total_steps=total_steps, lr=lr, weight_decay=weight_decay)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        \n",
    "        self.pre = nn.Sequential(\n",
    "            # nn.BatchNorm2d(c_in[1], affine=False),\n",
    "            nn.Conv2d(c_in[1], hs*4, (1, 2)),\n",
    "            nn.Conv2d(hs*4, hs*4, (2, 1)),\n",
    "        )\n",
    "\n",
    "        layers = [\n",
    "            nn.BatchNorm1d(hs*4, affine=False)\n",
    "            ]\n",
    "        for i in range(depth+1):\n",
    "            if (i>0) and (i<depth):\n",
    "                layers.append(InceptionBlock(hs*4, hs, conv_dropout=dropout))\n",
    "            elif i==0: # first layer\n",
    "                if depth==0: \n",
    "                    layers.append(InceptionBlock(hs*4, 1))\n",
    "                else:\n",
    "                    layers.append(InceptionBlock(hs*4, hs, conv_dropout=dropout))\n",
    "            else: # last layer\n",
    "                layers.append(nn.Conv1d(hs*4, 1, 1))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        n = c_in[0] - 1\n",
    "        self.head = nn.Sequential(\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            nn.Linear(n, 1),  \n",
    "            # nn.Tanh(), \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.ndim==4:\n",
    "            x = x.squeeze(3)\n",
    "        x = rearrange(x, 'b l h n -> b h l n')\n",
    "        x = self.pre(x)\n",
    "        x = rearrange(x, 'b h l n -> b h (l n)')\n",
    "        x = self.conv(x)\n",
    "        x = rearrange(x, 'b l h -> b (l h)')\n",
    "        return self.head(x).squeeze(1)\n",
    "    \n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x0, y = batch\n",
    "        logits = self(x0)\n",
    "        ypred = torch.sigmoid(logits)\n",
    "        \n",
    "        if stage=='pred':\n",
    "            return ypred.float()\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        \n",
    "        self.log(f\"{stage}/acc\", accuracy(ypred, y, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = ranking_truth_telling(ds_known)\n",
    "X = torch.stack([hs_normal,hs_intervene], 3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_ds = lambda hs, y: TensorDataset(hs, y)\n",
    "dl_train = DataLoader(to_ds(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(to_ds(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x1,y1 = next(iter(dl_train))\n",
    "c_in = x1.shape[1:]\n",
    "print(c_in)\n",
    "net = PLConvProbeLinearCls(c_in, total_steps=max_epochs * len(dl_train), depth=2, lr=4e-3, weight_decay=1e-5, hs=16, dropout=0.2)\n",
    "# print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(net, input_data=x1) # input_size=(batch_size, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c_in)\n",
    "# with torch.no_grad():\n",
    "#     net(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose, enable_model_summary=verbose\n",
    ")\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n",
    "\n",
    "\n",
    "\n",
    "df_hist, df_hist_step  = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "\n",
    "df_hist[['val/acc', 'train/acc']].plot()\n",
    "# df_hist[['val/loss', 'train/loss']].plot()\n",
    "# df_hist_step[['val/acc', 'train/acc']].plot(style=\".\")\n",
    "# df_hist_step[['val/loss', 'train/loss']].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = trainer1.predict(net, dataloaders=dl_val)\n",
    "y_pred_prob = torch.cat(r).flatten()\n",
    "y_pred = y_pred_prob > 0.5\n",
    "\n",
    "score = roc_auc_score(y_val, y_pred_prob)\n",
    "\n",
    "print(score)\n",
    "target_names = [0, 1]\n",
    "cm = confusion_matrix(y_val, y_pred, target_names=target_names, normalize='true')\n",
    "cr = classification_report(y_val, y_pred, target_names=target_names)\n",
    "print(cm)\n",
    "print(cr)\n",
    "\n",
    "# return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG DIST\n",
    "# hist_kwargs = dict(lw=2, alpha=0.75, histtype=\"step\", bins=26, range=(-2,2))\n",
    "# plt.hist(y_pred_prob, label='prob', **hist_kwargs, )\n",
    "# plt.hist(y_val2,label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_pred, label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(y_pred_raw, label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_val,  label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "class PL_CSS(PLRankingBase):\n",
    "    def __init__(self, epoch_steps: int, max_epochs: int, lr=4e-3, weight_decay=1e-9):\n",
    "        super().__init__()\n",
    "        self.probe = None # subclasses must add this\n",
    "        self.total_steps = epoch_steps * max_epochs\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.probe(x).squeeze(1)\n",
    "        \n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x0, x1, y = batch\n",
    "\n",
    "        if rand()>0.5:\n",
    "            x0, x1 = x1, x0\n",
    "            y = -y\n",
    "        ypred0 = self(x0)\n",
    "        ypred1 = self(x1)\n",
    "        \n",
    "        if stage=='pred':\n",
    "            return (ypred1-ypred0).float()\n",
    "        \n",
    "        loss = F.smooth_l1_loss(ypred1-ypred0, y)\n",
    "        # self.log(f\"{stage}/loss\", loss)\n",
    "        \n",
    "        y_cls = ypred1>ypred0 # switch2bool(ypred1-ypred0)\n",
    "        self.log(f\"{stage}/acc\", accuracy(y_cls, y>0, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB, MultinomialNB\n",
    "from mapie.classification import MapieClassifier\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = preproc(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced',).fit(X_train, y_train)\n",
    "mapie = MapieClassifier(estimator=clf, cv=\"prefit\",\n",
    "                        # method=\"score\",                        \n",
    "                        ).fit(X_train, y_train)\n",
    "y_val_prob, y_val_pred = mapie.predict(X_val, alpha=0.2)\n",
    "r = postproc(y_val_prob, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# botorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_known = filter_ds_to_known(ds_out, verbose=True)\n",
    "hs_normal = ds_known['end_residual_stream_base']\n",
    "hs_intervene = ds_known['end_residual_stream_adapt']\n",
    "hs = hs_normal - hs_intervene\n",
    "# hs = torch.stack([hs_normal,hs_intervene], 3)\n",
    "\n",
    "y = ranking_truth_telling(ds_known).unsqueeze(1).float()\n",
    "\n",
    "layers = X.shape[1]\n",
    "X = rearrange(hs, 'b l hs -> b (l hs)')\n",
    "X_train, X_val, y_train, y_val = preproc(X, y)\n",
    "X = rearrange(X, 'b (l hs) -> b l hs', l=layers)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import ModelListGP, SingleTaskGP\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from functools import partial\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "\n",
    "class GPClassificationModel(ApproximateGP, GPyTorchModel):\n",
    "    # https://github.com/pytorch/botorch/issues/640#issuecomment-751392547\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.train_inputs = (train_x,)\n",
    "        self.train_targets = train_y\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, train_x, variational_distribution\n",
    "        )\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "\n",
    "model = GPClassificationModel(train_x=X_train, train_y=y_train)\n",
    "\n",
    "eps = 1e-5\n",
    "mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, num_data=len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_gpytorch_model(mll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model.likelihood(model(X_val))\n",
    "\n",
    "y_val_prob = pred.probs.cpu().numpy()\n",
    "r = postproc(y_val_prob, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
