{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we try a variety of probes\n",
    "\n",
    "They all fit into a sklearn style api, except they take torch tensors of >2 dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TQDM_DISABLE'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union, Tuple\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset, load_preproc_datasets\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n",
    "# from src.eval.interventions import check_lr_intervention_predictive\n",
    "from src.probes.utils import postproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.style.use(['seaborn-v0_8', 'seaborn-v0_8-paper'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.labels import ranking_truth_telling, ds2label_model_truth\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "\n",
    "from src.probes.pl_ranking_probe import PLConvProbeLinear\n",
    "from src.helpers.lightning import read_metrics_csv\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from src.helpers.pandas_classification_report import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from functools import partial\n",
    "from src.eval.labels import ranking_truth_telling, undo_ranked_truth_telling\n",
    "from src.helpers.ds import train_test_split_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "batch_size=16\n",
    "verbose = True\n",
    "MAX_SAMPLES = 2600\n",
    "SKIP=1\n",
    "STRIDE=2\n",
    "DECIMATE=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data, previously collected, from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -altrh '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/'\n",
    "# # !ls -altrh '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_valtest_8c031b4aa03ae4d2'\n",
    "# /media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_4/\n",
    "\n",
    "BASE_FOLDER = Path(\"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_4/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_4/hidden_states/.ds/ds_valtest_228c8c7509d7d7d2'),\n",
       " PosixPath('/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_4/hidden_states/.ds/ds_OOD_e52cbd9274aede8b'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_val = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_valtest_*')))\n",
    "f1_ood = next(iter(BASE_FOLDER.glob('hidden_states/.ds/ds_OOD_*')))\n",
    "f1_val, f1_ood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 74.58% based on knowledge\n"
     ]
    }
   ],
   "source": [
    "# load hidden state from a previously loaded adapter\n",
    "# the columns with _base are from the base model, and adapt from adapter\n",
    "# FROM TRAINING TRUTH\n",
    "ds_val = Dataset.from_file(str(f1_val)).with_format(\"torch\")\n",
    "ds_oos = Dataset.from_file(str(f1_ood)).with_format(\"torch\")\n",
    "\n",
    "ds_out = datasets.interleave_datasets([ds_val, ds_oos], seed=42, \n",
    "                                    #   probabilities=[0.5, 0.5]\n",
    "                                      )\n",
    "ds_known1 = filter_ds_to_known(ds_out, verbose=True)\n",
    "ds_known1 = ds_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a69c52ab49a46e589e39384321d76a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['end_logits_base', 'choice_probs_base', 'binary_ans_base', 'label_true_base', 'label_instructed_base', 'instructed_to_lie_base', 'sys_instr_name_base', 'example_i_base', 'ds_string_base', 'template_name_base', 'correct_truth_telling_base', 'correct_instruction_following_base', 'end_residual_Wqkv_base', 'end_residual_fc1_base', 'end_logits_adapt', 'choice_probs_adapt', 'binary_ans_adapt', 'label_true_adapt', 'label_instructed_adapt', 'instructed_to_lie_adapt', 'sys_instr_name_adapt', 'example_i_adapt', 'ds_string_adapt', 'template_name_adapt', 'correct_truth_telling_adapt', 'correct_instruction_following_adapt', 'end_residual_Wqkv_adapt', 'end_residual_fc1_adapt', 'end_logits_adapt2', 'choice_probs_adapt2', 'binary_ans_adapt2', 'label_true_adapt2', 'label_instructed_adapt2', 'instructed_to_lie_adapt2', 'sys_instr_name_adapt2', 'example_i_adapt2', 'ds_string_adapt2', 'template_name_adapt2', 'correct_truth_telling_adapt2', 'correct_instruction_following_adapt2', 'end_residual_Wqkv_adapt2', 'end_residual_fc1_adapt2', 'end_logits_base2', 'choice_probs_base2', 'binary_ans_base2', 'label_true_base2', 'label_instructed_base2', 'instructed_to_lie_base2', 'sys_instr_name_base2', 'example_i_base2', 'ds_string_base2', 'template_name_base2', 'correct_truth_telling_base2', 'correct_instruction_following_base2', 'end_residual_Wqkv_base2', 'end_residual_fc1_base2'],\n",
       "    num_rows: 2004\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flip_even_rows(row: dict, idx: int) -> dict:\n",
    "    \"\"\"this will flip adapter and base keys for even rows, this balances the task.\"\"\"\n",
    "    # also record the original rows, and which ones are flipped?\n",
    "    row2 = {k.replace('_base', '_base2').replace('_adapt', '_adapt2'): v for k, v in row.items()}\n",
    "    row = {**row2, **row}\n",
    "    \n",
    "    if idx%2 == 0:\n",
    "        row = {k.replace('_base', '_tmp'): v for k, v in row.items()}\n",
    "        row = {k.replace('_adapt', '_base'): v for k, v in row.items()} \n",
    "        row = {k.replace('_tmp', '_adapt'): v for k, v in row.items()}\n",
    "    return row\n",
    "\n",
    "ds_known = ds_known1.map(flip_even_rows, with_indices=True)\n",
    "ds_known\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: \"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_4/checkpoint_last/adapter_model.safetensors\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mBASE_FOLDER\u001b[39m}\u001b[39;00m\u001b[39m/checkpoint_last/adapter_model.safetensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# +1 because the residuals are a product of diff, so the first row didn't have a diff. So we skip that corresponding first row of the importance matrix\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m importance_matrix \u001b[39m=\u001b[39m get_importance_matrix(f, layers\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mfc1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mWqkv\u001b[39;49m\u001b[39m'\u001b[39;49m])[SKIP\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m::STRIDE, ::DECIMATE]\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(importance_matrix\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mhist(importance_matrix\u001b[39m.\u001b[39mflatten(), bins\u001b[39m=\u001b[39m\u001b[39m55\u001b[39m);\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/probes/importance_matrix.py:7\u001b[0m, in \u001b[0;36mget_importance_matrix\u001b[0;34m(saved_adaptop_file, layers)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_importance_matrix\u001b[39m(saved_adaptop_file, layers\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfc1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWqkv\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m----> 7\u001b[0m     state_dict \u001b[39m=\u001b[39m safetensors\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49mload_file(saved_adaptop_file)\n\u001b[1;32m      8\u001b[0m     keys \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(state_dict\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m layers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/safetensors/torch.py:308\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mLoads a safetensors file into torch format.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    307\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 308\u001b[0m \u001b[39mwith\u001b[39;00m safe_open(filename, framework\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    309\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    310\u001b[0m         result[k] \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mget_tensor(k)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: \"/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/lightning_logs/version_4/checkpoint_last/adapter_model.safetensors\""
     ]
    }
   ],
   "source": [
    "from src.probes.importance_matrix import get_importance_matrix\n",
    "\n",
    "f = f\"{BASE_FOLDER}/checkpoint_last/adapter_model.safetensors\"\n",
    "# +1 because the residuals are a product of diff, so the first row didn't have a diff. So we skip that corresponding first row of the importance matrix\n",
    "importance_matrix = get_importance_matrix(f, layers=['fc1', 'Wqkv'])[SKIP+1::STRIDE, ::DECIMATE]\n",
    "print(importance_matrix.shape)\n",
    "plt.hist(importance_matrix.flatten(), bins=55);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split datasets\n",
    "\n",
    "we want to split by insample (from training) and out of distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ds_val['ds_string_base']).value_counts(), pd.Series(ds_oos['ds_string_base']).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate in sample and out of sample datasets\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "outsample_datasets = list(set(ds_oos['ds_string_base']))\n",
    "print(insample_datasets, outsample_datasets)\n",
    "\n",
    "trainval_datasets = insample_datasets + outsample_datasets[:1]\n",
    "test_datasets = outsample_datasets[1:]\n",
    "trainval_datasets, test_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_trainval = ds_known.filter(lambda s: s in trainval_datasets, input_columns=['ds_string_base'])\n",
    "ds_test = ds_known.filter(lambda s: s not in trainval_datasets, input_columns=['ds_string_base'])\n",
    "\n",
    "\n",
    "MAX_SAMPLES = min(len(ds_trainval), MAX_SAMPLES)\n",
    "ds_trainval = ds_trainval.select(range(MAX_SAMPLES))\n",
    "\n",
    "MAX_SAMPLES = min(len(ds_test), MAX_SAMPLES)\n",
    "ds_test = ds_test.select(range(MAX_SAMPLES))\n",
    "\n",
    "\n",
    "len(ds_trainval), len(ds_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ds_train, ds_val = train_test_split_ds(ds_trainval, test_size=0.3, stratify_columns=['ds_string_base'])\n",
    "# len(ds_train), len(ds_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels\n",
    "ds2proxy = ranking_truth_telling\n",
    "proxy2label = undo_ranked_truth_telling\n",
    "\n",
    "# unit test labels\n",
    "y_val1 = ds2proxy(ds_val)\n",
    "y1 = proxy2label(y_val1, ds_val)\n",
    "np.testing.assert_array_equal(y1, ds_val['label_true_base'], err_msg=\"undo_distance_truth_telling failed\")\n",
    "# assert (y1 == ds_val['label_true_base']).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds2xy(ds: Dataset, ds2proxy) -> Tuple[Tensor, Tensor]:\n",
    "    # x = torch.stack([ds['end_residual_stream_base'], ds['end_residual_stream_adapt']], -1)[:, SKIP::STIDE]\n",
    "    X1 = torch.stack([ds['end_residual_fc1_base'], ds['end_residual_fc1_adapt']], dim=-1)\n",
    "    X2 = torch.stack([ds['end_residual_Wqkv_base'], ds['end_residual_Wqkv_adapt']], dim=-1)\n",
    "    x = torch.concat([X1, X2], dim=2)[:, SKIP::STRIDE, ::DECIMATE] # [layers, features, base/adapt]\n",
    "    x = x * importance_matrix[None, :, :, None]**2\n",
    "    y = ds2proxy(ds)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor([[1.1, 1.3, 2.4]])\n",
    "torch.diff(xx, dim=1)\n",
    "# xx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_trainval\n",
    "# X1 = torch.stack([ds['end_residual_fc1_base'], ds['end_residual_fc1_adapt']], dim=-1)\n",
    "# X2 = torch.stack([ds['end_residual_Wqkv_base'], ds['end_residual_Wqkv_adapt']], dim=-1)\n",
    "# x = torch.concat([X1, X2], dim=2)[:, SKIP::STIDE] # [layers, features, base/adapt]\n",
    "# y = ds2proxy(ds)\n",
    "# x.shape\n",
    "\n",
    "X, y = ds2xy(ds, ds2proxy)\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.lr import TorchRobustScaler, TorchLogisticRegression, TorchDummyClassifier\n",
    "from src.eval.pl_sk import eval_ranking, analyze_dfres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TorchLogisticRegression(random_state=42, \n",
    "                                max_iter=1000,\n",
    "                                 class_weight='balanced',)\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test, proxy2label, ds2proxy)\n",
    "df_res2 = analyze_dfres(df_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: DummyClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TorchDummyClassifier(random_state=42, strategy=\"most_frequent\")\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test, proxy2label, ds2proxy)\n",
    "analyze_dfres(df_res);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL Conv MSE Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def distance_truth_telling(ds):\n",
    "    \"\"\"label whether the adapter or the base model were more truthfull.\"\"\"\n",
    "    # even thought it is a distance, we make it center around 0.5 instead of 0, to be compatible with the other metrics\n",
    "    return ds['correct_truth_telling_base'] - ds['correct_truth_telling_adapt'] + 0.5\n",
    "\n",
    "\n",
    "def undo_distance_truth_telling(base_more_truthful: Float[Tensor, ''], ds: Dataset, ):\n",
    "    #note if we know which model is more truthful, and it's a binary choice, we can take the choice that is in that direction as the truth\n",
    "    base_more_truthful = base_more_truthful - 0.5\n",
    "    base_more_positive = (ds['binary_ans_base'] > ds['binary_ans_adapt']) * 1.0\n",
    "    y = base_more_truthful * base_more_positive + (1-base_more_positive) * (-base_more_truthful)\n",
    "    return y > 0.\n",
    "    # return switch(base_more_positive, base_more_truthful)\n",
    "\n",
    "\n",
    "y = distance_truth_telling(ds_val)\n",
    "y1 = undo_distance_truth_telling(y, ds_val)\n",
    "np.testing.assert_array_equal(y1, ds_val['label_true_base'], err_msg=\"undo_distance_truth_telling failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_val['label_true_base']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2proxy = distance_truth_telling\n",
    "proxy2label = undo_distance_truth_telling\n",
    "\n",
    "# FIXME: the distance is 0.5 thresh but 0. Hmm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes.pl_ranking_probe import InceptionBlock, LinBnDrop\n",
    "\n",
    "    \n",
    "\n",
    "class HSModel2d(nn.Module):\n",
    "    def __init__(self, c_in, hs, dropout=0, depth=2):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.BatchNorm1d(c_in[1], affine=False)]\n",
    "        for i in range(depth+1):\n",
    "            if (i>0) and (i<depth):\n",
    "                layers.append(InceptionBlock(hs*4, hs, conv_dropout=dropout))\n",
    "            elif i==0: # first layer\n",
    "                if depth==0: \n",
    "                    layers.append(InceptionBlock(c_in[1], 1))\n",
    "                else:\n",
    "                    layers.append(InceptionBlock(c_in[1], hs, conv_dropout=dropout))\n",
    "            else: # last layer\n",
    "                layers.append(nn.Conv1d(hs*4, 1, 1))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        n = c_in[0]\n",
    "        self.head = nn.Sequential(\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            nn.Linear(n, 1),  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = rearrange(x, 'b l h -> b h l')\n",
    "        x = self.conv(x)\n",
    "        x = rearrange(x, 'b l h -> b (l h)')\n",
    "        return self.head(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.sklearn_lightning import PLSKWrapper, PLSKBase\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class PLSKMSErank(PLSKBase):\n",
    "    def __init__(self, c_in, dropout=0, hs=32, depth=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.model = HSModel2d(c_in, hs, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x, y = batch\n",
    "        x0 = x[..., 0]\n",
    "        x1 = x[..., 1]\n",
    "        ypred0 = self(x0)\n",
    "        ypred1 = self(x1)\n",
    "\n",
    "        y_probs = F.sigmoid(ypred0-ypred1+0.5)\n",
    "        y_cls = y_probs > 0.5\n",
    "        \n",
    "        if stage=='pred':\n",
    "            return y_probs\n",
    "        \n",
    "        loss = F.smooth_l1_loss(ypred0-ypred1, y)\n",
    "        \n",
    "        y_cls = ypred1>ypred0 # switch2bool(ypred1-ypred0)\n",
    "        self.log(f\"{stage}/acc\", accuracy(y_cls, y>0.5, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val = train_test_split_ds(ds_trainval, test_size=0.3)\n",
    "X_val, y_val_proxy = ds2xy(ds_val, ds2proxy)\n",
    "c_in = X_val.shape[1:]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_steps = int(len(ds_trainval)*0.7//batch_size)\n",
    "pl_model = PLSKMSErank(c_in=c_in, epoch_steps=epoch_steps, max_epochs=max_epochs, depth=3, hs=64, dropout=0.1)\n",
    "\n",
    "\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs)\n",
    "if verbose: print(model)\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test, proxy2label, ds2proxy)\n",
    "df_res2 = analyze_dfres(df_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL Direct classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HSModel3d(nn.Module):\n",
    "    def __init__(self, c_in, hs, dropout=0, depth=2):\n",
    "        super().__init__()\n",
    "        self.preconv = nn.Sequential(\n",
    "            nn.Conv2d(c_in[1], hs*4, (1, 2)),\n",
    "        )\n",
    "\n",
    "        layers = [nn.BatchNorm1d(hs*4, affine=False)]\n",
    "        for i in range(depth+1):\n",
    "            if (i>0) and (i<depth):\n",
    "                layers.append(InceptionBlock(hs*4, hs, conv_dropout=dropout))\n",
    "            elif i==0: # first layer\n",
    "                if depth==0: \n",
    "                    layers.append(InceptionBlock(hs*4, 1))\n",
    "                else:\n",
    "                    layers.append(InceptionBlock(hs*4, hs, conv_dropout=dropout))\n",
    "            else: # last layer\n",
    "                layers.append(nn.Conv1d(hs*4, 1, 1))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        n = c_in[0]\n",
    "        self.head = nn.Sequential(\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            nn.Linear(n, 1),  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        if x.ndim==4:\n",
    "            x = x.squeeze(3)\n",
    "        x = rearrange(x, 'b l h v -> b h l v')\n",
    "        x = self.preconv(x)\n",
    "        x = rearrange(x, 'b h l v -> b h (l v)')\n",
    "        x = self.conv(x)\n",
    "        x = rearrange(x, 'b l h -> b (l h)')\n",
    "        return self.head(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.sklearn_lightning import PLSKWrapper, PLSKBase\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class PLSKDirect(PLSKBase):\n",
    "    def __init__(self, c_in, dropout=0, hs=32, depth=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.model = HSModel3d(c_in, hs, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels\n",
    "ds2proxy = ranking_truth_telling\n",
    "proxy2label = undo_ranked_truth_telling\n",
    "\n",
    "# unit test labels\n",
    "y_val1 = ds2proxy(ds_val)\n",
    "y1 = proxy2label(y_val1, ds_val)\n",
    "np.testing.assert_array_equal(y1, ds_val['label_true_base'], err_msg=\"undo_distance_truth_telling failed\")\n",
    "# assert (y1 == ds_val['label_true_base']).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val = train_test_split_ds(ds_trainval, test_size=0.3)\n",
    "X_val, y_val_proxy = ds2xy(ds_val, ds2proxy)\n",
    "c_in = X_val.shape[1:]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_steps = int(len(ds_trainval)*0.7//batch_size)\n",
    "pl_model = PLSKDirect(c_in=c_in, epoch_steps=epoch_steps, max_epochs=max_epochs, dropout=0.25, hs=16, depth=3)\n",
    "if verbose: print(model)\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs)\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test, proxy2label, ds2proxy)\n",
    "df_res2 = analyze_dfres(df_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bool ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.sklearn_lightning import PLSKWrapper, PLSKBase\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class PLSKBoolRank(PLSKBase):\n",
    "    def __init__(self, c_in, dropout=0, hs=32, depth=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.model = HSModel2d(c_in, hs, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x, y = batch\n",
    "        x0 = x[..., 0]\n",
    "        x1 = x[..., 1]\n",
    "        ypred0 = self(x0)\n",
    "        ypred1 = self(x1)\n",
    "\n",
    "        y_probs = F.sigmoid(ypred1-ypred0+0.5)\n",
    "        y_cls = y_probs > 0.5\n",
    "        \n",
    "        if stage=='pred':\n",
    "            return y_probs\n",
    "\n",
    "        ranking_y = (y>0)*2-1 # from 0,1 to -1,1\n",
    "        loss = F.margin_ranking_loss(ypred0, ypred1, ranking_y, margin=1)\n",
    "        \n",
    "        y_cls = ypred1>ypred0 # switch2bool(ypred1-ypred0)\n",
    "        self.log(f\"{stage}/acc\", accuracy(y_cls, y>0.5, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val = train_test_split_ds(ds_trainval, test_size=0.3)\n",
    "X_val, y_val_proxy = ds2xy(ds_val, ds2proxy)\n",
    "c_in = X_val.shape[1:]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_steps = int(len(ds_trainval)*0.7//batch_size)\n",
    "pl_model = PLSKBoolRank(c_in=c_in, epoch_steps=epoch_steps, max_epochs=max_epochs, depth=3, hs=64)\n",
    "\n",
    "\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs)\n",
    "if verbose: print(model)\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test, proxy2label, ds2proxy)\n",
    "df_res2 = analyze_dfres(df_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.sklearn_lightning import PLSKWrapper, PLSKBase\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "def ccs_loss(p_neg, p_pos):\n",
    "    consistency_losses = (p_pos - (1 - p_neg)) ** 2\n",
    "    confidence_losses = torch.min(torch.stack((p_pos, p_neg), dim=-1), dim=-1).values ** 2\n",
    "    return torch.mean(consistency_losses + confidence_losses)\n",
    "\n",
    "class PLSKCCS(PLSKBase):\n",
    "    def __init__(self, c_in, dropout=0, hs=32, depth=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(np.prod(c_in), 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(1)\n",
    "\n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x, y = batch\n",
    "        x = rearrange(x, 'b l h v -> b (l h v)').float()\n",
    "        yb = y>0.5\n",
    "        ypred = self(x)\n",
    "        if stage=='pred':\n",
    "            return ypred\n",
    "\n",
    "        p_neg = ypred[~yb].mean()\n",
    "        p_pos = ypred[yb].mean()\n",
    "        loss = ccs_loss(p_neg, p_pos)\n",
    "        assert torch.isfinite(loss)\n",
    "\n",
    "        y_cls = ypred>0.5 # switch2bool(ypred1-ypred0)\n",
    "        self.log(f\"{stage}/acc\", accuracy(y_cls, y>0.5, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epoch_steps = int(len(ds_trainval)*0.7//batch_size)\n",
    "pl_model = PLSKCCS(c_in=c_in, epoch_steps=epoch_steps, max_epochs=max_epochs, depth=3, hs=64, lr=1e-4)\n",
    "\n",
    "\n",
    "model = PLSKWrapper(pl_model, verbose=verbose, max_epochs=max_epochs, batch_size=batch_size)\n",
    "if verbose: print(model)\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test, proxy2label, ds2proxy)\n",
    "df_res2 = analyze_dfres(df_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS doesn't know the direction, so sometimes we need to flip it\n",
    "df_res2['roc_auc'] = 1-df_res2['roc_auc']\n",
    "df_res2['improvement'] = df_res2['roc_auc'] - df_res2['roc_auc_adapter']\n",
    "df_res2.sort_values('improvement', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB, MultinomialNB\n",
    "from mapie.classification import MapieClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapieClassifier2(MapieClassifier):\n",
    "\n",
    "    def fit(self, X_train, y_train, sample_weight=None, X_val=None, y_val=None, **kwargs):\n",
    "        X_train = rearrange(X_train, 'b l h v -> b (l h v)')\n",
    "        return super().fit(X_train.numpy(), y_train.numpy(), sample_weight, **kwargs)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = rearrange(X, 'b l h v -> b (l h v)').numpy()\n",
    "        return torch.from_numpy(super().predict(X)).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced',)\n",
    "model = MapieClassifier2(estimator=clf, \n",
    "                        # cv=\"prefit\",\n",
    "                        method=\"score\",                        \n",
    "                        )\n",
    "\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test, proxy2label, ds2proxy)\n",
    "df_res2 = analyze_dfres(df_res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
