{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we try a variety of probes\n",
    "\n",
    "They all fit into a sklearn style api, except they take torch tensors of >2 dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TQDM_DISABLE'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "from typing import Optional, List, Dict, Union, Tuple\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from src.datasets.dm import DeceptionDataModule\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset, load_preproc_datasets\n",
    "from src.models.load import load_model\n",
    "from src.helpers.torch_helpers import clear_mem\n",
    "from src.models.phi.model_phi import PhiForCausalLMWHS\n",
    "# from src.eval.interventions import check_lr_intervention_predictive\n",
    "from src.probes.utils import postproc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.style.use(['seaborn-v0_8', 'seaborn-v0_8-paper'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.labels import ranking_truth_telling, ds2label_model_truth\n",
    "from src.eval.ds import filter_ds_to_known\n",
    "\n",
    "from src.probes.pl_ranking_probe import PLConvProbeLinear\n",
    "from src.helpers.lightning import read_metrics_csv\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from src.helpers.pandas_classification_report import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from functools import partial\n",
    "from src.eval.labels import ranking_truth_telling, undo_ranked_truth_telling\n",
    "from src.helpers.ds import train_test_split_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "batch_size=16\n",
    "verbose = False\n",
    "MAX_SAMPLES = 600\n",
    "SKIP=5\n",
    "STIDE=2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data, previously collected, from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -altrh '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/'\n",
    "# # !ls -altrh '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_valtest_8c031b4aa03ae4d2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select rows are 76.89% based on knowledge\n"
     ]
    }
   ],
   "source": [
    "# load hidden state from a previously loaded adapter\n",
    "# the columns with _base are from the base model, and adapt from adapter\n",
    "f1_ood = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_OOD_6d3ece46c44f6c3b'\n",
    "f1_val = '/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.ds/ds_valtest_73b754e8fdff9f2f'\n",
    "ds_val = Dataset.from_file(f1_val).with_format(\"torch\")\n",
    "ds_oos = Dataset.from_file(f1_ood).with_format(\"torch\")\n",
    "\n",
    "ds_out = datasets.interleave_datasets([ds_val, ds_oos], seed=42, \n",
    "                                    #   probabilities=[0.5, 0.5]\n",
    "                                      )\n",
    "ds_known1 = filter_ds_to_known(ds_out, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd367112ddb4405f8165037fc79c13e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['end_logits_base', 'choice_probs_base', 'binary_ans_base', 'label_true_base', 'label_instructed_base', 'instructed_to_lie_base', 'sys_instr_name_base', 'example_i_base', 'ds_string_base', 'template_name_base', 'correct_truth_telling_base', 'correct_instruction_following_base', 'end_residual_stream_base', 'end_logits_adapt', 'choice_probs_adapt', 'binary_ans_adapt', 'label_true_adapt', 'label_instructed_adapt', 'instructed_to_lie_adapt', 'sys_instr_name_adapt', 'example_i_adapt', 'ds_string_adapt', 'template_name_adapt', 'correct_truth_telling_adapt', 'correct_instruction_following_adapt', 'end_residual_stream_adapt', 'end_logits_adapt2', 'choice_probs_adapt2', 'binary_ans_adapt2', 'label_true_adapt2', 'label_instructed_adapt2', 'instructed_to_lie_adapt2', 'sys_instr_name_adapt2', 'example_i_adapt2', 'ds_string_adapt2', 'template_name_adapt2', 'correct_truth_telling_adapt2', 'correct_instruction_following_adapt2', 'end_residual_stream_adapt2', 'end_logits_base2', 'choice_probs_base2', 'binary_ans_base2', 'label_true_base2', 'label_instructed_base2', 'instructed_to_lie_base2', 'sys_instr_name_base2', 'example_i_base2', 'ds_string_base2', 'template_name_base2', 'correct_truth_telling_base2', 'correct_instruction_following_base2', 'end_residual_stream_base2'],\n",
       "    num_rows: 2140\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flip_even_rows(row: dict, idx: int) -> dict:\n",
    "    \"\"\"this will flip adapter and base keys for even rows, this balances the task.\"\"\"\n",
    "    # also record the original rows, and which ones are flipped?\n",
    "    row2 = {k.replace('_base', '_base2').replace('_adapt', '_adapt2'): v for k, v in row.items()}\n",
    "    row = {**row2, **row}\n",
    "    \n",
    "    if idx%2 == 0:\n",
    "        row = {k.replace('_base', '_tmp'): v for k, v in row.items()}\n",
    "        row = {k.replace('_adapt', '_base'): v for k, v in row.items()} \n",
    "        row = {k.replace('_tmp', '_adapt'): v for k, v in row.items()}\n",
    "    return row\n",
    "\n",
    "ds_known = ds_known1.map(flip_even_rows, with_indices=True)\n",
    "ds_known\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split datasets\n",
    "\n",
    "we want to split by insample (from training) and out of distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(amazon_polarity    267\n",
       " glue:qnli          267\n",
       " super_glue:rte     267\n",
       " super_glue:axg     267\n",
       " sst2               267\n",
       " hans               267\n",
       " Name: count, dtype: int64,\n",
       " super_glue:boolq    534\n",
       " super_glue:axg      534\n",
       " imdb                534\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ds_val['ds_string_base']).value_counts(), pd.Series(ds_oos['ds_string_base']).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glue:qnli', 'hans', 'super_glue:rte', 'sst2', 'amazon_polarity', 'super_glue:axg'] ['imdb', 'super_glue:axg', 'super_glue:boolq']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['glue:qnli',\n",
       "  'hans',\n",
       "  'super_glue:rte',\n",
       "  'sst2',\n",
       "  'amazon_polarity',\n",
       "  'super_glue:axg',\n",
       "  'imdb'],\n",
       " ['super_glue:axg', 'super_glue:boolq'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "outsample_datasets = list(set(ds_oos['ds_string_base']))\n",
    "print(insample_datasets, outsample_datasets)\n",
    "\n",
    "trainval_datasets = insample_datasets + outsample_datasets[:1]\n",
    "test_datasets = outsample_datasets[1:]\n",
    "trainval_datasets, test_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glue:qnli', 'hans', 'super_glue:rte', 'sst2', 'amazon_polarity', 'super_glue:axg'] ['imdb', 'super_glue:axg', 'super_glue:boolq']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['super_glue:rte',\n",
       "  'amazon_polarity',\n",
       "  'hans',\n",
       "  'glue:qnli',\n",
       "  'super_glue:axg',\n",
       "  'sst2'],\n",
       " ['super_glue:boolq', 'imdb'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: DELETEME: ONCE I HAVE RERUN BASE MODEL\n",
    "\n",
    "insample_datasets = list(set(ds_val['ds_string_base']))\n",
    "outsample_datasets = list(set(ds_oos['ds_string_base']))\n",
    "print(insample_datasets, outsample_datasets)\n",
    "\n",
    "trainval_datasets = ['super_glue:rte', 'amazon_polarity', 'hans', 'glue:qnli', 'super_glue:axg', 'sst2']\n",
    "test_datasets = ['super_glue:boolq', 'imdb']\n",
    "trainval_datasets, test_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e554af19ae5f4515b44f7f39974627e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c324bfbb0e549f584bc5d55054b7f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(600, 600)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_trainval = ds_known.filter(lambda example: example[\"ds_string_base\"] in trainval_datasets)\n",
    "ds_test = ds_known.filter(lambda example: example[\"ds_string_base\"] not in trainval_datasets)\n",
    "\n",
    "MAX_SAMPLES = min(len(ds_known), MAX_SAMPLES)\n",
    "ds_trainval = ds_trainval.select(range(MAX_SAMPLES))\n",
    "\n",
    "MAX_SAMPLES = min(len(ds_test), MAX_SAMPLES)\n",
    "ds_test = ds_test.select(range(MAX_SAMPLES))\n",
    "\n",
    "\n",
    "len(ds_trainval), len(ds_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ds_train, ds_val = train_test_split_ds(ds_trainval, test_size=0.3, stratify_columns=['ds_string_base'])\n",
    "# len(ds_train), len(ds_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels\n",
    "ds2proxy = ranking_truth_telling\n",
    "proxy2label = undo_ranked_truth_telling\n",
    "\n",
    "# unit test labels\n",
    "y_val1 = ds2proxy(ds_val)\n",
    "y1 = proxy2label(y_val1, ds_val)\n",
    "assert (y1 == ds_val['label_true_base']).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "class TorchRobustScaler(RobustScaler):\n",
    "\n",
    "    def wrap(self, X, method: str):\n",
    "        b, l, h, v = X.shape\n",
    "        X = rearrange(X, \"b l h v -> b (l h v)\")\n",
    "        X = getattr(super(), method)(X)\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = torch.from_numpy(rearrange(X, \"b (l h v) -> b l h v\", l=l, h=h, v=v))\n",
    "        return X\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self.wrap(X, \"fit\")\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.wrap(X, \"transform\")\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return self.wrap(X, \"inverse_transform\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 14, 2559, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds_trainval\n",
    "x = torch.stack([ds['end_residual_stream_base'], ds['end_residual_stream_adapt']], -1)[:, SKIP::STIDE]\n",
    "y = ds2proxy(ds)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.ds import ds2df\n",
    "\n",
    "\n",
    "\n",
    "def ds2dfres(model, scaler, ds_test, verbose=True):\n",
    "    \"\"\"dataset to dataframe with predictions\"\"\"\n",
    "    X_test, y_test_proxy = ds2xy(ds_test)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_test_proxy_prob = model.predict_proba(X_test)\n",
    "    y_test = proxy2label(y_test_proxy, ds_test)\n",
    "    y_test_prob = proxy2label(y_test_proxy_prob, ds_test)\n",
    "    df_test = ds2df(ds_test)\n",
    "    df_test['y_prob'] = y_test_prob\n",
    "    df_test['y'] = y_test\n",
    "    df_test['y_proxy_prob'] = y_test_prob\n",
    "    df_test['y_test_proxy'] = y_test_proxy\n",
    "    if verbose:\n",
    "        postproc(y_test_prob, y_test, verbose=verbose)\n",
    "    return df_test\n",
    "\n",
    "def ds2xy(ds: Dataset) -> Tuple[Tensor, Tensor]:\n",
    "    x = torch.stack([ds['end_residual_stream_base'], ds['end_residual_stream_adapt']], -1)[:, SKIP::STIDE]\n",
    "    y = ds2proxy(ds)\n",
    "\n",
    "    # TODO flip labels, and final dim of x, on some random inds\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def eval_ranking(model, ds_trainval: Dataset, ds_test: Dataset):\n",
    "    \"\"\"Evaluate a scikit learn style model, with a ranking proxy label.\"\"\"\n",
    "\n",
    "    # split\n",
    "    ds_train, ds_val = train_test_split_ds(ds_trainval, test_size=0.3)\n",
    "    X_val, y_val_proxy = ds2xy(ds_val)\n",
    "    X_train, y_train_proxy = ds2xy(ds_train)\n",
    "    X_test, y_test = ds2xy(ds_test)\n",
    "\n",
    "    # scale\n",
    "    scaler = TorchRobustScaler(with_centering=False, with_scaling=True)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    model.fit(X_train=X_train, y_train=y_train_proxy, X_val=X_val, y_val=y_val_proxy)\n",
    "\n",
    "    # must return a single torch float probability\n",
    "    y_val_proxy_prob = model.predict_proba(X_val)\n",
    "    assert y_val_proxy_prob.max()<=1\n",
    "    assert y_val_proxy_prob.min()>=0\n",
    "    assert y_val_proxy_prob.ndim == 1, \"model must return a single probability\"\n",
    "    assert y_val_proxy_prob.shape[0] == X_val.shape[0], \"model must return a single probability for each sample\"\n",
    "    assert isinstance(y_val_proxy_prob, torch.Tensor), \"model must return a torch tensor\"\n",
    "    assert torch.is_floating_point(y_val_proxy_prob), \"model must return a torch tensor\"\n",
    "\n",
    "    df_res1 = ds2dfres(model, scaler, ds_val, False)\n",
    "    df_res2 = ds2dfres(model, scaler, ds_test, False)\n",
    "    df_res = pd.concat([df_res1, df_res2])\n",
    "    return df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dfres(df_res):\n",
    "    data = {}\n",
    "    for n, g in df_res.groupby('ds_string_base'):\n",
    "        roc_auc = roc_auc_score(g['y'], g['y_prob'])\n",
    "        acc = accuracy_score(g['y'], g['y_prob']>0.5)\n",
    "        in_adapter_distribution = n in insample_datasets\n",
    "        in_train = n in trainval_datasets\n",
    "\n",
    "\n",
    "        roc_auc_adapter = roc_auc_score(g['label_true_adapt2'], g['binary_ans_adapt2'])\n",
    "        roc_auc_base = roc_auc_score(g['label_true_base2'], g['binary_ans_base2'])\n",
    "\n",
    "\n",
    "        s= pd.Series(dict(\n",
    "            roc_auc=roc_auc,\n",
    "            improvement=roc_auc-max(roc_auc_adapter, roc_auc_base),\n",
    "            acc=acc,\n",
    "            n=len(g),\n",
    "            in_dist_adapter=in_adapter_distribution,\n",
    "            in_dist_probe=in_train,\n",
    "            balance=g['y'].mean(),\n",
    "            balance_proxy=g['y_test_proxy'].mean(),\n",
    "            roc_auc_adapter=roc_auc_adapter,\n",
    "            roc_auc_base=roc_auc_base,\n",
    "\n",
    "            # baseline=g['y_proxy_prob'].mean(),\n",
    "        ))\n",
    "        # print(s)\n",
    "        data[n]=s\n",
    "\n",
    "    df = pd.DataFrame(data).T.sort_values('improvement', ascending=False)\n",
    "    # df['better'] = df['roc_auc']-df[['roc_auc_adapter', 'roc_auc_base']].values.max(1)\n",
    "    display(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from einops import rearrange\n",
    "\n",
    "class TorchLogisticRegression(LogisticRegression):\n",
    "\n",
    "    def fit(self, X_train, y_train, sample_weight = None, **kwargs):\n",
    "        X_train = rearrange(X_train, 'b l h v -> b (l h v)')\n",
    "        return super().fit(X_train.numpy(), y_train.numpy(), sample_weight)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = rearrange(X, 'b l h v -> b (l h v)').numpy()\n",
    "        return torch.from_numpy(super().predict_proba(X)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>improvement</th>\n",
       "      <th>acc</th>\n",
       "      <th>n</th>\n",
       "      <th>in_dist_adapter</th>\n",
       "      <th>in_dist_probe</th>\n",
       "      <th>balance</th>\n",
       "      <th>balance_proxy</th>\n",
       "      <th>roc_auc_adapter</th>\n",
       "      <th>roc_auc_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>glue:qnli</th>\n",
       "      <td>0.85625</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.66875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sst2</th>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.865079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super_glue:axg</th>\n",
       "      <td>0.997685</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.97619</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.976852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imdb</th>\n",
       "      <td>0.944318</td>\n",
       "      <td>-0.002652</td>\n",
       "      <td>0.904255</td>\n",
       "      <td>282</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.492908</td>\n",
       "      <td>0.94697</td>\n",
       "      <td>0.945455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super_glue:rte</th>\n",
       "      <td>0.805128</td>\n",
       "      <td>-0.005128</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.810256</td>\n",
       "      <td>0.789744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hans</th>\n",
       "      <td>0.684524</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.577381</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon_polarity</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>-0.046667</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>35</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super_glue:boolq</th>\n",
       "      <td>0.862177</td>\n",
       "      <td>-0.099196</td>\n",
       "      <td>0.795597</td>\n",
       "      <td>318</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.459119</td>\n",
       "      <td>0.449686</td>\n",
       "      <td>0.961373</td>\n",
       "      <td>0.935808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   roc_auc improvement       acc    n in_dist_adapter  \\\n",
       "glue:qnli          0.85625      0.1875  0.730769   26            True   \n",
       "sst2              0.968254    0.071429  0.869565   23            True   \n",
       "super_glue:axg    0.997685    0.020833   0.97619   42            True   \n",
       "imdb              0.944318   -0.002652  0.904255  282           False   \n",
       "super_glue:rte    0.805128   -0.005128  0.785714   28            True   \n",
       "hans              0.684524   -0.005952  0.615385   26            True   \n",
       "amazon_polarity   0.933333   -0.046667  0.914286   35            True   \n",
       "super_glue:boolq  0.862177   -0.099196  0.795597  318           False   \n",
       "\n",
       "                 in_dist_probe   balance balance_proxy roc_auc_adapter  \\\n",
       "glue:qnli                 True  0.384615      0.538462          0.6375   \n",
       "sst2                      True  0.608696      0.521739        0.896825   \n",
       "super_glue:axg            True  0.428571       0.52381        0.898148   \n",
       "imdb                     False  0.531915      0.492908         0.94697   \n",
       "super_glue:rte            True  0.464286      0.535714        0.810256   \n",
       "hans                      True  0.538462      0.576923        0.577381   \n",
       "amazon_polarity           True  0.428571           0.4            0.98   \n",
       "super_glue:boolq         False  0.459119      0.449686        0.961373   \n",
       "\n",
       "                 roc_auc_base  \n",
       "glue:qnli             0.66875  \n",
       "sst2                 0.865079  \n",
       "super_glue:axg       0.976852  \n",
       "imdb                 0.945455  \n",
       "super_glue:rte       0.789744  \n",
       "hans                 0.690476  \n",
       "amazon_polarity          0.92  \n",
       "super_glue:boolq     0.935808  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TorchLogisticRegression(random_state=42, \n",
    "                                # max_iter=1000,\n",
    "                                 class_weight='balanced',)\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test)\n",
    "df_res2 = analyze_dfres(df_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: DummyClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from einops import rearrange\n",
    "\n",
    "class TorchDummyClassifier(DummyClassifier):\n",
    "\n",
    "    def fit(self, X_train, y_train, sample_weight = None, **kwargs):\n",
    "        X_train = rearrange(X_train, 'b l h v -> b (l h v)')\n",
    "        return super().fit(X_train.numpy(), y_train.numpy(), sample_weight)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = rearrange(X, 'b l h v -> b (l h v)').numpy()\n",
    "        return torch.from_numpy(super().predict_proba(X)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>acc</th>\n",
       "      <th>n</th>\n",
       "      <th>in_dist_adapter</th>\n",
       "      <th>in_dist_probe</th>\n",
       "      <th>balance</th>\n",
       "      <th>balance_proxy</th>\n",
       "      <th>roc_auc_adapter</th>\n",
       "      <th>roc_auc_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>glue:qnli</th>\n",
       "      <td>0.60625</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.66875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hans</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.577381</td>\n",
       "      <td>0.690476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super_glue:rte</th>\n",
       "      <td>0.535897</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.810256</td>\n",
       "      <td>0.789744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super_glue:axg</th>\n",
       "      <td>0.534722</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.976852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sst2</th>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.865079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imdb</th>\n",
       "      <td>0.49197</td>\n",
       "      <td>0.492908</td>\n",
       "      <td>282</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.492908</td>\n",
       "      <td>0.94697</td>\n",
       "      <td>0.945455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>super_glue:boolq</th>\n",
       "      <td>0.446241</td>\n",
       "      <td>0.449686</td>\n",
       "      <td>318</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.459119</td>\n",
       "      <td>0.449686</td>\n",
       "      <td>0.961373</td>\n",
       "      <td>0.935808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon_polarity</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>35</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   roc_auc       acc    n in_dist_adapter in_dist_probe  \\\n",
       "glue:qnli          0.60625  0.538462   26            True          True   \n",
       "hans              0.571429  0.576923   26            True          True   \n",
       "super_glue:rte    0.535897  0.535714   28            True          True   \n",
       "super_glue:axg    0.534722   0.52381   42            True          True   \n",
       "sst2              0.527778  0.521739   23            True          True   \n",
       "imdb               0.49197  0.492908  282           False         False   \n",
       "super_glue:boolq  0.446241  0.449686  318           False         False   \n",
       "amazon_polarity        0.4       0.4   35            True          True   \n",
       "\n",
       "                   balance balance_proxy roc_auc_adapter roc_auc_base  \n",
       "glue:qnli         0.384615      0.538462          0.6375      0.66875  \n",
       "hans              0.538462      0.576923        0.577381     0.690476  \n",
       "super_glue:rte    0.464286      0.535714        0.810256     0.789744  \n",
       "super_glue:axg    0.428571       0.52381        0.898148     0.976852  \n",
       "sst2              0.608696      0.521739        0.896825     0.865079  \n",
       "imdb              0.531915      0.492908         0.94697     0.945455  \n",
       "super_glue:boolq  0.459119      0.449686        0.961373     0.935808  \n",
       "amazon_polarity   0.428571           0.4            0.98         0.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TorchDummyClassifier(random_state=42, strategy=\"most_frequent\")\n",
    "df_res = eval_ranking(model, ds_trainval, ds_test)\n",
    "analyze_dfres(df_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TorchDummyClassifier(random_state=42, strategy=\"most_frequent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split\n",
    "ds_train, ds_val = train_test_split_ds(ds_trainval, test_size=0.3)\n",
    "X_val, y_val_proxy = ds2xy(ds_val)\n",
    "X_train, y_train_proxy = ds2xy(ds_train)\n",
    "X_test, y_test = ds2xy(ds_test)\n",
    "\n",
    "# scale\n",
    "scaler = TorchRobustScaler(with_centering=False, with_scaling=True)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model.fit(X_train=X_train, y_train=y_train_proxy, X_val=X_val, y_val=y_val_proxy)\n",
    "\n",
    "# must return a single torch float probability\n",
    "y_val_proxy_prob = model.predict_proba(X_val)\n",
    "assert y_val_proxy_prob.max()<=1\n",
    "assert y_val_proxy_prob.min()>=0\n",
    "assert y_val_proxy_prob.ndim == 1, \"model must return a single probability\"\n",
    "assert y_val_proxy_prob.shape[0] == X_val.shape[0], \"model must return a single probability for each sample\"\n",
    "assert isinstance(y_val_proxy_prob, torch.Tensor), \"model must return a torch tensor\"\n",
    "assert torch.is_floating_point(y_val_proxy_prob), \"model must return a torch tensor\"\n",
    "\n",
    "\n",
    "X_test, y_test_proxy = ds2xy(ds_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_test_proxy_prob = model.predict_proba(X_test)\n",
    "y_test = proxy2label(y_test_proxy, ds_test)\n",
    "y_test_prob = proxy2label(y_test_proxy_prob, ds_test)\n",
    "\n",
    "\n",
    "# df_res1 = ds2dfres(model, scaler, ds_val, False)\n",
    "# df_res2 = ds2dfres(model, scaler, ds_test, False)\n",
    "# df_res = pd.concat([df_res1, df_res2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test_proxy*1.0).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_res['y_prob']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETEME BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # psudocode\n",
    "\n",
    "# ds_train, ds_val = train_test_split_ds(ds_known, test_size=0.3)\n",
    "# X_val, y_val = ds2xy(ds_val)\n",
    "# X_train, y_train = ds2xy(ds_train)\n",
    "# # scale\n",
    "# # fit\n",
    "# # pred\n",
    "# # proxy2label\n",
    "# # score\n",
    "\n",
    "# the only part that varies is the fit and pred, so maybe I just need to pass in cls!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'preproc' from 'src.probes.utils' (/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/probes/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb Cell 40\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Here we look at the score for seeing which version is more truthfull, this is not that real label\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterventions\u001b[39;00m \u001b[39mimport\u001b[39;00m check_lr_intervention_predictive2\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/10_compare_probes.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m r \u001b[39m=\u001b[39m check_lr_intervention_predictive2(X_train, y_train, X_val, y_val, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, proxy2label\u001b[39m=\u001b[39mproxy2label)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/eval/interventions.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhelpers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas_classification_report\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_classification_report, preproc, postproc, make_dfres_pretty\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprobes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msk_lr\u001b[39;00m \u001b[39mimport\u001b[39;00m check_lr_intervention_predictive\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_intervention_quality2\u001b[39m(ds_out, label_fn, thresh\u001b[39m=\u001b[39m\u001b[39m0.03\u001b[39m, take_diff\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIntervention predictive power\u001b[39m\u001b[39m\"\u001b[39m, skip\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, model_kwargs\u001b[39m=\u001b[39m{}):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'preproc' from 'src.probes.utils' (/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/probes/utils.py)"
     ]
    }
   ],
   "source": [
    "# Here we look at the score for seeing which version is more truthfull, this is not that real label\n",
    "from src.eval.interventions import check_lr_intervention_predictive2\n",
    "\n",
    "r = check_lr_intervention_predictive2(X_train, y_train, X_val, y_val, verbose=False, proxy2label=proxy2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "r2 = copy.deepcopy(r) # not sure where this is being modified in place, but this should stop it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test\n",
    "y = label_fn(ds_known)\n",
    "y2 = undo_ranked_truth_telling(y, ds_known)\n",
    "y3 = ds_known['label_true_base']\n",
    "np.testing.assert_array_equal(y2, y3, err_msg=\"we should be able to undo the label transform\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO clean this up somehow\n",
    "# TODO, I need to split the ds or fs. That way I can undo the label easily\n",
    "y_val_pred = torch.tensor(r2['y_val_pred']).clone()\n",
    "y_val = torch.tensor(r2['y_val']).clone()\n",
    "ds_known_val = ds_known.select(range(len(ds_known)-len(y_val_pred), len(ds_known)))\n",
    "y_val_pred_true = undo_ranked_truth_telling(y_val_pred, ds=ds_known_val)\n",
    "y_val2 = undo_ranked_truth_telling(y_val, ds=ds_known_val)\n",
    "\n",
    "# just to make sure they match up\n",
    "y_val3 = ds_known_val['label_true_base']\n",
    "np.testing.assert_array_equal(y_val2, y_val3, err_msg=\"make sure we used the right dataset\")\n",
    "\n",
    "r = postproc(y_val_pred_true, y_val2, verbose=True)\n",
    "# r = postproc(y_val_pred_true, y_val3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv mse ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.torch_helpers import clear_mem, detachcpu, recursive_copy, switch\n",
    "\n",
    "def distance_truth_telling(ds):\n",
    "    \"\"\"label whether the adapter or the base model were more truthfull.\"\"\"\n",
    "    return ds['correct_truth_telling_base'] - ds['correct_truth_telling_adapt']\n",
    "\n",
    "\n",
    "def undo_distance_truth_telling(base_more_truthful: Float[Tensor, ''], ds: Dataset, ):\n",
    "    #note if we know which model is more truthful, and it's a binary choice, we can take the choice that is in that direction as the truth\n",
    "    base_more_positive = (ds['binary_ans_base'] > ds['binary_ans_adapt']) * 1.0\n",
    "    return base_more_truthful * base_more_positive + (1-base_more_positive) * (-base_more_truthful)\n",
    "    # return switch(base_more_positive, base_more_truthful)\n",
    "\n",
    "\n",
    "y = distance_truth_telling(ds_known)\n",
    "y2 = undo_distance_truth_telling(y, ds_known)\n",
    "np.testing.assert_array_equal(y2>0, ds_known['label_true_base'], err_msg=\"we should be able to undo the label transform\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y, bins=20)\n",
    "# plt.hist(y2, bins=20)\n",
    "# plt.hist(ds_known['label_true_base']*1.0, bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_convrank_intervention_predictive(hs, y, verbose=True):\n",
    "\n",
    "y = distance_truth_telling(ds_known)\n",
    "X = torch.stack([hs_normal,hs_intervene], 1)\n",
    "X_train0, X_val0, X_train1, X_val1, y_train, y_val = train_test_split(hs_normal, hs_intervene, y, test_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "to_ds = lambda hs0, hs1, y: TensorDataset(hs0, hs1, y)\n",
    "dl_train = DataLoader(to_ds(X_train0, X_train1, y_train), batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(to_ds(X_val0, X_val1, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x0, x1,y1 = next(iter(dl_train))\n",
    "c_in = x1.shape[1:]\n",
    "net = PLConvProbeLinear(c_in, epoch_steps=len(dl_train),  max_epochs=max_epochs, depth=3, lr=4e-3, weight_decay=1e-5, hs=32, dropout=0.1)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(net, input_data=x1) # input_size=(batch_size, 1, 28, 28))\n",
    "\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose,\n",
    "      enable_model_summary=verbose\n",
    ")\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n",
    "\n",
    "df_hist, df_hist_step  = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "\n",
    "df_hist[['val/acc', 'train/acc']].plot()\n",
    "df_hist[['val/loss_epoch', 'train/loss_epoch']].plot()\n",
    "# df_hist_step[['val/acc', 'train/acc']].plot(style=\".\")\n",
    "# df_hist_step[['val/loss', 'train/loss']].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = trainer1.predict(net, dataloaders=dl_val)\n",
    "y_pred_raw = torch.cat(r).flatten()\n",
    "# y_pred_prob = (y_pred_raw+1)/2\n",
    "y_pred_prob = (torch.tanh(y_pred_raw)+1)/2\n",
    "# y_val2 = y_val > 0.\n",
    "\n",
    "\n",
    "# undo label transform...\n",
    "ds_known_val = ds_known.select(range(len(ds_known)-len(y_pred_raw), len(ds_known)))\n",
    "y_val_pred_true = undo_distance_truth_telling(y_pred_prob, ds=ds_known_val)\n",
    "\n",
    "y_val3 = undo_distance_truth_telling(y_val, ds=ds_known_val)\n",
    "y_val4 = ds_known_val['label_true_base']\n",
    "np.testing.assert_array_equal(y_val3>0, y_val4, err_msg=\"make sure we used the right dataset\")\n",
    "\n",
    "y_pred = y_val_pred_true > 0.\n",
    "y_val5 = y_val3 > 0.\n",
    "\n",
    "score = roc_auc_score(y_val5, y_val_pred_true)\n",
    "print(score)\n",
    "target_names = [0, 1]\n",
    "cm = confusion_matrix(y_val5, y_pred, target_names=target_names, normalize='true')\n",
    "cr = classification_report(y_val5, y_pred, target_names=target_names)\n",
    "print(cm)\n",
    "print(cr)\n",
    "# return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_kwargs = dict(lw=2, alpha=0.75, histtype=\"step\", bins=26, range=(-2,2))\n",
    "# plt.hist(y_pred_prob, label='prob', **hist_kwargs, )\n",
    "# plt.hist(y_val2,label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_pred, label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(y_pred_raw, label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_val,  label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv bool ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy, auroc, f1_score, jaccard_index, dice\n",
    "from random import random as rand\n",
    "\n",
    "class PLConvProbeBoolRank(PLConvProbeLinear):\n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x0, x1, y = batch\n",
    "\n",
    "        # if rand()>0.5:\n",
    "        #     x0, x1 = x1, x0\n",
    "        #     y = 1-y\n",
    "            \n",
    "        ypred0 = self(x0)\n",
    "        ypred1 = self(x1)\n",
    "        \n",
    "        if stage=='pred':\n",
    "            return (ypred1-ypred0).float()\n",
    "        \n",
    "        ranking_y = (y>0)*2-1 # from 0,1 to -1,1\n",
    "        loss = F.margin_ranking_loss(ypred1, ypred0, ranking_y, margin=1)\n",
    "        # loss = F.smooth_l1_loss(ypred1-ypred0, y)\n",
    "        # self.log(f\"{stage}/loss\", loss)\n",
    "        \n",
    "        y_cls = ypred1>ypred0 # switch2bool(ypred1-ypred0)\n",
    "        self.log(f\"{stage}/acc\", accuracy(y_cls, y>0, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def check_convrank_intervention_predictive(hs, y, verbose=True):\n",
    "\n",
    "\n",
    "def dist_truth_telling(ds):\n",
    "    \"\"\"label whether the adapter or the base model were more truthfull.\"\"\"\n",
    "    return ds['correct_truth_telling_adapt'] > ds['correct_truth_telling_base']\n",
    "\n",
    "y = dist_truth_telling(ds_known)\n",
    "X = torch.stack([hs_normal,hs_intervene], 1)\n",
    "X_train0, X_val0, X_train1, X_val1, y_train, y_val = train_test_split(hs_normal, hs_intervene, y, test_size=0.5, random_state=42)\n",
    "\n",
    "to_ds = lambda hs0, hs1, y: TensorDataset(hs0, hs1, y)\n",
    "dl_train = DataLoader(to_ds(X_train0, X_train1, y_train), batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(to_ds(X_val0, X_val1, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x0, x1,y1 = next(iter(dl_train))\n",
    "c_in = x1.shape[1:]\n",
    "net = PLConvProbeBoolRank(c_in, epoch_steps=len(dl_train),  max_epochs=max_epochs, depth=3, lr=4e-3, weight_decay=1e-5, hs=16, dropout=0.2)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(net, input_data=x1) # input_size=(batch_size, 1, 28, 28))\n",
    "\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose,\n",
    "      enable_model_summary=verbose\n",
    ")\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n",
    "\n",
    "\n",
    "\n",
    "df_hist, df_hist_step  = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "\n",
    "df_hist[['val/acc', 'train/acc']].plot()\n",
    "df_hist[['val/loss', 'train/loss']].plot()\n",
    "# df_hist_step[['val/acc', 'train/acc']].plot(style=\".\")\n",
    "# df_hist_step[['val/loss', 'train/loss']].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = trainer1.predict(net, dataloaders=dl_val)\n",
    "y_pred_raw = torch.cat(r).flatten()\n",
    "y_pred_prob = (torch.tanh(y_pred_raw)+1)/2\n",
    "# y_pred = y_pred_raw > 0.\n",
    "y_val2 = y_val > 0.\n",
    "\n",
    "\n",
    "\n",
    "# undo label transform...\n",
    "ds_known_val = ds_known.select(range(len(ds_known)-len(y_pred_raw), len(ds_known)))\n",
    "y_val_pred_true = undo_ranked_truth_telling(y_pred_prob, ds=ds_known_val)\n",
    "y_val3 = undo_ranked_truth_telling(y_val2, ds=ds_known_val)\n",
    "\n",
    "y_pred = y_val_pred_true > 0.5\n",
    "\n",
    "score = roc_auc_score(y_val3, y_val_pred_true)\n",
    "print(score)\n",
    "target_names = [0, 1]\n",
    "cm = confusion_matrix(y_val3, y_pred, target_names=target_names, normalize='true')\n",
    "cr = classification_report(y_val3, y_pred, target_names=target_names)\n",
    "print(cm)\n",
    "print(cr)\n",
    "\n",
    "# return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG dist\n",
    "hist_kwargs = dict(lw=2, alpha=0.75, histtype=\"step\", bins=26, range=(-2,2))\n",
    "plt.hist(y_pred_prob, label='prob', **hist_kwargs, )\n",
    "plt.hist(y_val2,label='pred',  **hist_kwargs,)\n",
    "plt.hist(y_pred, label='truth',  **hist_kwargs,)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(y_pred_raw, label='pred',  **hist_kwargs,)\n",
    "plt.hist(y_val,  label='truth',  **hist_kwargs,)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv direct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy, auroc, f1_score, jaccard_index, dice\n",
    "from src.probes.pl_ranking_probe import LinBnDrop, InceptionBlock, PLRankingBase\n",
    "\n",
    "class PLConvProbeLinearCls(PLRankingBase):\n",
    "\n",
    "    def __init__(self, c_in, epoch_steps, max_epochs, depth=0, lr=4e-3, weight_decay=1e-9, hs=8, dropout=0, **kwargs):\n",
    "        super().__init__(epoch_steps=epoch_steps, max_epochs=max_epochs, lr=lr, weight_decay=weight_decay)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.pre = nn.Sequential(\n",
    "            # nn.BatchNorm2d(c_in[1], affine=False),\n",
    "            nn.Conv2d(c_in[1], hs*4, (1, 2)),\n",
    "            nn.Conv2d(hs*4, hs*4, (2, 1)),\n",
    "        )\n",
    "\n",
    "        layers = [\n",
    "            nn.BatchNorm1d(hs*4, affine=False)\n",
    "            ]\n",
    "        for i in range(depth+1):\n",
    "            if (i>0) and (i<depth):\n",
    "                layers.append(InceptionBlock(hs*4, hs, conv_dropout=dropout))\n",
    "            elif i==0: # first layer\n",
    "                if depth==0: \n",
    "                    layers.append(InceptionBlock(hs*4, 1))\n",
    "                else:\n",
    "                    layers.append(InceptionBlock(hs*4, hs, conv_dropout=dropout))\n",
    "            else: # last layer\n",
    "                layers.append(nn.Conv1d(hs*4, 1, 1))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "        n = c_in[0] - 1\n",
    "        self.head = nn.Sequential(\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            LinBnDrop(n, n, p=dropout),\n",
    "            nn.Linear(n, 1),  \n",
    "            # nn.Tanh(), \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.ndim==4:\n",
    "            x = x.squeeze(3)\n",
    "        x = rearrange(x, 'b l h n -> b h l n')\n",
    "        x = self.pre(x)\n",
    "        x = rearrange(x, 'b h l n -> b h (l n)')\n",
    "        x = self.conv(x)\n",
    "        x = rearrange(x, 'b l h -> b (l h)')\n",
    "        return self.head(x).squeeze(1)\n",
    "    \n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        x0, y = batch\n",
    "        logits = self(x0)\n",
    "        ypred = torch.sigmoid(logits)\n",
    "        \n",
    "        if stage=='pred':\n",
    "            return ypred.float()\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y.float())\n",
    "        \n",
    "        self.log(f\"{stage}/acc\", accuracy(ypred, y, \"binary\"), on_epoch=True, )\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True,  prog_bar=True)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = ds2label_model_truth(ds_known)\n",
    "y = ranking_truth_telling(ds_known)\n",
    "X = torch.stack([hs_normal,hs_intervene], 3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_ds = lambda hs, y: TensorDataset(hs, y)\n",
    "dl_train = DataLoader(to_ds(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(to_ds(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "x1,y1 = next(iter(dl_train))\n",
    "c_in = x1.shape[1:]\n",
    "print(c_in)\n",
    "net = PLConvProbeLinearCls(c_in, epoch_steps=len(dl_train),  max_epochs=max_epochs, depth=2, lr=4e-3, weight_decay=1e-5, hs=16, dropout=0.2)\n",
    "# print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(net, input_data=x1) # input_size=(batch_size, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c_in)\n",
    "# with torch.no_grad():\n",
    "#     net(x1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "trainer1 = pl.Trainer(\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose, enable_model_summary=verbose\n",
    ")\n",
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n",
    "\n",
    "\n",
    "\n",
    "df_hist, df_hist_step  = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "\n",
    "df_hist[['val/acc', 'train/acc']].plot()\n",
    "# df_hist[['val/loss', 'train/loss']].plot()\n",
    "# df_hist_step[['val/acc', 'train/acc']].plot(style=\".\")\n",
    "# df_hist_step[['val/loss', 'train/loss']].plot(style=\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = trainer1.predict(net, dataloaders=dl_val)\n",
    "y_pred_prob = torch.cat(r).flatten()\n",
    "y_pred = y_pred_prob > 0.5\n",
    "\n",
    "score = roc_auc_score(y_val, y_pred_prob)\n",
    "\n",
    "print(score)\n",
    "target_names = [0, 1]\n",
    "cm = confusion_matrix(y_val, y_pred, target_names=target_names, normalize='true')\n",
    "cr = classification_report(y_val, y_pred, target_names=target_names)\n",
    "print(cm)\n",
    "print(cr)\n",
    "\n",
    "# return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG DIST\n",
    "# hist_kwargs = dict(lw=2, alpha=0.75, histtype=\"step\", bins=26, range=(-2,2))\n",
    "# plt.hist(y_pred_prob, label='prob', **hist_kwargs, )\n",
    "# plt.hist(y_val2,label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_pred, label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(y_pred_raw, label='pred',  **hist_kwargs,)\n",
    "# plt.hist(y_val,  label='truth',  **hist_kwargs,)\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def ccs_loss(p_neg, p_pos):\n",
    "    consistency_losses = (p_pos - (1 - p_neg)) ** 2\n",
    "    confidence_losses = t.min(t.stack((p_pos, p_neg), dim=-1), dim=-1).values ** 2\n",
    "    return t.mean(consistency_losses + confidence_losses)\n",
    "\n",
    "class PL_CSS(PLRankingBase):\n",
    "    def __init__(self, d_in, epoch_steps: int, max_epochs: int, lr=4e-3, weight_decay=1e-9):\n",
    "        super().__init__(epoch_steps=epoch_steps, max_epochs=max_epochs, lr=lr, weight_decay=weight_decay)\n",
    "        self.model = t.nn.Sequential(\n",
    "            t.nn.Linear(d_in, 1, bias=False),\n",
    "            t.nn.Sigmoid()\n",
    "        )\n",
    "        self.total_steps = epoch_steps * max_epochs\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.squeeze(2))\n",
    "        \n",
    "    def _step(self, batch, batch_idx, stage='train'):\n",
    "        h, y = batch\n",
    "        yb = y>0.5\n",
    "        ypred = self(h)\n",
    "        if stage=='pred':\n",
    "            return ypred\n",
    "        p_neg = ypred[~yb].mean()\n",
    "        p_pos = ypred[yb].mean()\n",
    "        loss = ccs_loss(p_neg, p_pos)\n",
    "        assert torch.isfinite(loss)\n",
    "        \n",
    "        y_cls = ypred>0.5 # switch2bool(ypred1-ypred0)\n",
    "        self.log(f\"{stage}/acc\", accuracy(y_cls, y>0, \"binary\"), on_epoch=True, on_step=False)\n",
    "        self.log(f\"{stage}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(f\"{stage}/n\", len(y), on_epoch=True, on_step=False, reduce_fx=torch.sum)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_normal = ds_known['end_residual_stream_base']\n",
    "hs_intervene = ds_known['end_residual_stream_adapt']\n",
    "label_fn = ranking_truth_telling\n",
    "hs = hs_normal - hs_intervene\n",
    "y = label_fn(ds_known)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ds = lambda hs, y: TensorDataset(hs, y)\n",
    "dl_train = DataLoader(to_ds(X_train, y_train), batch_size=128, shuffle=True)\n",
    "dl_val = DataLoader(to_ds(X_val, y_val), batch_size=128, shuffle=False)\n",
    "x, y = next(iter(dl_train))\n",
    "c_in = x.shape[1]\n",
    "\n",
    "net = PL_CSS(d_in=c_in, epoch_steps=len(dl_train),  max_epochs=max_epochs, lr=4e-4, \n",
    "             weight_decay=1e-5)\n",
    "trainer = pl.Trainer(\n",
    "    # gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=verbose, enable_model_summary=verbose\n",
    ")\n",
    "trainer.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n",
    "\n",
    "\n",
    "df_hist, df_hist_step  = read_metrics_csv(trainer.logger.experiment.metrics_file_path)\n",
    "\n",
    "df_hist[['val/acc', 'train/acc']].plot()\n",
    "df_hist[['val/loss_epoch', 'train/loss_epoch']].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = trainer1.predict(net, dataloaders=dl_val)\n",
    "y_pred_prob = torch.cat(r).flatten()\n",
    "y_pred = y_pred_prob > 0.5\n",
    "\n",
    "score = roc_auc_score(y_val, y_pred_prob)\n",
    "\n",
    "print(score)\n",
    "target_names = [0, 1]\n",
    "cm = confusion_matrix(y_val, y_pred, target_names=target_names, normalize='true')\n",
    "cr = classification_report(y_val, y_pred, target_names=target_names)\n",
    "print(cm)\n",
    "print(cr)\n",
    "\n",
    "# return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB, MultinomialNB\n",
    "from mapie.classification import MapieClassifier\n",
    "\n",
    "layers = X.shape[1]\n",
    "X = rearrange(hs, 'b l hs -> b (l hs)')\n",
    "X_train, X_val, y_train, y_val = preproc(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced',).fit(X_train, y_train)\n",
    "mapie = MapieClassifier(estimator=clf, cv=\"prefit\",\n",
    "                        # method=\"score\",                        \n",
    "                        ).fit(X_train, y_train)\n",
    "y_val_prob, y_val_pred = mapie.predict(X_val, alpha=0.2)\n",
    "r = postproc(y_val_prob, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
