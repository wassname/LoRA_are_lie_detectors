{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment to use lora to make a lying model. Here we think of Lora as a probe, as it acts in a very similar way - modifying the residual stream.\n",
    "\n",
    "Then the hope is it will assist at lie detecting and generalize to unseen dataset\n",
    "\n",
    "- https://github.dev/JD-P/minihf/blob/b54075c34ef88d9550e37fdf709e78e5a68787c4/lora_tune.py\n",
    "- https://github.com/jonkrohn/NLP-with-LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from typing import Optional, List, Dict, Union\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from einops import rearrange\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, LoftQConfig, IA3Config\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.add(os.sys.stderr, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "\n",
    "# # quiet please\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\n",
    "#     \"ignore\", \".*sampler has shuffling enabled, it is strongly recommended that.*\"\n",
    "# )\n",
    "# warnings.filterwarnings(\"ignore\", \".*has been removed as a dependency of.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.config import ExtractConfig\n",
    "from src.prompts.prompt_loading import load_preproc_dataset\n",
    "from src.models.load import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "max_epochs = 1\n",
    "device = \"cuda:0\"\n",
    "\n",
    "cfg = ExtractConfig(\n",
    "    batch_size=2,\n",
    "    max_examples=(400, 150),\n",
    "    intervention_fit_examples=60,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 409,600 || all params: 2,780,093,440 || trainable%: 0.014733317740572058\n"
     ]
    }
   ],
   "source": [
    "# TODO I would like to only have biases, but for now lets just try a very small intervention on the last parts of a layer...\n",
    "peft_config = LoraConfig(\n",
    "    target_modules=[\n",
    "        \"out_proj\",\n",
    "        \"mlp.fc2\",\n",
    "    ],  # only the layers that go directly to the residual\n",
    "    bias=\"lora_only\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=1,\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "peft_config = IA3Config(\n",
    "    task_type=TaskType.SEQ_CLS, target_modules=[ \"out_proj\",\n",
    "        \"mlp.fc2\",], feedforward_modules=[\"out_proj\", \"mlp.fc2\",]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-19 18:56:00.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m364\u001b[0m - \u001b[1mmedian token length: 302.0 for amazon_polarity. max_length=777\u001b[0m\n",
      "2023-12-19T18:56:00.410639+0800 INFO median token length: 302.0 for amazon_polarity. max_length=777\n",
      "\u001b[32m2023-12-19 18:56:00.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m368\u001b[0m - \u001b[1mtruncation rate: 0.00% on amazon_polarity\u001b[0m\n",
      "2023-12-19T18:56:00.412199+0800 INFO truncation rate: 0.00% on amazon_polarity\n",
      "Filter: 100%|██████████| 1652/1652 [00:00<00:00, 2657.10 examples/s]\n",
      "\u001b[32m2023-12-19 18:56:01.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.prompts.prompt_loading\u001b[0m:\u001b[36mload_preproc_dataset\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mnum_rows (after filtering out truncated rows) 1652=>1652\u001b[0m\n",
      "2023-12-19T18:56:01.045738+0800 INFO num_rows (after filtering out truncated rows) 1652=>1652\n"
     ]
    }
   ],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_name = \"amazon_polarity\"\n",
    "ds_tokens = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ").with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/jonkrohn/NLP-with-LLMs/blob/main/code/Finetune-T5-on-GPU.ipynb\n",
    "import lightning.pytorch as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.dm import DeceptionDataModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.pl_lora_ft import AtapterFinetuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.datasets.dm.DeceptionDataModule at 0x7fd67672cbd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size)\n",
    "dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dm.train_dataloader()\n",
    "dl_val = dm.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ds_string', 'example_i', 'answer', 'messages', 'answer_choices', 'template_name', 'label_true', 'label_instructed', 'instructed_to_lie', 'sys_instr_name', 'question', 'input_ids', 'attention_mask', 'truncated', 'length', 'prompt_truncated', 'choice_ids']) torch.Size([2, 777])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(dl_train))\n",
    "print(b.keys(), b[\"input_ids\"].shape)\n",
    "c_in = b[\"input_ids\"].shape[1]\n",
    "c_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n"
     ]
    }
   ],
   "source": [
    "net = AtapterFinetuner(\n",
    "    model, tokenizer, lr=5e-5, weight_decay=0, total_steps=len(dl_train) * max_epochs\n",
    ")\n",
    "\n",
    "print(c_in)\n",
    "# net.model.enable_adapters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.training_step(b, None)\n",
    "# o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debug\n",
    "# with torch.no_grad():\n",
    "#     o = net.predict_step(b, None)\n",
    "# # o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# we want to init lightning early, so it inits accelerate\n",
    "trainer1 = pl.Trainer(\n",
    "    precision=\"16-true\",\n",
    "    # precision=\"16-mixed\",\n",
    "    # precision=\"b16-mixed\",\n",
    "    # precision=\"b16-mixed\",\n",
    "    gradient_clip_val=20,\n",
    "    # accelerator=\"auto\",\n",
    "    # devices=\"1\",\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    accumulate_grad_batches=6,\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=3,\n",
    "    # enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   4%|▍         | 6/138 [00:05<02:05,  1.06it/s, v_num=82]         "
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/01_mjc.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/notebooks/01_mjc.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer1\u001b[39m.\u001b[39;49mfit(model\u001b[39m=\u001b[39;49mnet, train_dataloaders\u001b[39m=\u001b[39;49mdl_train, val_dataloaders\u001b[39m=\u001b[39;49mdl_val);\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:180\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m     \u001b[39m# when the strategy handles accumulation, we want to always call the optimizer step\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mhandles_gradient_accumulation\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39m# -------------------\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[39m# automatic_optimization=True: perform ddp sync only when performing optimizer_step\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[39mwith\u001b[39;00m _block_parallel_sync_behavior(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy, block\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step(batch_idx, closure)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/.venv/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/models/pl_lora_ft.py:127\u001b[0m, in \u001b[0;36mAtapterFinetuner.training_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(batch, batch_idx)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/models/pl_lora_ft.py:119\u001b[0m, in \u001b[0;36mAtapterFinetuner._step\u001b[0;34m(self, batch, batch_idx, stage)\u001b[0m\n\u001b[1;32m    114\u001b[0m     res_a \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m_adapt\u001b[39m\u001b[39m'\u001b[39m:v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m postprocess_result(batch, out_a)\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    116\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mres, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mres_a\n\u001b[1;32m    117\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m loss, loss_choices, loss_all \u001b[39m=\u001b[39m get_loss(batch, out, out_a)\n\u001b[1;32m    120\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39misfinite(loss)\n\u001b[1;32m    122\u001b[0m batch_size \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/sgd_probes_are_lie_detectors/src/models/pl_lora_ft.py:70\u001b[0m, in \u001b[0;36mget_loss\u001b[0;34m(batch, out, out_a)\u001b[0m\n\u001b[1;32m     65\u001b[0m loss_all \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mkl_div(\n\u001b[1;32m     66\u001b[0m     log_probs_a, log_probs_r, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatchmean\u001b[39m\u001b[39m\"\u001b[39m, log_target\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m loss \u001b[39m=\u001b[39m loss_choices \u001b[39m# + loss_all * 1e-8\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39misfinite(loss)\n\u001b[1;32m     72\u001b[0m \u001b[39mreturn\u001b[39;00m loss, loss_choices, loss_all\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer1.fit(model=net, train_dataloaders=dl_train, val_dataloaders=dl_val);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(trainer1.log_dir)/'final'\n",
    "model.save_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.lightning import read_metrics_csv\n",
    "\n",
    "pd.read_csv(trainer1.logger.experiment.metrics_file_path).bfill().ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hist_epoch, df_hist_step = read_metrics_csv(trainer1.logger.experiment.metrics_file_path)\n",
    "# # for key in [\"loss\"]:\n",
    "# #     df_hist_epoch[[c for c in df_hist_epoch.columns if key in c]].plot(logy=True)\n",
    "\n",
    "# df_hist_step = df_hist_step.dropna()\n",
    "# df_hist_step.plot()\n",
    "# df_hist_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row\n",
    "bi = cfg.intervention_fit_examples + 4\n",
    "inputs = ds_tokens.with_format(\"torch\")[bi]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# generate\n",
    "# https://huggingface.co/docs/transformers/v4.34.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen(model):\n",
    "    s = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"][None, :].to(model.device),\n",
    "        attention_mask=inputs[\"attention_mask\"][None, :].to(model.device),\n",
    "        use_cache=False,\n",
    "        max_new_tokens=100,\n",
    "        min_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        early_stopping=False,\n",
    "    )\n",
    "    input_l = inputs[\"input_ids\"].shape[0]\n",
    "    old = tokenizer.decode(\n",
    "        s[0, :input_l], clean_up_tokenization_spaces=False, skip_special_tokens=False\n",
    "    )\n",
    "    new = tokenizer.decode(\n",
    "        s[0, input_l:], clean_up_tokenization_spaces=False, skip_special_tokens=False\n",
    "    )\n",
    "    display(HTML(f\"<pre>{old}</pre><b><pre>{new}</pre></b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with model.disable_adapters():\n",
    "# with model.disable_adapter():\n",
    "#     gen(model)\n",
    "\n",
    "# gen(model)\n",
    "\n",
    "# # RuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = model, tokenizer = load_model(\n",
    "    cfg.model,\n",
    "    device=device,\n",
    "    adaptor_path=checkpoint_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with model.disable_adapters():\n",
    "with model.disable_adapter():\n",
    "    gen(model)\n",
    "\n",
    "gen(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(cfg.max_examples)\n",
    "ds_name = \"imdb\"\n",
    "ds_tokens = load_preproc_dataset(\n",
    "    ds_name,\n",
    "    tokenizer,\n",
    "    N=N // 4,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    max_length=cfg.max_length,\n",
    "    prompt_format=cfg.prompt_format,\n",
    ").with_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DeceptionDataModule(ds_tokens, batch_size=cfg.batch_size * 3)\n",
    "dl_train2 = dm.train_dataloader()\n",
    "dl_val2 = dm.val_dataloader()\n",
    "dl_test2 = dm.test_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_oos2 = DataLoader(\n",
    "    ds_tokens, batch_size=cfg.batch_size * 3, drop_last=False, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_acc_subset(df, query, verbose=True):\n",
    "#     if query:\n",
    "#         df = df.query(query)\n",
    "#     acc = (df[\"probe_pred\"] == df[\"y\"]).mean()\n",
    "#     if verbose:\n",
    "#         print(f\"acc={acc:2.2%},\\tn={len(df)},\\t[{query}] \")\n",
    "#     return acc\n",
    "\n",
    "\n",
    "# def calc_metrics(dm, trainer, net, use_val=False, verbose=True):\n",
    "#     dl_test = dm.test_dataloader()\n",
    "#     rt = trainer.predict(net, dataloaders=dl_test)\n",
    "#     y_test_pred = np.concatenate(rt)\n",
    "#     splits = dm.splits[\"test\"]\n",
    "#     df_test = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "#     df_test[\"probe_pred\"] = y_test_pred > 0.0\n",
    "\n",
    "#     if use_val:\n",
    "#         dl_val = dm.val_dataloader()\n",
    "#         rv = trainer.predict(net, dataloaders=dl_val)\n",
    "#         y_val_pred = np.concatenate(rv)\n",
    "#         splits = dm.splits[\"val\"]\n",
    "#         df_val = dm.df.iloc[splits[0] : splits[1]].copy()\n",
    "#         df_val[\"probe_pred\"] = y_val_pred > 0.0\n",
    "\n",
    "#         df_test = pd.concat([df_val, df_test])\n",
    "\n",
    "#     if verbose:\n",
    "#         print(\"probe results on subsets of the data\")\n",
    "#     acc = get_acc_subset(df_test, \"\", verbose=verbose)\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True\", verbose=verbose\n",
    "#     )  # it was ph told to lie\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==False\", verbose=verbose\n",
    "#     )  # it was told not to lie\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"llm_ans==label_true\", verbose=verbose\n",
    "#     )  # the llm gave the true ans\n",
    "#     get_acc_subset(\n",
    "#         df_test, \"llm_ans==label_instructed\", verbose=verbose\n",
    "#     )  # the llm gave the desired ans\n",
    "#     acc_lie_lie = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=verbose\n",
    "#     )  # it was told to lie, and it did lie\n",
    "#     acc_lie_truth = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=verbose\n",
    "#     )\n",
    "\n",
    "#     a = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==False & llm_ans==label_instructed\", verbose=False\n",
    "#     )\n",
    "#     b = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==False & llm_ans!=label_instructed\", verbose=False\n",
    "#     )\n",
    "#     c = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans==label_instructed\", verbose=False\n",
    "#     )\n",
    "#     d = get_acc_subset(\n",
    "#         df_test, \"instructed_to_lie==True & llm_ans!=label_instructed\", verbose=False\n",
    "#     )\n",
    "#     d1 = pd.DataFrame(\n",
    "#         [[a, b], [c, d]],\n",
    "#         index=[\"instructed_to_lie==False\", \"instructed_to_lie==True\"],\n",
    "#         columns=[\"llm_ans==label_instructed\", \"llm_ans!=label_instructed\"],\n",
    "#     )\n",
    "#     d1 = pd.DataFrame(\n",
    "#         [[a, b], [c, d]],\n",
    "#         index=[\"tell a truth\", \"tell a lie\"],\n",
    "#         columns=[\"did\", \"didn't\"],\n",
    "#     )\n",
    "#     d1.index.name = \"instructed to\"\n",
    "#     d1.columns.name = \"llm gave\"\n",
    "#     print(\"probe accuracy for quadrants\")\n",
    "#     display(d1.round(2))\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"⭐PRIMARY METRIC⭐ acc={acc:2.2%} from probe\")\n",
    "#         print(f\"⭐SECONDARY METRIC⭐ acc_lie_lie={acc_lie_lie:2.2%} from probe\")\n",
    "#     return dict(acc=acc, acc_lie_lie=acc_lie_lie, acc_lie_truth=acc_lie_truth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "# def transform_dl_k(k: str) -> str:\n",
    "#     p = re.match(r\"test\\/(.+)\\/dataloader_idx_\\d\", k)\n",
    "#     return p.group(1) if p else k\n",
    "\n",
    "\n",
    "# def rename(rs, ks=[\"train\", \"val\", \"test\"]):\n",
    "#     rs = {\n",
    "#         ks[i]: {transform_dl_k(k): v for k, v in rs[i].items()} for i in range(len(ks))\n",
    "#     }\n",
    "#     return rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = trainer1.test(\n",
    "#     net,\n",
    "#     dataloaders=[\n",
    "#         # dl_train2, dl_val2,\n",
    "#         dl_test2,\n",
    "#         dl_oos2,\n",
    "#     ],\n",
    "# )\n",
    "# rs = rename(rs, [\"train\", \"val\", \"test\", \"oos\"])\n",
    "# rs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "Here we want to see if we can do a probe on the hidden states to see if it's lying...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = trainer1.predict(net, dataloaders=dl_val2)\n",
    "y_val_pred = np.concatenate(rv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testval_metrics = calc_metrics(dm, trainer1, net, use_val=True)\n",
    "\n",
    "# rs['test'] = {**rs['test'], **test_metrics}\n",
    "rs[\"test\"][\"acc_lie_lie\"] = testval_metrics[\"acc_lie_lie\"]\n",
    "rs[\"testval_metrics\"] = rs[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
